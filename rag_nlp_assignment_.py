# -*- coding: utf-8 -*-
"""RAG-NLP assignment .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11CIJ4ORePgXVagOyiCO9VVsPwgJPHGSI

##RAG NLP project / Assynment
"""

#install some important packages
!pip install langchain
!pip install openai
!pip install PyPDF2
!pip install faiss-cpu
!pip install tiktoken

#importing some important libraries
from PyPDF2 import PdfReader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI
import os

#loading API key from openai
os.environ["OPENAI_API_KEY"]="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"

#connecting google drive
from google.colab import drive
drive.mount('/content/drive')

#import given pdf file by giving file path
reader= PdfReader('/content/drive/MyDrive/My_project_/farmframeAI/RAG project/mian_merge _file.pdf')
reader

#reading data from the file and put them into a variable named raw_text
raw_text=''
for i, page in enumerate(reader.pages):
  text=page.extract_text()
  if text:
    raw_text+=text
raw_text

#need to split the text that we read into smaller chunks so that during information retreival we dont hit the tokeb size limits
text_splitter=CharacterTextSplitter(
    separator="\n",
    chunk_size=1000,
    chunk_overlap=0,
    length_function=len,
)
texts=text_splitter.split_text(raw_text)

raw_text[:150]

#download embedding from OpenAI
embeddings=OpenAIEmbeddings(openai_api_key=os.environ["OPENAI_API_KEY"])

docsearch=FAISS.from_texts(texts,embeddings)
docsearch

chain=load_qa_chain(OpenAI(),chain_type="stuff")

"""##building chatbot  """

#some important lib to build chatbot as chatgpt
from IPython.display import display
import ipywidgets as widgets
from langchain.chains import ConversationalRetrievalChain

#create conversation chain that uses vectordb as retriever
qa=ConversationalRetrievalChain.from_llm(OpenAI(temperature=0.1),docsearch.as_retriever())

chat_history=[]
def on_submit(_):
  query=input_box.value
  input_box.vakue=""

  if query.lower()=='exit':
    print("thanks for using the chatbot")
    return

  result=qa({"question": query ,"chat_history":chat_history})
  chat_history.append((query,result['answer']))

  display(widgets.HTML(f'<b><font color="red">User : </font></b>{query}'))
  display(widgets.HTML(f'<b><font color="blue">Chatbot :</font></b>{result["answer"]}'))

print('Welcome to the article')

input_box= widgets.Text(placeholder='please type question: ')
input_box.on_submit(on_submit)

display(input_box)











