{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "58ff365cfc8140e2b298c880a6971cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_08bd0e1dcdf54b22abd93c93cc482fc4",
            "placeholder": "please type question: ",
            "style": "IPY_MODEL_93d8a3d6be4f4462b18b9e6f279ac436",
            "value": "is this article is worthy"
          }
        },
        "08bd0e1dcdf54b22abd93c93cc482fc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93d8a3d6be4f4462b18b9e6f279ac436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5f530a5b3c14689b69bdabe445f1e14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee2c0724ff394844a69340f9ec04d0d1",
            "placeholder": "​",
            "style": "IPY_MODEL_f165e5a55cb24099bd9f6d0f2425d4fe",
            "value": "<b><font color=\"red\">User : </font></b>what is this in artic"
          }
        },
        "ee2c0724ff394844a69340f9ec04d0d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f165e5a55cb24099bd9f6d0f2425d4fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e79213c89ad472090424ac717e2b5e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fb6a6e50c824566b26ec922580a3cf5",
            "placeholder": "​",
            "style": "IPY_MODEL_02d8190d2db843ea96121e3736c3abb1",
            "value": "<b><font color=\"blue\">Chatbot :</font></b> This is an article about ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT."
          }
        },
        "9fb6a6e50c824566b26ec922580a3cf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02d8190d2db843ea96121e3736c3abb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f0cb01978f4419684d5dde8785cab45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25d9a4339a9c4f9c9e9b9fe10a76e1c4",
            "placeholder": "​",
            "style": "IPY_MODEL_c4f3ad06d9b44a3eb4911d7303b7a7e5",
            "value": "<b><font color=\"red\">User : </font></b>how many pages in thiss artic"
          }
        },
        "25d9a4339a9c4f9c9e9b9fe10a76e1c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4f3ad06d9b44a3eb4911d7303b7a7e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "427613941e284d7897e9bda31fff6c2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4cc6cd4fa5b45689bb1d76a35aac63b",
            "placeholder": "​",
            "style": "IPY_MODEL_cbe1231509cf434981e8ea215aba4f13",
            "value": "<b><font color=\"blue\">Chatbot :</font></b> 10 pages."
          }
        },
        "b4cc6cd4fa5b45689bb1d76a35aac63b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbe1231509cf434981e8ea215aba4f13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "054c74edba7a49fb94bb4f2ef24bef4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6a230f9e36346168ed429026be71b2f",
            "placeholder": "​",
            "style": "IPY_MODEL_e3e213ba2b99432ebac79239431ee177",
            "value": "<b><font color=\"red\">User : </font></b>who is main auther"
          }
        },
        "b6a230f9e36346168ed429026be71b2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3e213ba2b99432ebac79239431ee177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6656c481303a46cfb79a3e494bf66e16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca90d2a23ab6460b9fc055d5870c0c91",
            "placeholder": "​",
            "style": "IPY_MODEL_b4f0a9f041944d75b89dd48873363c2e",
            "value": "<b><font color=\"blue\">Chatbot :</font></b> The main author of this article is Rodrigo Nogueira."
          }
        },
        "ca90d2a23ab6460b9fc055d5870c0c91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4f0a9f041944d75b89dd48873363c2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f0194cb626f445fa33351d84acd8134": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf3764bbd1c04b5ab8ca793ed1d4e19e",
            "placeholder": "​",
            "style": "IPY_MODEL_709d6e7454b342d69c1a95448a8f21c1",
            "value": "<b><font color=\"red\">User : </font></b>is this article is worth"
          }
        },
        "bf3764bbd1c04b5ab8ca793ed1d4e19e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "709d6e7454b342d69c1a95448a8f21c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4d2d5391ab74014975d750aacdca4a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6efd3462cffd4217b8794ed4165e0ffd",
            "placeholder": "​",
            "style": "IPY_MODEL_51e657ca3ced405dafd005c5422a4b51",
            "value": "<b><font color=\"blue\">Chatbot :</font></b> No, this article is not worth reading. It is a research paper that provides technical information about a technique for improving the effectiveness and efficiency of the Doc2Query model."
          }
        },
        "6efd3462cffd4217b8794ed4165e0ffd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51e657ca3ced405dafd005c5422a4b51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##RAG NLP project / Assynment"
      ],
      "metadata": {
        "id": "Is5qkxfqKjRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install some important packages\n",
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install PyPDF2\n",
        "!pip install faiss-cpu\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq3ybrJ4K3nP",
        "outputId": "c52eaf28-1873-4a3a-87f6-6b54c9f68b74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.257-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.11 (from langchain)\n",
            "  Downloading langsmith-0.0.19-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, langsmith, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.14 langchain-0.0.257 langsmith-0.0.19 marshmallow-3.20.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m630.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing some important libraries\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "import os"
      ],
      "metadata": {
        "id": "Pp-BX15SKtjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading API key from openai\n",
        "os.environ[\"OPENAI_API_KEY\"]=\"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\""
      ],
      "metadata": {
        "id": "_TzsUtBrKtP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#connecting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDgqa6i6LR24",
        "outputId": "6871dc55-9799-473f-9b01-d8c1d28fe091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import given pdf file by giving file path\n",
        "reader= PdfReader('/content/drive/MyDrive/My_project_/farmframeAI/RAG project/mian_merge _file.pdf')\n",
        "reader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSF5MPs5LX-2",
        "outputId": "72629e09-c363-4ef8-e597-09619fa4985d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PyPDF2._reader.PdfReader at 0x7a610992d7b0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reading data from the file and put them into a variable named raw_text\n",
        "raw_text=''\n",
        "for i, page in enumerate(reader.pages):\n",
        "  text=page.extract_text()\n",
        "  if text:\n",
        "    raw_text+=text\n",
        "raw_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "PioeI5uwLX6B",
        "outputId": "29f9ded3-a12d-4bfb-a8c2-73c3c93634d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Query2doc: Query Expansion with Large Language Models\\nLiang Wang and Nan Yang and Furu Wei\\nMicrosoft Research\\n{wangliang,nanya,fuwei}@microsoft.com\\nAbstract\\nThis paper introduces a simple yet effec-\\ntive query expansion approach, denoted as\\nquery2doc , to improve both sparse and dense\\nretrieval systems. The proposed method\\nﬁrst generates pseudo-documents by few-shot\\nprompting large language models (LLMs), and\\nthen expands the query with generated pseudo-\\ndocuments. LLMs are trained on web-scale\\ntext corpora and are adept at knowledge mem-\\norization. The pseudo-documents from LLMs\\noften contain highly relevant information that\\ncan aid in query disambiguation and guide\\nthe retrievers. Experimental results demon-\\nstrate that query2doc boosts the performance\\nof BM25 by 3% to 15% on ad-hoc IR datasets,\\nsuch as MS-MARCO and TREC DL, with-\\nout any model ﬁne-tuning. Furthermore, our\\nmethod also beneﬁts state-of-the-art dense re-\\ntrievers in terms of both in-domain and out-of-\\ndomain results.\\n1 Introduction\\nInformation retrieval (IR) aims to locate relevant\\ndocuments from a large corpus given a user is-\\nsued query. It is a core component in modern\\nsearch engines and researchers have invested for\\ndecades in this ﬁeld. There are two mainstream\\nparadigms for IR: lexical-based sparse retrieval,\\nsuch as BM25, and embedding-based dense re-\\ntrieval (Xiong et al., 2021; Qu et al., 2021). Al-\\nthough dense retrievers perform better when large\\namounts of labeled data are available (Karpukhin\\net al., 2020), BM25 remains competitive on out-of-\\ndomain datasets (Thakur et al., 2021).\\nQuery expansion (Rocchio, 1971; Lavrenko\\nand Croft, 2001) is a long-standing technique\\nthat rewrites the query based on pseudo-relevance\\nfeedback or external knowledge sources such as\\nWordNet. For sparse retrieval, it can help bridge\\nthe lexical gap between the query and the docu-\\nments. However, query expansion methods like\\nRM3 (Lavrenko and Croft, 2001; Lv and Zhai,2009) have only shown limited success on popular\\ndatasets (Campos et al., 2016), and most state-of-\\nthe-art dense retrievers do not adopt this technique.\\nIn the meantime, document expansion methods like\\ndoc2query (Nogueira et al., 2019) have proven to\\nbe effective for sparse retrieval.\\nIn this paper, we demonstrate the effectiveness\\nof LLMs (Brown et al., 2020) as query expan-\\nsion models by generating pseudo-documents con-\\nditioned on few-shot prompts. Given that search\\nqueries are often short, ambiguous, or lack neces-\\nsary background information, LLMs can provide\\nrelevant information to guide retrieval systems, as\\nthey memorize an enormous amount of knowledge\\nand language patterns by pre-training on trillions\\nof tokens.\\nOur proposed method, called query2doc , gen-\\nerates pseudo-documents by few-shot prompting\\nLLMs and concatenates them with the original\\nquery to form a new query. This method is simple\\nto implement and does not require any changes in\\ntraining pipelines or model architectures, making it\\northogonal to the progress in the ﬁeld of LLMs and\\ninformation retrieval. Future methods can easily\\nbuild upon our query expansion framework.\\nFor in-domain evaluation, we adopt the MS-\\nMARCO passage ranking (Campos et al., 2016),\\nTREC DL 2019 and 2020 datasets. Pseudo-\\ndocuments are generated by prompting an im-\\nproved version of GPT-3 text-davinci-003 from\\nOpenAI (Brown et al., 2020). Results show that\\nquery2doc substantially improves the off-the-shelf\\nBM25 algorithm without ﬁne-tuning any model,\\nparticularly for hard queries from the TREC DL\\ntrack. Strong dense retrievers, including DPR\\n(Karpukhin et al., 2020), SimLM (Wang et al.,\\n2022a), and E5 (Wang et al., 2022b) also bene-\\nﬁt from query2doc , although the gains tend to be\\ndiminishing when distilling from a strong cross-\\nencoder based re-ranker. Experiments in zero-shot\\nOOD settings demonstrate that our method out-arXiv:2303.07678v1  [cs.IR]  14 Mar 2023performs strong baselines on most datasets. Fur-\\nther analysis also reveals the importance of model\\nscales: query2doc works best when combined with\\nthe most capable LLMs while small language mod-\\nels only provide marginal improvements over base-\\nlines.\\nTo aid reproduction, we release all\\nthe generations from text-davinci-003\\nat https://huggingface.co/datasets/\\nintfloat/query2doc_msmarco .\\n2 Method\\nWrite a passage that answers the given query:\\nQuery: what state is this zip code 85282\\nPassage: Welcome to TEMPE, AZ 85282. \\n85282 is a rural zip code in Tempe, Arizona. \\nThe population is primarily white…\\n…\\nQuery: when was pokemon green released\\nPassage:LLM Prompts\\nPokemon Green was released in Japan on \\nFebruary 27th, 1996. It was the first in the \\nPokemon series of games and served as the \\nbasis for Pokemon Red and Blue, which were \\nreleased in the US in 1998. The original \\nPokemon Green remains a beloved classic \\namong fans of the series.LLM Output\\nFigure 1: Illustration of query2doc few-shot prompting.\\nWe omit some in-context examples for space reasons.\\nGiven a query q, we employ few-shot prompting\\nto generate a pseudo-document d0as depicted in\\nFigure 1. The prompt comprises a brief instruction\\n“Write a passage that answers the given query:”\\nandklabeled pairs randomly sampled from a\\ntraining set. We use k= 4throughout this paper.\\nSubsequently, we rewrite qto a new query q+\\nby concatenating with the pseudo-document d0.\\nThere are slight differences in the concatenation\\noperation for sparse and dense retrievers, which\\nwe elaborate on in the following section.\\nSparse Retrieval Since the query qis typically\\nmuch shorter than pseudo-documents, we boost the\\nquery term weights by repeating the query ntimes\\nbefore concatenating with the pseudo-document d0:q+=concat(fqg\\x02n; d0) (1)\\nHere, “concat” denotes the string concatenation\\nfunction. q+is used as the new query for\\nBM25 retrieval. We ﬁnd that n= 5 is a gener-\\nally good value and do not tune it on a dataset basis.\\nDense Retrieval The new query q+is a sim-\\nple concatenation of the original query qand the\\npseudo-document d0separated by [SEP]:\\nq+=concat(q;[SEP]; d0) (2)\\nFor training dense retrievers, several factors can\\ninﬂuence the ﬁnal performance, such as hard nega-\\ntive mining (Xiong et al., 2021), intermediate pre-\\ntraining (Gao and Callan, 2021), and knowledge\\ndistillation from a cross-encoder based re-ranker\\n(Qu et al., 2021). In this paper, we investigate two\\nsettings to gain a more comprehensive understand-\\ning of our method. The ﬁrst setting is training DPR\\n(Karpukhin et al., 2020) models initialized from\\nBERT basewith BM25 hard negatives only. The op-\\ntimization objective is a standard contrastive loss:\\nLcont=\\x00logehq\\x01hd\\nehq\\x01hd+P\\ndi2Nehq\\x01hdi(3)\\nwhere hqandhdrepresent the embeddings for the\\nquery and document, respectively. Ndenotes the\\nset of hard negatives.\\nThe second setting is to build upon state-of-the-\\nart dense retrievers and use KL divergence to distill\\nfrom a cross-encoder teacher model.\\nminDKL(pce;pstu) +\\x0bLcont (4)\\npceandpstuare the probabilities from the cross-\\nencoder and our student model, respectively. \\x0bis\\na coefﬁcient to balance the distillation loss and\\ncontrastive loss.\\nComparison with Pseudo-relevance Feedback\\nOur proposed method can be viewed as a variant\\nof pseudo-relevance feedback (PRF) (Lavrenko\\nand Croft, 2001; Lv and Zhai, 2009). In conven-\\ntional PRF, the feedback signals for query expan-\\nsion come from the top-k documents obtained in\\nthe initial retrieval step, while our method prompts\\nLLMs to generate pseudo-documents. Our method\\ndoes not rely on the quality of the initial retrieval re-\\nsults, which are often noisy or irrelevant. Rather, it\\nexploits cutting-edge LLMs to generate documents\\nthat are more likely to contain relevant terms.Method Fine-tuningMS MARCO dev TREC DL 19 TREC DL 20\\nMRR@10 R@50 R@1k nDCG@10 nDCG@10\\nSparse retrieval\\nBM25 7 18.4 58.5 85.7 51.2\\x0347.7\\x03\\n+ query2doc 7 21.4+3.065.3+6.891.8+6.166.2+15.062.9+15.2\\nBM25 + RM3 7 15.8 56.7 86.4 52.2 47.4\\ndocT5query (Nogueira and Lin) 3 27.7 75.6 94.7 64.2 -\\nDense retrieval w/o distillation\\nANCE (Xiong et al., 2021) 3 33.0 - 95.9 64.5 64.6\\nHyDE (Gao et al., 2022) 7 - - - 61.3 57.9\\nDPR bert-base (our impl.) 3 33.7 80.5 95.9 64.7 64.1\\n+ query2doc 3 35.1+1.482.6+2.197.2+1.368.7+4.067.1+3.0\\nDense retrieval w/ distillation\\nRocketQAv2 (Ren et al., 2021) 3 38.8 86.2 98.1 - -\\nAR2 (Zhang et al., 2021) 3 39.5 87.8 98.6 - -\\nSimLM (Wang et al., 2022a) 3 41.1 87.8 98.7 71.4 69.7\\n+ query2doc 3 41.5+0.488.0+0.298.8+0.172.9+1.571.6+1.9\\nE5base+ KD (Wang et al., 2022b) 3 40.7 87.6 98.6 74.3 70.7\\n+ query2doc 3 41.5+0.888.1+0.598.7+0.174.9+0.672.5+1.8\\nTable 1: Main results on the MS-MARCO passage ranking and TREC datasets. The “Fine-tuning” column indi-\\ncates whether the method requires ﬁne-tuning model on labeled data or not. \\x03: our reproduction.\\n3 Experiments\\n3.1 Setup\\nEvaluation Datasets For in-domain evaluation,\\nwe utilize the MS-MARCO passage ranking (Cam-\\npos et al., 2016), TREC DL 2019 (Craswell et al.,\\n2020a) and 2020 (Craswell et al., 2020b) datasets.\\nFor zero-shot out-of-domain evaluation, we select\\nﬁve low-resource datasets from the BEIR bench-\\nmark (Thakur et al., 2021). The evaluation met-\\nrics include MRR@10, R@k ( k2f50;1kg), and\\nnDCG@10.\\nHyperparameters For sparse retrieval including\\nBM25 and RM3, we adopt the default implementa-\\ntion from Pyserini (Lin et al., 2021). When training\\ndense retrievers, we use mostly the same hyper-\\nparameters as SimLM (Wang et al., 2022a), with\\nthe exception of increasing the maximum query\\nlength to 144to include pseudo-documents. When\\nprompting LLMs, we include 4in-context exam-\\nples and use the default temperature of 1to sample\\nat most 128tokens. For further details, please refer\\nto Appendix A.\\n3.2 Main Results\\nIn Table 1, we list the results on the MS-MARCO\\npassage ranking and TREC DL datasets. For sparse\\nretrieval, “BM25 + query2doc” beats the BM25\\nbaseline with over 15%improvements on TREC\\nDL 2019 and 2020 datasets. Our manual inspection\\nreveals that most queries from the TREC DL trackare long-tailed entity-centric queries, which beneﬁt\\nmore from the exact lexical match. The traditional\\nquery expansion method RM3 only marginally\\nimproves the R@1k metric. Although the docu-\\nment expansion method docT5query achieves bet-\\nter numbers on the MS-MARCO dev set, it requires\\ntraining a T5-based query generator with all the\\navailable labeled data, while “BM25 + query2doc”\\ndoes not require any model ﬁne-tuning.\\nFor dense retrieval, the model variants that com-\\nbine with query2doc also outperform the corre-\\nsponding baselines on all metrics. However, the\\ngain brought by query2doc tends to diminish when\\nusing intermediate pre-training or knowledge distil-\\nlation from cross-encoder re-rankers, as shown by\\nthe “SimLM + query2doc” and “E5 + query2doc”\\nresults.\\nFor zero-shot out-of-domain retrieval, the results\\nare mixed as shown in Table 2. Entity-centric\\ndatasets like DBpedia see the largest improvements.\\nOn the NFCorpus and Scifact datasets, we observe\\na minor decrease in ranking quality. This is likely\\ndue to the distribution mismatch between training\\nand evaluation.\\n4 Analysis\\nScaling up LLMs is Critical For our proposed\\nmethod, a question that naturally arises is: how\\ndoes the model scale affect the quality of query\\nexpansion? Table 3 shows that the performance\\nsteadily improves as we go from the 1.3B modelDBpedia NFCorpus Scifact Trec-Covid Touche2020\\nBM25 31.3 32.5 66.5 65.6 36.7\\n+ query2doc 37.0+5.734.9+2.468.6+2.172.2+6.639.8+3.1\\nSimLM (Wang et al., 2022a) 34.9 32.7 62.4 55.0 18.9\\n+ query2doc 38.3+3.432.1-0.659.5-2.959.9+4.925.6+6.7\\nE5base+ KD (Wang et al., 2022b) 40.7 35.0 70.4 74.1 30.9\\n+ query2doc 42.4+1.735.2+0.267.5-2.975.1+1.031.7+0.8\\nTable 2: Zero-shot out-of-domain results on 5 low-resource datasets from the BEIR benchmark (Thakur et al.,\\n2021). The reported numbers are nDCG@10. For a fair comparison, the in-context examples for prompting LLMs\\ncome from the MS-MARCO training set.\\n# params TREC 19 TREC 20\\nBM25 - 51.2 47.7\\nw/ babbage 1.3B 52.0 50.2\\nw/ curie 6.7B 55.1 50.1\\nw/ davinci-001 175B 63.5 58.2\\nw/ davinci-003 175B 66.2 62.9\\nTable 3: Query expansion with different model sizes.\\nto 175B models. Empirically, the texts generated\\nby smaller language models tend to be shorter and\\ncontain more factual errors. Also, the “davinci-003”\\nmodel outperforms its earlier version “davinci-001”\\nby using better training data and improved\\ninstruction tuning.\\n1 10 30 50 100\\n% labeled data for fine-tuning202224262830323436MRR on dev set\\n21.427.331.432.833.7\\n22.728.532.134.135.1\\nDPR w/o query2doc\\nDPR w/ query2doc\\nFigure 2: MRR on MS-MARCO dev set w.r.t the per-\\ncentage of labeled data used for ﬁne-tuning.\\nPerformance Gains are Consistent across Data\\nScales Figure 2 presents a comparison between\\ntwo variants of DPR models, which differ in the\\namount of labeled data used. The results show\\nthat the “DPR + query2doc” variant consistently\\noutperforms the DPR baseline by approximately\\n1%, regardless of the amount of data used for\\nﬁne-tuning. This observation highlights that ourcontribution is orthogonal to the continual scaling\\nup of supervision signals.\\nTREC 19 TREC 20\\nBM25 + query2doc 66.2 62.9\\nw/ query only 51.2 47.7\\nw/ pseudo-doc only 48.7 44.5\\nTable 4: Using the concatenation of the original query\\nand the generated pseudo-documents perform substan-\\ntially better.\\nHow to Use Pseudo-documents In this paper,\\nwe concatenate the original query and pseudo-\\ndocuments as the new query. Alternatively, one can\\nsolely use the pseudo-documents, as done in the\\napproach of HyDE (Gao et al., 2022). The results\\npresented in Table 4 demonstrate that the original\\nquery and pseudo-documents are complementary,\\nand their combination leads to substantially better\\nperformance in sparse retrieval.\\nCase Analysis In Table 5, we show two queries\\nalong with their respective pseudo-documents and\\ngroundtruth. The pseudo-documents, which are\\ngenerated by LLMs, offer detailed and mostly ac-\\ncurate information, thereby reducing the lexical\\nmismatch between the query and documents. In\\nsome cases, the pseudo-documents are sufﬁcient to\\nmeet the user’s information needs, rendering the\\nretrieval step unnecessary. However, it is worth\\nnoting that the LLM generations may contain fac-\\ntual errors. For instance, in the second query, the\\ntheme song \"It’s a Jungle Out There\" was used as\\nof season two in 2003, not 20021. Although such\\nerrors may appear subtle and difﬁcult to verify, they\\npose a signiﬁcant challenge to building trustworthy\\nsystems using LLMs.\\n1Refer to https://en.wikipedia.org/wiki/It’s_a_\\nJungle_Out_There_(song)query who killed nicholas ii of russia\\nLLM generation Nicholas II of Russia, the last Tsar of Russia, was executed by Bolshevik revolutionaries on\\nJuly 17th, 1918 , along with his wife Alexandra and their ﬁve children . The execution was\\nordered by Vladimir Lenin , the leader of the Bolshevik Party. The actual execution was carried\\nout by a ﬁring squad of Bolshevik troops, and the bodies were later buried in a mass grave. . .\\nGroundtruth (1868- 1918 ). Nicholas II was the lastemperor, or tsar, of Russia, serving from 1894 to\\n1917. Nicholas, his wife, and their ﬁve children were killed by the Bolsheviks , members of a\\nrevolutionary group (led by Lenin ) who seized control of the government in Russia during\\nthe October Revolution (see Russian Revolution) of 1917.\\nquery who sings monk theme song\\nLLM generation The theme song for the television show Monk is entitled \" It’s a Jungle Out There \" and is\\nsung by American singer-songwriter Randy Newman . The song was written speciﬁcally for\\nthe show, and it has been used as the theme song since the series premiered in 2002 . It has\\nbeen praised by critics and fans alike and is often regarded as one of the best theme songs in\\ntelevision history.\\nGroundtruth exists and is an alternate of. The Monk theme song is It’s a Jungle Out There byRandy\\nNewman . The Monk theme song is It’s a Jungle Out There byRandy Newman .\\nTable 5: Examples from the TREC DL 2020 dataset. Bold texts are the overlapping words between groundtruth\\nand pseudo-documents generated from LLMs. The italicized red sentence demonstrates a factual error in language\\nmodel generations.\\n5 Related Work\\nQuery Expansion and Document Expansion\\nare two classical techniques to improve retrieval\\nquality, particularly for sparse retrieval systems.\\nBoth techniques aim to minimize the lexical gap be-\\ntween the query and the documents. Query expan-\\nsion typically involves rewriting the query based\\non relevance feedback (Lavrenko and Croft, 2001;\\nRocchio, 1971) or lexical resources such as Word-\\nNet (Miller, 1992). In cases where labeled rele-\\nvance feedback is not available, the top-k retrieved\\ndocuments can serve as pseudo-relevance feedback\\nsignals (Lv and Zhai, 2009).\\nIn contrast, document expansion enriches the\\ndocument representation by appending additional\\nrelevant terms. Doc2query (Nogueira et al., 2019)\\ntrains a seq2seq model to predict pseudo-queries\\nbased on documents and then adds generated\\npseudo-queries to the document index. Learned\\nsparse retrieval models such as SPLADE (Formal\\net al., 2021) and uniCOIL (Lin and Ma, 2021)\\nalso learn document term weighting in an end-to-\\nend fashion. However, most state-of-the-art dense\\nretrievers (Ren et al., 2021; Wang et al., 2022a)\\ndo not adopt any expansion techniques. Our pa-\\nper demonstrates that strong dense retrievers also\\nbeneﬁt from query expansion using LLMs.\\nLarge Language Models (LLMs) such as GPT-3\\n(Brown et al., 2020), PaLM (Chowdhery et al.,\\n2022), and LLaMA (Touvron et al., 2023) are\\ntrained on trillions of tokens with billions of param-\\neters, exhibiting unparalleled generalization abilityacross various tasks. LLMs can follow instruc-\\ntions in a zero-shot manner or conduct in-context\\nlearning through few-shot prompting. Labeling a\\nfew high-quality examples only requires minimal\\nhuman effort. In this paper, we employ few-shot\\nprompting to generate pseudo-documents from a\\ngiven query. A closely related recent work HyDE\\n(Gao et al., 2022) instead focuses on the zero-\\nshot setting and uses embeddings of the pseudo-\\ndocuments for similarity search. HyDE implicitly\\nassumes that the groundtruth document and pseudo-\\ndocuments express the same semantics in different\\nwords, which may not hold for some queries. In the\\nﬁeld of question answering, RECITE (Sun et al.,\\n2022) and GENREAD (Yu et al., 2022) demon-\\nstrate that LLMs are powerful context generators\\nand can encode abundant factual knowledge. How-\\never, as our analysis shows, LLMs can sometimes\\ngenerate false claims, hindering their practical ap-\\nplication in critical areas.\\n6 Conclusion\\nThis paper presents a simple method query2doc\\nto leverage LLMs for query expansion. It ﬁrst\\nprompts LLMs with few-shot examples to gener-\\nate pseudo-documents and then integrates with ex-\\nisting sparse or dense retrievers by augmenting\\nqueries with generated pseudo-documents. The un-\\nderlying motivation is to distill the LLMs through\\nprompting. Despite its simplicity, empirical evalua-\\ntions demonstrate consistent improvements across\\nvarious retrieval models and datasets.Limitations\\nLLM call Index search\\nBM25 - 16ms\\n+ query2doc >2000ms 177ms\\nTable 6: Latency analysis for retrieval systems with our\\nproposed query2doc. We retrieve the top 100 results for\\nMS-MARCO dev queries with a single thread and then\\naverage over all the queries. The latency for LLM API\\ncalls depends on server load and is difﬁcult to precisely\\nmeasure.\\nAn apparent limitation is the efﬁciency of re-\\ntrieval. Our method requires running inference with\\nLLMs which can be considerably slower due to the\\ntoken-by-token autoregressive decoding. Moreover,\\nwith query2doc, searching the inverted index also\\nbecomes slower as the number of query terms in-\\ncreases after expansion. This is supported by the\\nbenchmarking results in Table 6. Real-world de-\\nployment of our method should take these factors\\ninto consideration.\\nReferences\\nAlexander Bondarenko, Maik Fröbe, Johannes Kiesel,\\nShahbaz Syed, Timon Gurcke, Meriem Beloucif,\\nAlexander Panchenko, Chris Biemann, Benno Stein,\\nHenning Wachsmuth, et al. 2022. Overview of\\ntouché 2022: argument retrieval. In Interna-\\ntional Conference of the Cross-Language Evalua-\\ntion Forum for European Languages , pages 311–\\n336. Springer.\\nVera Boteva, Demian Gholipour, Artem Sokolov, and\\nStefan Riezler. 2016. A full-text learning to rank\\ndataset for medical information retrieval. In Euro-\\npean Conference on Information Retrieval , pages\\n716–722. Springer.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\\nClemens Winter, Christopher Hesse, Mark Chen,\\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\\nChess, Jack Clark, Christopher Berner, Sam Mc-\\nCandlish, Alec Radford, Ilya Sutskever, and Dario\\nAmodei. 2020. Language models are few-shot learn-\\ners. In Advances in Neural Information Processing\\nSystems 33: Annual Conference on Neural Informa-\\ntion Processing Systems 2020, NeurIPS 2020, De-\\ncember 6-12, 2020, virtual .\\nDaniel Fernando Campos, Tri Nguyen, Mir Rosenberg,\\nXia Song, Jianfeng Gao, Saurabh Tiwary, RanganMajumder, Li Deng, and Bhaskar Mitra. 2016. Ms\\nmarco: A human generated machine reading com-\\nprehension dataset. ArXiv , abs/1611.09268.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\\nRao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vin-\\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben-\\nton C. Hutchinson, Reiner Pope, James Bradbury, Ja-\\ncob Austin, Michael Isard, Guy Gur-Ari, Pengcheng\\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\\nGarcía, Vedant Misra, Kevin Robinson, Liam Fe-\\ndus, Denny Zhou, Daphne Ippolito, David Luan,\\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\\nErica Moreira, Rewon Child, Oleksandr Polozov,\\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\\nnan Saeta, Mark Díaz, Orhan Firat, Michele Catasta,\\nJason Wei, Kathleen S. Meier-Hellstern, Douglas\\nEck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.\\nPalm: Scaling language modeling with pathways.\\nArXiv , abs/2204.02311.\\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\\nCampos, and Ellen M V oorhees. 2020a. Overview\\nof the trec 2019 deep learning track. ArXiv preprint ,\\nabs/2003.07820.\\nNick Craswell, Bhaskar Mitra, Emine Yilmaz,\\nDaniel Fernando Campos, and Ellen M. V oorhees.\\n2020b. Overview of the trec 2020 deep learning\\ntrack. ArXiv , abs/2003.07820.\\nThibault Formal, Benjamin Piwowarski, and Stéphane\\nClinchant. 2021. Splade: Sparse lexical and expan-\\nsion model for ﬁrst stage ranking. Proceedings of\\nthe 44th International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval .\\nLuyu Gao and Jamie Callan. 2021. Condenser: a pre-\\ntraining architecture for dense retrieval. In Proceed-\\nings of the 2021 Conference on Empirical Methods\\nin Natural Language Processing , pages 981–993,\\nOnline and Punta Cana, Dominican Republic. Asso-\\nciation for Computational Linguistics.\\nLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie\\nCallan. 2022. Precise zero-shot dense retrieval with-\\nout relevance labels. ArXiv , abs/2212.10496.\\nFaegheh Hasibi, Fedor Nikolaev, Chenyan Xiong,\\nKrisztian Balog, Svein Erik Bratsberg, Alexander\\nKotov, and Jamie Callan. 2017. Dbpedia-entity v2:\\nA test collection for entity search. In Proceedings\\nof the 40th International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval,\\nShinjuku, Tokyo, Japan, August 7-11, 2017 , pages\\n1265–1268. ACM.Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Nat-\\nural Language Processing (EMNLP) , pages 6769–\\n6781, Online. Association for Computational Lin-\\nguistics.\\nVictor Lavrenko and W. Bruce Croft. 2001. Relevance-\\nbased language models. ACM SIGIR Forum , 51:260\\n– 267.\\nJimmy J. Lin and Xueguang Ma. 2021. A few brief\\nnotes on deepimpact, coil, and a conceptual frame-\\nwork for information retrieval techniques. ArXiv ,\\nabs/2106.14807.\\nJimmy J. Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-\\nHong Yang, Ronak Pradeep, Rodrigo Nogueira, and\\nDavid R. Cheriton. 2021. Pyserini: A python toolkit\\nfor reproducible information retrieval research with\\nsparse and dense representations. Proceedings of the\\n44th International ACM SIGIR Conference on Re-\\nsearch and Development in Information Retrieval .\\nYuanhua Lv and ChengXiang Zhai. 2009. A compara-\\ntive study of methods for estimating query language\\nmodels with pseudo feedback. Proceedings of the\\n18th ACM conference on Information and knowl-\\nedge management .\\nGeorge A. Miller. 1992. WordNet: A lexical database\\nfor English. In Speech and Natural Language: Pro-\\nceedings of a Workshop Held at Harriman, New\\nYork, February 23-26, 1992 .\\nRodrigo Nogueira and Jimmy Lin. From doc2query to\\ndoctttttquery.\\nRodrigo Nogueira, Wei Yang, Jimmy J. Lin, and\\nKyunghyun Cho. 2019. Document expansion by\\nquery prediction. ArXiv , abs/1904.08375.\\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,\\nand Haifeng Wang. 2021. RocketQA: An opti-\\nmized training approach to dense passage retrieval\\nfor open-domain question answering. In Proceed-\\nings of the 2021 Conference of the North Ameri-\\ncan Chapter of the Association for Computational\\nLinguistics: Human Language Technologies , pages\\n5835–5847, Online. Association for Computational\\nLinguistics.\\nRuiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao,\\nQiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong\\nWen. 2021. RocketQAv2: A joint training method\\nfor dense passage retrieval and passage re-ranking.\\nInProceedings of the 2021 Conference on Empiri-\\ncal Methods in Natural Language Processing , pages\\n2825–2835, Online and Punta Cana, Dominican Re-\\npublic. Association for Computational Linguistics.\\nJ. J. Rocchio. 1971. Relevance feedback in information\\nretrieval.Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and\\nDenny Zhou. 2022. Recitation-augmented language\\nmodels. ArXiv , abs/2210.01296.\\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\\nhishek Srivastava, and Iryna Gurevych. 2021. Beir:\\nA heterogeneous benchmark for zero-shot evalua-\\ntion of information retrieval models. In Thirty-ﬁfth\\nConference on Neural Information Processing Sys-\\ntems Datasets and Benchmarks Track (Round 2) .\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro,\\nFaisal Azhar, Aur’elien Rodriguez, Armand Joulin,\\nEdouard Grave, and Guillaume Lample. 2023.\\nLlama: Open and efﬁcient foundation language mod-\\nels.ArXiv , abs/2302.13971.\\nEllen V oorhees, Tasmeer Alam, Steven Bedrick, Dina\\nDemner-Fushman, William R Hersh, Kyle Lo, Kirk\\nRoberts, Ian Soboroff, and Lucy Lu Wang. 2021.\\nTrec-covid: constructing a pandemic information re-\\ntrieval test collection. In ACM SIGIR Forum , vol-\\nume 54, pages 1–12. ACM New York, NY , USA.\\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\\nWang, Madeleine van Zuylen, Arman Cohan, and\\nHannaneh Hajishirzi. 2020. Fact or ﬁction: Verify-\\ning scientiﬁc claims. In Proceedings of the 2020\\nConference on Empirical Methods in Natural Lan-\\nguage Processing (EMNLP) , pages 7534–7550, On-\\nline. Association for Computational Linguistics.\\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao,\\nLinjun Yang, Daxin Jiang, Rangan Majumder, and\\nFuru Wei. 2022a. Simlm: Pre-training with rep-\\nresentation bottleneck for dense passage retrieval.\\nArXiv , abs/2207.02578.\\nLiang Wang, Nan Yang, Xiaolong Huang, Binx-\\ning Jiao, Linjun Yang, Daxin Jiang, Rangan Ma-\\njumder, and Furu Wei. 2022b. Text embeddings by\\nweakly-supervised contrastive pre-training. ArXiv ,\\nabs/2212.03533.\\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\\nArnold Overwijk. 2021. Approximate nearest neigh-\\nbor negative contrastive learning for dense text re-\\ntrieval. In 9th International Conference on Learning\\nRepresentations, ICLR 2021, Virtual Event, Austria,\\nMay 3-7, 2021 . OpenReview.net.\\nW. Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingx-\\nuan Ju, Soumya Sanyal, Chenguang Zhu, Michael\\nZeng, and Meng Jiang. 2022. Generate rather than\\nretrieve: Large language models are strong context\\ngenerators. ArXiv , abs/2209.10063.\\nHang Zhang, Yeyun Gong, Yelong Shen, Jiancheng\\nLv, Nan Duan, and Weizhu Chen. 2021. Adversar-\\nial retriever-ranker for dense text retrieval. ArXiv\\npreprint , abs/2110.03611.query who sings hey good looking\\nLLM generation Hey Good Lookin’ was originally written and recorded by the country music singer Hank\\nWilliams in1951 . The song has since been covered by several other artists and bands,\\nincluding Jimmy Buffett, Conway Twitty, and Willie Nelson.\\nGroundtruth Hey, Good Lookin’. (1951) Hey, Good Lookin’ is a 1951 song written and recorded byHank\\nWilliams , and his version was inducted into the Grammy Hall of Fame in 2001. Since its\\noriginal 1951 recording it has been covered by a variety of artists .\\nquery trumbull marriott fax number\\nLLM generation The fax number for the Trumbull Marriott Shelton is 203-378 -4444 .\\nGroundtruth Business name: Trumbull Marriott Merritt Parkway; Address: 180 Hawley Lane Trumbull,\\nConnecticut 06611; Phone number: 203-378-4958; Fax number: 203-378 -1400; Business\\nhours: 24; Credit cards accepted: Yes; Number of employees: 10-19; Map:\\nquery what is hra and hsa\\nLLM generation HRA stands for Health Reimbursement Arrangement. It is a type of beneﬁt plan where an\\nemployer reimburses employees for qualifying medical expenses . It also allows employers\\nto set aside money tax-free to reimburse employees for medical expenses that are not covered\\nunder the employer’s health insurance plan. HSA stands for Health Savings Account. It is\\na savings account that is used in conjunction with a qualiﬁed High Deductible Health Plan\\n(HDHP) . An HSA enables an individual to pay for current health expenses and save for future\\nhealth expenses on a tax-free basis. Contributions to the account are tax\\nGroundtruth The key difference between HRA and HSA is that HRA is an employer funded health beneﬁt\\nplan that reimburses formedical expenses including personal health insurance policy pre-\\nmiums of employees whereas HSA is also a tax-advantaged health beneﬁt plan exclusively\\navailable to taxpayers in the United States who are enrolled in a High-Deductible Health Plan\\n(HDHP) .\\nTable 7: More examples of LLM generations. The format is the same as in Table 5.\\nA Implementation Details\\nFor dense retrieval experiments in Table 1, we\\nlist the hyperparameters in Table 8. When\\ntraining dense retrievers with distillation from\\ncross-encoder, we use the same teacher score\\nreleased by Wang et al.. The SimLM and\\nE5 checkpoints for initialization are pub-\\nlicly available at https://huggingface.\\nco/intfloat/simlm-base-msmarco and\\nhttps://huggingface.co/intfloat/\\ne5-base-unsupervised . To compute the\\ntext embeddings, we utilize the [CLS] vector for\\nSimLM and mean pooling for E5. This makes sure\\nthat the pooling mechanisms remain consistent\\nbetween intermediate pre-training and ﬁne-tuning.\\nWhen prompting LLMs, we include 4 in-context\\nexamples from the MS-MARCO training set. To\\nincrease prompt diversity, we randomly select 4\\nexamples for each API call. A complete prompt is\\nshown in Table 9.\\nRegarding out-of-domain evaluations on DBpe-\\ndia (Hasibi et al., 2017), NFCorpus (Boteva et al.,\\n2016), Scifact (Wadden et al., 2020), Trec-Covid\\n(V oorhees et al., 2021), and Touche2020 (Bon-\\ndarenko et al., 2022), SimLM’s results are based\\non the released checkpoint by Wang et al..DPR w/ distillation\\nlearning rate 2\\x0210\\x0053\\x0210\\x005\\nPLM BERT base SimLM / E5 base-unsup\\n# of GPUs 4 4\\nwarmup steps 1000 1000\\nbatch size 64 64\\nepoch 3 6\\n\\x0b n.a. 0.2\\nnegatives depth 1000 200\\nquery length 144 144\\npassage length 144 144\\n# of negatives 15 23\\nTable 8: Hyper-parameters for training dense retrievers\\non MS-MARCO passage ranking dataset.\\nFor ablation experiments in Figure 2, we ﬁne-\\ntune for 40 epochs or 18k steps, whichever is\\nreached ﬁrst.promptsWrite a passage that answers the given query:\\nQuery: what state is this zip code 85282\\nPassage: Welcome to TEMPE, AZ 85282. 85282 is a rural zip code in Tempe, Arizona. The population\\nis primarily white, and mostly single. At $200,200 the average home value here is a bit higher than\\naverage for the Phoenix-Mesa-Scottsdale metro area, so this probably isn’t the place to look for housing\\nbargains.5282 Zip code is located in the Mountain time zone at 33 degrees latitude (Fun Fact: this is the\\nsame latitude as Damascus, Syria!) and -112 degrees longitude.\\nQuery: why is gibbs model of reﬂection good\\nPassage: In this reﬂection, I am going to use Gibbs (1988) Reﬂective Cycle. This model is a recognised\\nframework for my reﬂection. Gibbs (1988) consists of six stages to complete one cycle which is able\\nto improve my nursing practice continuously and learning from the experience for better practice in the\\nfuture.n conclusion of my reﬂective assignment, I mention the model that I chose, Gibbs (1988) Reﬂective\\nCycle as my framework of my reﬂective. I state the reasons why I am choosing the model as well as some\\ndiscussion on the important of doing reﬂection in nursing practice.\\nQuery: what does a thousand pardons means\\nPassage: Oh, that’s all right, that’s all right, give us a rest; never mind about the direction, hang the\\ndirection - I beg pardon, I beg a thousand pardons, I am not well to-day; pay no attention when I soliloquize,\\nit is an old habit, an old, bad habit, and hard to get rid of when one’s digestion is all disordered with eating\\nfood that was raised forever and ever before he was born; good land! a man can’t keep his functions\\nregular on spring chickens thirteen hundred years old.\\nQuery: what is a macro warning\\nPassage: Macro virus warning appears when no macros exist in the ﬁle in Word. When you open\\na Microsoft Word 2002 document or template, you may receive the following macro virus warning,\\neven though the document or template does not contain macros: C:\\\\<path>\\\\<ﬁle name>contains macros.\\nMacros may contain viruses.\\nQuery: when was pokemon green released\\nPassage:\\noutputPokemon Green was released in Japan on February 27th, 1996. It was the ﬁrst in the Pokemon series of\\ngames and served as the basis for Pokemon Red and Blue, which were released in the US in 1998. The\\noriginal Pokemon Green remains a beloved classic among fans of the series.\\nTable 9: The full prompt used for the example in Figure 1.Doc2Query--: When Less is More\\nMitko Gospodinov1, Sean MacAvaney2, and Craig Macdonald2\\nUniversity of Glasgow\\n12024810G@student.gla.ac.uk\\n2{first}.{last}@glasgow.ac.uk\\nAbstract. Doc2Query — the process of expanding the content of a\\ndocument before indexing using a sequence-to-sequence model — has\\nemerged as a prominent technique for improving the ﬁrst-stage retrieval\\neﬀectivenessofsearchengines.However,sequence-to-sequencemodelsare\\nknown to be prone to “hallucinating” content that is not present in the\\nsource text. We argue that Doc2Query is indeed prone to hallucination,\\nwhich ultimately harms retrieval eﬀectiveness and inﬂates the index size.\\nIn this work, we explore techniques for ﬁltering out these harmful queries\\nprior to indexing. We ﬁnd that using a relevance model to remove poor-\\nquality queries can improve the retrieval eﬀectiveness of Doc2Query by\\nup to 16%, while simultaneously reducing mean query execution time by\\n23% and cutting the index size by 33%. We release the code, data, and\\na live demonstration to facilitate reproduction and further exploration.1\\n1 Introduction\\nNeural network models, particularly those based on contextualised language\\nmodels, have been shown to improve search eﬀectiveness [3]. While some ap-\\nproaches focus on re-ranking document sets from a ﬁrst-stage retrieval function\\nto improve precision [27], others aim to improve the ﬁrst stage itself [4]. In this\\nwork, we focus on one of these ﬁrst-stage approaches: Doc2Query [29]. This ap-\\nproach trains a sequence-to-sequence model (e.g., T5 [33]) to predict queries that\\nmay be relevant to a particular text. Then, when indexing, this model is used\\ntoexpandthe document by generating a collection of queries and appending\\nthem to the document. Though computationally expensive at index time [34],\\nthis approach has been shown to be remarkably eﬀective even when retrieving\\nusing simple lexical models like BM25 [28]. Numerous works have shown that\\nthe approach can produce a high-quality pool of results that are eﬀective for\\nsubsequent stages in the ranking pipeline [19,20,23,40].\\nHowever, sequence-to-sequence models are well-known to be prone to gener-\\nate content that does not reﬂect the input text – a defect known in literature\\nas “hallucination” [25]. We ﬁnd that existing Doc2Query models are no excep-\\ntion. Figure 1 provides example generated queries from the state-of-the-art T5\\nDoc2Query model [28]. In this example, we see that many of the generated\\nqueries cannot actually be answered by the source passage (score \\x141).\\n1https://github.com/terrierteam/pyterrier_doc2queryarXiv:2301.03266v3  [cs.IR]  27 Feb 2023\\n2 Gospodinov et al.\\nOriginal Passage: Barley (Hordeum vulgare L.), a\\nmember of the grass family, is a major cereal grain. It\\nwas one of the ﬁrst cultivated grains and is now grown\\nwidely. Barley grain is a staple in Tibetan cuisine and\\nwaseatenwidelybypeasantsinMedievalEurope.Bar-\\nley has also been used as animal fodder, as a source\\nof fermentable material for beer and certain distilled\\nbeverages,andasacomponentofvarioushealthfoods.Generated Queries: (1) where does barley originate\\nfrom\\x01(2) what is the name of the cereal grain used\\nin tibetan cooking? \\x01(3) what is barley used for \\x01(1)\\nwhat is barley in food \\x01(0) what is bare wheat \\x01(3)\\nwhat family of organisms is barley in \\x01(1) why is bar-\\nley important in tibetan diet \\x01(3) what is barley \\x01\\n(2) where is barley grown \\x01(1) where was barley ﬁrst\\ngrown and eaten \\x01(1) where was barley ﬁrst used ...\\nFig. 1.Example passage from MS MARCO and generated queries using the T5\\nDoc2Query model. The relevance of each query to the passage is scored by the au-\\nthors on a scale of 0–3 using the TREC Deep Learning passage relevance criteria.\\nBased on this observation, we hypothesise that retrieval performance of\\nDoc2Querywouldimproveifhallucinatedquerieswereremoved.Inthispaper,we\\nconduct experiments where we apply a new ﬁltering phase that aims to remove\\npoor queries prior to indexing. Given that this approach removes queries, we\\ncall the approach Doc2Query-- (Doc2Query-minus-minus). Rather than training\\na new model for this task, we identify that relevance models are already ﬁt for\\nthis purpose: they estimate how relevant a passage is to a query. We therefore\\nexplore ﬁltering strategies that make use of existing neural relevance models.\\nThrough experimentation on the MS MARCO dataset, we ﬁnd that our ﬁl-\\ntering approach can improve the retrieval eﬀectiveness of indexes built using\\nDoc2Query-- by up to 16%; less can indeed be more. Meanwhile, ﬁltering nat-\\nurally reduces the index size, lowering storage and query-time computational\\ncosts. Finally, we conduct an exploration of the index-time overheads introduced\\nbytheﬁlteringprocessandconcludethatthegainsfromﬁlteringmorethanmake\\nup for the additional time spent generating more queries. The approach also has\\na positive impact on the environmental costs of applying Doc2Query; the same\\nretrieval eﬀectiveness can be achieved with only about a third of the compu-\\ntational cost when indexing. To facilitate last-metre, last-mile, and complete\\nreproduction eﬀorts [36], we release the code, indices, and ﬁltering scores.1In\\nsummary, we contribute a technique to improve the eﬀectiveness and eﬃciency\\nof Doc2Query by ﬁltering out queries that do not reﬂect the original passage.\\n2 Related Work\\nThe classical lexical mismatch problem is a key one in information retrieval -\\ndocuments that do not contain the query terms may not be retrieved. In the\\nliterature, various approaches have addressed this: query reformulation – includ-\\ning stemming, query expansion models (e.g. Rocchio, Bo1 [1], RM3 [12]) – and\\ndocument expansion [9,30,35]. Classically, query expansion models have been\\npopular, as they avoid the costs associated with making additional processing\\nfor each document needed for document expansion. However, query expansion\\nmay result in reduced performance [11], as queries are typically short and the\\nnecessary evidence to understand the context of the user is limited.\\nDoc2Query--: When Less is More 3\\nThe application of latent representations of queries and documents, such\\nas using latent semantic indexing [8] allow retrieval to not be driven directly\\nby lexical signals. More recently, transformer-based language models (such as\\nBERT [6]) have resulted in representations of text where the contextualised\\nmeaning of words are accounted for. In particular, in dense retrieval, queries\\nand documents are represented in embeddings spaces [14,37], often facilitated\\nby Approximate Nearest Neighbour (ANN) data structures [13]. However, even\\nwhen using ANN, retrieval can still be ineﬃcient or insuﬃciently eﬀective [15].\\nOthers have explored approaches for augmenting lexical representations with\\nadditional terms that may be relevant. In this work, we explore Doc2Query [29],\\nwhich uses a sequence-to-sequence model that maps a document to queries that\\nit might be able to answer. By appending these generated queries to a docu-\\nment’s content before indexing, the document is more likely to be retrieved for\\nuser queries when using a model like BM25. An alternative style of document\\nexpansion, proposed by MacAvaney et al. [19] and since used by several other\\nmodels (e.g., [10,39,40]), uses the built-in Masked Language Modelling (MLM)\\nmechanism. MLM expansion generates individual tokens to append to the docu-\\nment as a bag of words (rather than as a sequence). Although MLM expansion is\\nalso prone to hallucination,2the bag-of-words nature of MLM expansion means\\nthat individual expansion tokens may not have suﬃcient context to apply ﬁl-\\ntering eﬀectively. We therefore focus only on sequence-style expansion and leave\\nthe exploration of MLM expansion for future work.\\n3 Doc2Query--\\nDoc2Query-- consists of two phases: a generation phrase and a ﬁltering phase.\\nIn the generation phase, a Doc2Query model generates a set of nqueries that\\neach document might be able to answer. However, as shown in Figure 1, not\\nall of the queries are necessarily relevant to the document. To mitigate this\\nproblem, Doc2Query-- then proceeds to a ﬁltering phase, which is responsible\\nfor eliminating the generated queries that are least relevant to the source doc-\\nument. Because hallucinated queries contain details not present in the original\\ntext (by deﬁnition), we argue that hallucinated queries are less useful for re-\\ntrieval than non-hallucinated ones. Filtering is accomplished by retaining only\\nthe most relevant pproportion of generated queries over the entire corpus. The\\nretained queries are then concatenated to their corresponding documents prior\\nto indexing, as per the existing Doc2Query approach.\\nMore formally, consider an expansion function ethat maps a document to n\\nqueries: e:D7!Qn. In Doc2Query, each document in corpus Dare concate-\\nnatedwiththeirexpansionqueries,forminganewcorpus D0=fConcat (d; e(d))j\\nd2Dg,whichisthen indexedbya retrievalsystem.Doc2Query--addsaﬁltering\\nmechanism that uses a relevance model that maps a query and document to a\\nreal-valued relevance score s:Q\\x02D7!R(with larger values indicating higher\\n2For instance, we ﬁnd that SPLADE [10] generates the following seemingly-unrelated\\nterms for the passage in Figure 1 in the top 20 expansion terms: reed,herb, and troy.\\n4 Gospodinov et al.\\nrelevance). The relevance scoring function is used to ﬁlter down the queries to\\nthose that meet a certain score threshold tas follows:\\nD0=n\\nConcat\\x00\\nd;\\x08\\nqjq2e(d)^s(q; d)\\x15t\\t\\x01\\njd2Do\\n(1)\\nThe relevance threshold tis naturally dependent upon the relevance scoring\\nfunction. It can be set empirically, chosen based on operational criteria (e.g.,\\ntargetindexsize),or(forawell-calibratedrelevancescoringfunction)determined\\na priori. In this work, we combine the ﬁrst two strategies: we pick tbased on\\nthe distribution of relevance scores across all expansion queries. For instance,\\natp= 0:3we only keep queries with relevance scores in the top 30%, which is\\nt= 3:215for the ELECTRA [31] scoring model on the MS MARCO dataset [26].\\n4 Experimental Setup\\nWe conduct experiments to answer the following research questions:\\nRQ1 Does Doc2Query-- improve the eﬀectiveness of document expansion?\\nRQ2 What are the trade-oﬀs in terms of eﬀectiveness, eﬃciency, and storage when\\nusing Doc2Query--?\\nDatasets and Measures. We conduct tests using the MS MARCO [26] v1\\npassage corpus. We use ﬁve test collections:3(1) the MS MARCO Dev (small)\\ncollection, consisting of 6,980 queries (1.1 qrels/query); (2) the Dev2 collection,\\nconsisting of 4,281 (1.1 qrels/query); (3) the MS MARCO Eval set, consisting of\\n6,837 queries (held-out leaderboard set); (4/5) the TREC DL’19/’20 collections,\\nconsisting of 43/54 queries (215/211 qrels/query). We evaluate using the oﬃcial\\ntask evaluation measures: Reciprocal Rank at 10 (RR@10) for Dev/Dev2/Eval,\\nnDCG@10 for DL’19/’20. We tune systems4on Dev, leaving the remaining col-\\nlections as held-out test sets.\\nModels. We use the T5 Doc2Query model from Nogueira and Lin [28], mak-\\ning use of the inferred queries released by the authors (80 per passage). To the\\nbest of our knowledge, this is the highest-performing Doc2Query model avail-\\nable. We consider three neural relevance models for ﬁltering: ELECTRA5[31],\\nMonoT56[32],andTCT-ColBERT7[16],coveringtwostrongcross-encodermod-\\nels and one strong bi-encoder model. We also explored ﬁlters that use the prob-\\nabilities from the generation process itself but found them to be ineﬀective and\\ntherefore omit these results due to space constraints.\\nTools and Environment. WeusethePyTerriertoolkit[22]withaPISA[24,17]\\nindex to conduct our experiments. We deploy PISA’s Block-Max WAND [7] im-\\nplementation for BM25 retrieval. Inference was conducted on an NVIDIA 3090\\nGPU. Evaluation was conducted using the ir-measures package [18].\\n3ir-datasets [21] IDs: msmarco-passage/dev/small ,msmarco-passage/dev/2 ,\\nmsmarco-passage/eval/small , msmarco-passage/trec-dl-2019/judged ,\\nmsmarco-passage/trec-dl-2020/judged4BM25’s k1,b, and whether to\\nremove stopwords were tuned for all systems; the ﬁltering percentage ( p)\\nwas also tuned for ﬁltered systems.5crystina-z/monoELECTRA_LCE_nneg31\\n6castorini/monot5-base-msmarco7castorini/tct_colbert-v2-hnp-msmarco\\nDoc2Query--: When Less is More 5\\nTable 1. Eﬀectiveness and eﬃciency measurements for Doc2Query-- and baselines.\\nSigniﬁcant diﬀerences between Doc2Query and their corresponding ﬁltered versions\\nfor Dev, Dev2, DL’19 and DL’20 are indicated with * (paired t-test, p < 0:05). Values\\nmarked withyare taken from the corresponding submissions to the public leaderboard.\\nRR@10 nDCG@10 ms/q GB\\nSystem Dev Dev2 Eval DL’19 DL’20 MRT Index\\nBM25 0.185 0.182y0.186 0.499 0.479 5 0.71\\nDoc2Query ( n= 40) 0.277 0.265y0.272 0.626 0.607 30 1.17\\nw/ ELECTRA Filter (30%) *0.316 *0.310 -0.667 0.611 23 0.89\\nw/ MonoT5 Filter (40%) *0.308 *0.298 0.306 0.650 0.611 29 0.93\\nw/ TCT Filter (50%) *0.287 *0.280 - 0.640 0.599 30 0.94\\nDoc2Query ( n= 80) 0.279 0.267 - 0.627 0.605 30 1.41\\nw/ ELECTRA Filter (30%) *0.323 *0.316 0.325 0.670 0.614 23 0.95\\nw/ MonoT5 Filter (40%) *0.311 *0.298 - 0.665 0.609 28 1.04\\nw/ TCT Filter (50%) *0.293 *0.283 - 0.642 0.588 28 1.05\\n5 Results\\nWe ﬁrst explore RQ1: whether relevance ﬁltering can improve the retrieval of\\nDoc2Query models. Table 1 compares the eﬀectiveness of Doc2Query with var-\\nious ﬁlters. We observe that all the ﬁlters signiﬁcantly improve the retrieval\\neﬀectiveness on the Dev and Dev2 datasets at both n= 40andn= 80. We also\\nobserve a large boost in performance on the Eval dataset.8Though the diﬀer-\\nences in DL’19 and DL’20 appear to be considerable (e.g., 0.627 to 0.670), these\\ndiﬀerences are not statistically signiﬁcant.\\nDiggingalittledeeper,Figure2showstheretrievaleﬀectivenessofDoc2Query\\nwith various numbers of generated queries (in dotted black) and the correspond-\\ning performance when ﬁltering using the top-performing ELECTRA scorer (in\\nsolid blue). We observe that performing relevance ﬁltering at each value of n\\nimproves the retrieval eﬀectiveness. For instance, keeping only 30% of expan-\\nsion queries at n= 80, performance is increased from 0.279 to 0.323 – a 16%\\nimprovement.\\nIn aggregate, results from Table 1 and Figure 2 answer RQ1: Doc2Query--\\nﬁltering can signiﬁcantly improve the retrieval eﬀectiveness of Doc2Query across\\nvarious scoring models, numbers of generated queries ( n) and thresholds ( p).\\nNext,weexplorethetrade-oﬀsintermsofeﬀectiveness,eﬃciency,andstorage\\nwhen using Doc2Query--. Table 1 includes the mean response time and index\\nsizes for each of the settings. As expected, ﬁltering reduces the index size since\\nfewer terms are stored. For the best-performing setting ( n= 80with ELECTRA\\n8Signiﬁcance cannot be determined due to the held-out nature of the dataset. Further,\\ndue to restrictions on the number of submissions to the leaderboard, we only are able\\nto submit two runs. The ﬁrst aims to be a fair comparison with the existing Doc2Query\\nEval result, using the same number of generated queries and same base T5 model for\\nscoring. The second is our overall best-performing setting, using the ELECTRA ﬁlter\\natn= 80generated queries.\\n6 Gospodinov et al.\\n0 1 2 3 4 5\\nTotal Tokens 1e90.2250.2500.2750.3000.325RR@10\\n90%80%70%60%50%40% 30%\\nn=5n=10n=20n=40 n=80\\nGeneration PhaseFiltering Phase\\nFig. 2.Eﬀectiveness (RR@10) on the Dev set, compared with the total number of\\nindexed tokens. The generation phase is shown in dotted black (at various values of\\nn), and the ELECTRA ﬁltering phase is shown in solid blue (at various values of p).\\nﬁlter), this amounts to a 33% reduction in index size (1.41 GB down to 0.95 GB).\\nNaturally, such a reduction has an impact on query processing time as well; it\\nyields a 23% reduction in mean response time (30ms down to 23ms).\\nDoc2Query-- ﬁltering adds substantial cost an indexing time, mostly due to\\nscoring each of the generated queries. Table 2 reports the cost (in hours of GPU\\ntime) of the generation and ﬁltering phases. We observe that ELECTRA ﬁlter-\\ning can yield up to a 78% increase in GPU time ( n= 10). However, we ﬁnd that\\nthe improved eﬀectiveness makes up for this cost. To demonstrate this, we al-\\nlocate the time spent ﬁltering to generating additional queries for each passage.\\nFor instance, the 15 hours spent scoring n= 5queries could instead be spent\\ngenerating 6 more queries per passage (for a total of n= 11). We ﬁnd that when\\ncomparing against an unﬁltered nthat closely approximates the total time when\\nTable 2. Retrieval eﬀectiveness comparison for comparable indexing computational\\nbudgets (in hours of GPU time). Values of nwithout a ﬁlter are chosen to best approx-\\nimate the total compute hours or the Dev eﬀectiveness of the corresponding ﬁltered\\nversion. Signiﬁcant diﬀerences between in RR@10 performance are indicated with *\\n(paired t-test, p < 0:05).\\nGPU Hours RR@10\\nnFilter Gen+Filt=Tot Dev Dev2 Comment\\n5 ELECTRA 20 + 15 = 34 0.273 0.270\\n11None 34 + 0 = 34 *0.261 *0.256 \\x004% Dev RR for sim. GPU hrs\\n31None 99 + 0 = 99 0.273 0.265\\x022:9GPU hrs to match Dev RR\\n10 ELECTRA 32 + 25 = 57 0.292 0.292\\n18None 59 + 0 = 59 *0.270 *0.260 \\x008% Dev RR for sim. GPU hrs\\n20 ELECTRA 66 + 47 = 113 0.307 0.303\\n36None 113 + 0 = 113 *0.275 *0.265 \\x0010% Dev RR for sim. GPU hrs\\n40 ELECTRA 128 + 86 = 214 0.316 0.310\\n68None 216 + 0 = 216 *0.279 *0.267 \\x0012% Dev RR for sim. GPU hrs\\nDoc2Query--: When Less is More 7\\nﬁltering, the ﬁltered results consistently yield signiﬁcantly higher retrieval eﬀec-\\ntiveness. As the computational budget increases, so does the margin between\\nDoc2Query and Doc2Query--, from 4% at 34 hours up to 12% at 216 hours.\\nFrom the opposite perspective, Doc2Query consumes 2.9 \\x02or more GPU\\ntime than Doc2Query-- to achieve similar eﬀectiveness ( n= 13with no ﬁlter\\nvs.n= 5with ELECTRA ﬁlter). Since the eﬀectiveness of Doc2Query ﬂattens\\nout between n= 40andn= 80(as seen in Figure 2), it likely requires a\\nmassive amount of additional compute to reach the eﬀectiveness of Doc2Query--\\natn\\x1510, if that eﬀectiveness is achievable at all. These comparisons show that\\nif a deployment is targeting a certain level of eﬀectiveness (rather than a target\\ncompute budget), Doc2Query-- is also preferable to Doc2Query.\\nTheseresultscollectivelyanswerRQ2:Doc2Query--provideshighereﬀective-\\nness at lower query-time costs, even when controlling for the additional compute\\nrequired at index time.\\n6 Conclusions\\nThisworkdemonstratedthatthereareuntappedadvantagesingeneratingnatural-\\nlanguage for document expansion. Speciﬁcally, we presented Doc2Query--, which\\nisanewapproachforimprovingtheeﬀectivenessandeﬃciencyoftheDoc2Query\\nmodel by ﬁltering out the least relevant queries. We observed that a 16% im-\\nprovement in retrieval eﬀectiveness can be achieved, while reducing the index\\nsize by 33% and mean query execution time by 23%.\\nThe technique of ﬁltering text generated from language models using rel-\\nevance scoring is ripe for future work. For instance, relevance ﬁltering could\\npotentially apply to approaches that generate alternative forms of queries [38],\\ntraining data [2], or natural language responses to queries [5] — all of which\\nare potentially aﬀected by hallucinated content. Furthermore, future work could\\nexplore approaches for relevance ﬁltering over masked language modelling ex-\\npansion [19], rather than sequence-to-sequence expansion.\\nAcknowledgements\\nSeanMacAvaneyandCraigMacdonaldacknowledgeEPSRCgrantEP/R018634/1:\\nClosed-Loop Data Science for Complex, Computationally- & Data-Intensive An-\\nalytics.\\nReferences\\n1. Amati, G., Van Rijsbergen, C.J.: Probabilistic models of information retrieval\\nbased on measuring the divergence from randomness. ACM Trans. Inf. Syst. 20(4)\\n(2002)\\n2. Bonifacio,L.,Abonizio,H.,Fadaee,M.,Nogueira,R.:InPars:Unsuperviseddataset\\ngeneration for information retrieval. In: Proceedings of SIGIR (2022)\\n8 Gospodinov et al.\\n3. Dai, Z., Callan, J.: Deeper text understanding for IR with contextual neural lan-\\nguage modeling. In: Proceedings of SIGIR (2019)\\n4. Dai, Z., Callan, J.: Context-aware document term weighting for ad-hoc search. In:\\nProceedings of The Web Conference (2020)\\n5. Das, R., Dhuliawala, S., Zaheer, M., McCallum, A.: Multi-step retriever-reader\\ninteraction for scalable open-domain question answering. In: Proceedings of ICLR\\n(2019)\\n6. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep\\nbidirectional transformers for language understanding. In: Proceedings of NAACL-\\nHLT (2019)\\n7. Ding, S., Suel, T.: Faster top-k document retrieval using block-max indexes. In:\\nProceedings of SIGIR (2011)\\n8. Dumais, S.T., Furnas, G.W., Landauer, T.K., Deerwester, S., Harshman, R.: Using\\nlatent semantic analysis to improve access to textual information. In: Proceedings\\nof SIGCHI CHI (1988)\\n9. Efron, M., Organisciak, P., Fenlon, K.: Improving retrieval of short texts through\\ndocument expansion. In: Proceedings of SIGIR (2012)\\n10. Formal, T., Piwowarski, B., Clinchant, S.: SPLADE: Sparse lexical and expansion\\nmodel for ﬁrst stage ranking. In: Proceedings of SIGIR (2021)\\n11. He, B., Ounis, I.: Studying query expansion eﬀectiveness. In: Proceedings of ECIR\\n(2009)\\n12. Jaleel, N.A., Allan, J., Croft, W.B., Diaz, F., Larkey, L.S., Li, X., Smucker, M.D.,\\nWade, C.: Umass at TREC 2004: Novelty and HARD. In: TREC (2004)\\n13. Johnson, J., Douze, M., Jegou, H.: Billion-scale similarity search with GPUs. IEEE\\nTransactions on Big Data 7(03) (2021)\\n14. Khattab, O., Zaharia, M.: ColBERT: Eﬃcient and eﬀective passage search via\\ncontextualized late interaction over BERT. In: Proceedings of SIGIR (2020)\\n15. Lin, J., Ma, X., Mackenzie, J., Mallia, A.: On the separation of logical and physical\\nranking models for text retrieval applications. In: Proceedings of DESIRES (2021)\\n16. Lin, S.C., Yang, J.H., Lin, J.: In-batch negatives for knowledge distillation with\\ntightly-coupled teachers for dense retrieval. In: Proceedings of RepL4NLP (2021)\\n17. MacAvaney, S., Macdonald, C.: A Python interface to PISA! In: Proceedings of\\nSIGIR (2022)\\n18. MacAvaney,S.,Macdonald,C.,Ounis,I.:Streamliningevaluationwithir-measures.\\nIn: Proceedings of ECIR (2022)\\n19. MacAvaney, S., Nardini, F.M., Perego, R., Tonellotto, N., Goharian, N., Frieder,\\nO.: Expansion via prediction of importance with contextualization. In: Proceedings\\nof SIGIR (2020)\\n20. MacAvaney, S., Tonellotto, N., Macdonald, C.: Adaptive re-ranking with a corpus\\ngraph. In: Proceedings of CIKM (2022)\\n21. MacAvaney, S., Yates, A., Feldman, S., Downey, D., Cohan, A., Goharian, N.:\\nSimpliﬁed data wrangling with ir_datasets. In: Proceedings of SIGIR (2021)\\n22. Macdonald, C., Tonellotto, N.: Declarative experimentation in information re-\\ntrieval using PyTerrier. In: Proceedings of ICTIR (2020)\\n23. Mallia, A., Khattab, O., Suel, T., Tonellotto, N.: Learning passage impacts for\\ninverted indexes. In: Proceedings of SIGIR (2021)\\n24. Mallia, A., Siedlaczek, M., Mackenzie, J., Suel, T.: PISA: performant indexes and\\nsearch for academia. In: Proceedings of OSIRRC@SIGIR (2019)\\n25. Maynez, J., Narayan, S., Bohnet, B., McDonald, R.: On faithfulness and factuality\\nin abstractive summarization. In: Proceedings of ACL (2020)Doc2Query--: When Less is More 9\\n26. Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., Deng,\\nL.: MS MARCO: A human generated machine reading comprehension dataset. In:\\nProceedings of CoCo@NIPS (2016)\\n27. Nogueira, R., Cho, K.: Passage re-ranking with BERT. ArXiv abs/1901.04085\\n(2019)\\n28. Nogueira, R., Lin, J.: From doc2query to doctttttquery (2019)\\n29. Nogueira, R., Yang, W., Lin, J.J., Cho, K.: Document expansion by query predic-\\ntion. ArXiv abs/1904.08375 (2019)\\n30. Pickens, J., Cooper, M., Golovchinsky, G.: Reverted indexing for feedback and\\nexpansion. In: Proceedings of CIKM (2010)\\n31. Pradeep, R., Liu, Y., Zhang, X., Li, Y., Yates, A., Lin, J.: Squeezing water from a\\nstone: A bag of tricks for further improving cross-encoder eﬀectiveness for rerank-\\ning. In: Proceedings of ECIR (2022)\\n32. Pradeep, R., Nogueira, R., Lin, J.: The expando-mono-duo design pattern for text\\nranking with pretrained sequence-to-sequence models. ArXiv abs/2101.05667\\n(2021)\\n33. Raﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y.,\\nLi, W., Liu, P.J., et al.: Exploring the limits of transfer learning with a uniﬁed\\ntext-to-text transformer. J. Mach. Learn. Res. 21(140) (2020)\\n34. Scells, H., Zhuang, S., Zuccon, G.: Reduce, reuse, recycle: Green information re-\\ntrieval research. In: Proceedings of SIGIR (2022)\\n35. Tao, T., Wang, X., Mei, Q., Zhai, C.: Language model information retrieval with\\ndocument expansion. In: Proceedings of HLT-NAACL (2006)\\n36. Wang, X., MacAvaney, S., Macdonald, C., Ounis, I.: An inspection of the repro-\\nducibility and replicability of TCT-ColBERT. In: Proceedings of SIGIR (2022)\\n37. Xiong, L., Xiong, C., Li, Y., Tang, K.F., Liu, J., Bennett, P.N., Ahmed, J., Over-\\nwijk,A.:Approximatenearestneighbornegativecontrastivelearningfordensetext\\nretrieval. In: Proceedings of ICLR (2021)\\n38. Yu, S.Y., Liu, J., Yang, J., Xiong, C., Bennett, P.N., Gao, J., Liu, Z.: Few-shot\\ngenerative conversational query rewriting. In: Proceedings of SIGIR (2020)\\n39. Zhao, T., Lu, X., Lee, K.: SPARTA: Eﬃcient open-domain question answering via\\nsparse transformer matching retrieval. arXiv abs/2009.13013 (2020)\\n40. Zhuang, S., Zuccon, G.: TILDE: Term independent likelihood model for passage\\nre-ranking. In: Proceedings of SIGIR (2021)A Few Brief Notes on DeepImpact, COIL, and a Conceptual\\nFramework for Information Retrieval Techniques\\nJimmy Lin andXueguang Ma\\nDavid R. Cheriton School of Computer Science\\nUniversity of Waterloo\\nAbstract\\nRecent developments in representational learn-\\ning for information retrieval can be organized\\nin a conceptual framework that establishes two\\npairs of contrasts: sparse vs. dense representa-\\ntions and unsupervised vs. learned representa-\\ntions. Sparse learned representations can fur-\\nther be decomposed into expansion and term\\nweighting components. This framework al-\\nlows us to understand the relationship between\\nrecently proposed techniques such as DPR,\\nANCE, DeepCT, DeepImpact, and COIL, and\\nfurthermore, gaps revealed by our analysis\\npoint to “low hanging fruit” in terms of tech-\\nniques that have yet to be explored. We present\\na novel technique dubbed “uniCOIL”, a simple\\nextension of COIL that achieves to our knowl-\\nedge the current state-of-the-art in sparse re-\\ntrieval on the popular MS MARCO passage\\nranking dataset. Our implementation using\\nthe Anserini IR toolkit is built on the Lucene\\nsearch library and thus fully compatible with\\nstandard inverted indexes.\\n1 Introduction\\nWe present a novel conceptual framework for un-\\nderstanding recent developments in information re-\\ntrieval that organizes techniques along two dimen-\\nsions. The ﬁrst dimension establishes the contrast\\nbetween sparse and dense vector representations\\nfor queries and documents.1The second dimen-\\nsion establishes the contrast between unsupervised\\nand learned (supervised) representations. Figure 1\\nillustrates our framework.\\nRecent proposals for dense retrieval, exempliﬁed\\nby DPR (Karpukhin et al., 2020) and ANCE (Xiong\\net al., 2021), but also encompassing many other\\ntechniques (Gao et al., 2021b; Hofstätter et al.,\\n2020; Qu et al., 2021; Hofstätter et al., 2021; Lin\\n1Consistent with parlance in information retrieval, we use\\n“document” throughout this paper in a generic sense to refer to\\nthe unit of retrieved text. To be more precise, our experiments\\nare in fact focused on passage retrieval.Dense Sparse\\nSupervised DPR, ANCE DeepImpact, COIL\\nUnsupervised LSI, LDA BM25, tf–idf\\nTable 1: Our conceptual framework for organizing re-\\ncent developments in information retrieval.\\net al., 2021), can be understood as learned dense\\nrepresentations for retrieval. This is formulated\\nas a representational learning problem where the\\ntask is to learn (transformer-based) encoders that\\nmap queries and documents into dense ﬁxed-width\\nvectors (768 dimensions is typical) in which inner\\nproducts between queries and relevant documents\\nare maximized, based on supervision signals from\\na large dataset such as the MS MARCO passage\\nranking test collection (Bajaj et al., 2018). See Lin\\net al. (2020) for a survey.\\nDense retrieval techniques are typically com-\\npared against a bag-of-words exact match ranking\\nmodel such as BM25, which in this context can be\\nunderstood as unsupervised sparse retrieval. Al-\\nthough it may be unnatural to describe BM25 in\\nthis way, it is technically accurate: each document\\nis represented by a sparse vector where each dimen-\\nsion corresponds to a unique term in the vocabulary,\\nand the scoring function assigns a weight to each di-\\nmension. As with dense retrieval, query–document\\nscores are computed via inner products.\\nWhat about learned sparse retrieval? The most\\nprominent recent example of this in the literature\\nis DeepCT (Dai and Callan, 2019), which uses\\na transformer to learn term weights based on a re-\\ngression model, with the supervision signal coming\\nfrom the MS MARCO passage ranking test collec-\\ntion.2DeepCT has an interesting “quirk”: in truth,\\nit only learns the term frequency (tf) component\\nof term weights, but still relies on the remaining\\n2Learning sparse representations is by no means a new idea.\\nThe earliest example we are aware of is Wilbur (2001), who\\nattempted to learn global term weights using TREC data, but\\nthe idea likely dates back even further.arXiv:2106.14807v1  [cs.IR]  28 Jun 2021parts of the BM25 scoring function via the gen-\\neration of pseudo-documents. This approach also\\nhas a weakness: it only assigns weights to terms\\nthat are already present in the document, which\\nlimits retrieval to exact match. This is an impor-\\ntant limitation that is addressed by the use of dense\\nrepresentations, which are capable of capturing se-\\nmantic matches.\\nThese two issues were resolved by the recently\\nproposed DeepImpact model (Mallia et al., 2021),\\nwhich also belongs in the family of learned sparse\\nrepresentations. DeepImpact brought together two\\nkey ideas: the use of document expansion to iden-\\ntify dimensions in the sparse vector that should\\nhave non-zero weights and a term weighting model\\nbased on a pairwise loss between relevant and non-\\nrelevant texts with respect to a query. Expansion\\nterms were identiﬁed by doc2query–T5 (Nogueira\\nand Lin, 2019), a sequence-to-sequence model for\\ndocument expansion that predicts queries for which\\na text would be relevant. Since the DeepImpact\\nscoring model directly predicts term weights that\\nare then quantized, it would be more accurate to\\ncall these weights learned impacts, since query–\\ndocument scores are simply the sum of weights of\\ndocument terms that are found in the query. Calling\\nthese impact scores draws an explicit connection to\\na thread of research in information retrieval dating\\nback two decades (Anh et al., 2001).\\nThe recently proposed COIL architecture (Gao\\net al., 2021a) presents an interesting case for this\\nconceptual framework. Where does it belong? The\\nauthors themselves describe COIL as “a new ex-\\nact lexical match retrieval architecture armed with\\ndeep LM representations”. COIL produces repre-\\nsentations for each document token that are then\\ndirectly stored in the inverted index, where the\\nterm frequency usually goes in an inverted list.\\nAlthough COIL is perhaps best described as the\\nintellectual descendant of ColBERT (Khattab and\\nZaharia, 2020), another way to think about it within\\nour conceptual framework is that instead of assign-\\ningscalar weights to terms in a query, the “scoring”\\nmodel assigns each term a vector “weight”. Query\\nevaluation in COIL involves accumulating inner\\nproducts instead of scalar weights.\\nOur conceptual framework highlights a ﬁnal\\nclass of techniques: unsupervised dense represen-\\ntations. While there is little work in this space of\\nlate, it does describe techniques such as LSI (Deer-\\nwester et al., 1990; Atreya and Elkan, 2010) andLDA (Wei and Croft, 2006), which have been previ-\\nously explored. Thus, all quadrants in our proposed\\nconceptual framework are populated with known\\nexamples from the literature.\\n2 Comments and Observations\\nBased on this framework, we can make a number of\\ninteresting observations that highlight obvious next\\nsteps in the development of retrieval techniques.\\nWe discuss as follows:\\nChoice of bases. Retrieval techniques using learned\\ndense representations and learned sparse represen-\\ntations present an interesting contrast. Nearly all\\nrecent proposals take advantage of transformers, so\\nthat aspect of the design is not a salient difference.\\nThe critical contrast is the basis of the vector rep-\\nresentations: In sparse approaches, the basis of the\\nvector space remains ﬁxed to the corpus vocabulary,\\nand thus techniques such as DeepCT, COIL, and\\nDeepImpact can be understood as term weighting\\nmodels. In dense approaches, the model is given\\nthe freedom to choose a new basis derived from\\ntransformer representations. This change in basis\\nallows the encoder to represent the “meaning” of\\ntexts in relatively small ﬁxed-width vectors (com-\\npared to sparse vectors that may have millions of\\ndimensions). This leads us to the next important\\nobservation:\\nExpansions for sparse representation. Without\\nsome form of expansion, learned sparse represen-\\ntations remain limited to (better) exact matching\\nbetween queries and documents. The nature of\\nsparse representations means that it is impractical\\nto consider non-zero weights for allelements in\\nthe vector (i.e., the vocabulary space). Thus, docu-\\nment expansion serves the critical role of proposing\\na set of candidate terms that should receive non-\\nzero weights; since the number of candidate terms\\nis small compared to the vocabulary size, the re-\\nsulting vector remains sparse. Without expansion,\\nlearned sparse representations cannot address the\\nvocabulary mismatch problem (Furnas et al., 1987),\\nbecause document terms not present in the query\\ncannot contribute any score. For DeepImpact, this\\nexpansion is performed by doc2query–T5, but in\\nprinciple we can imagine other methods also. This\\nleads us to the next important observation:\\nRelating DeepCT, DeepImpact, and COIL. The up-\\nshot of the above analysis is that retrieval tech-\\nniques based on learned sparse representations\\nshould be divided into an expansion model andSparse Representations MRR@10 Notes\\nTerm Weighting Expansion\\n(1a) BM25 None 0.184 copied from (Nogueira and Lin, 2019)\\n(1b) BM25 doc2query–T5 0.277 copied from (Nogueira and Lin, 2019)\\n(2a) DeepCT None 0.243 copied from (Dai and Callan, 2019)\\n(2b) DeepCT doc2query–T5 ? no publicly reported ﬁgure\\n(2c) DeepImpact None ? no publicly reported ﬁgure\\n(2d) DeepImpact doc2query–T5 0.326 copied from (Mallia et al., 2021)\\n(2e) COIL-tok ( d= 32 ) None 0.341 copied from (Gao et al., 2021a)\\n(2f) COIL-tok ( d= 32 ) doc2query–T5 0.361 our experiment\\n(2g) uniCOIL None 0.315 our experiment\\n(2h) uniCOIL doc2query–T5 0.352 our experiment\\nDense Representations MRR@10 Notes\\n(3a) ColBERT 0.360 copied from (Khattab and Zaharia, 2020)\\n(3b) ANCE 0.330 copied from (Xiong et al., 2021)\\n(3c) DistillBERT 0.323 copied from (Hofstätter et al., 2020)\\n(3d) RocketQA 0.370 copied from (Qu et al., 2021)\\n(3e) TAS-B 0.347 copied from (Hofstätter et al., 2021)\\n(3f) TCT-ColBERTv2 0.359 copied from (Lin et al., 2021)\\nDense–Sparse Hybrids MRR@10 Notes\\n(4a) CLEAR 0.338 copied from (Gao et al., 2021b)\\n(4b) COIL-full 0.355 copied from (Gao et al., 2021a)\\n(4c) TCT-ColBERTv2 + BM25 (1a) 0.369 copied from (Lin et al., 2021)\\n(4d) TCT-ColBERTv2 + doc2query–T5 (1b) 0.375 copied from (Lin et al., 2021)\\n(4e) TCT-ColBERTv2 + DeepImpact (2d) 0.378 our experiment\\n(4f) TCT-ColBERTv2 + uniCOIL (2h) 0.378 our experiment\\n(4g) TCT-ColBERTv2 + COIL (2f) 0.382 our experiment\\nTable 2: Results on the development queries of the MS MARCO passage ranking task.\\na term weighting model. For example, DeepCT\\nperforms no expansion and uses a regression-based\\nscoring model. DeepImpact performs document ex-\\npansion and uses a pairwise scoring model. COIL\\nperforms no expansion and uses a “scoring” model\\nthat generates a contextualized “weight vector” (in-\\nstead of a scalar weight). This breakdown suggests\\na number of obvious experiments that help us un-\\nderstand the contributions of these components,\\nwhich we report next.\\n3 Experiments\\nOur proposed conceptual framework can be used\\nto organize results from the literature, which are\\nshown in Table 2 on the development queries of\\nthe MS MARCO passage ranking task (Bajaj et al.,\\n2018). Some of these entries represent ﬁgures di-\\nrectly copied from previous papers (with references\\nshown), while others are novel experimental condi-\\ntions that we report.\\nThe ﬁrst main block of the table shows retrieval\\nwith sparse representations. Row (1a) shows the\\nBM25 baseline, and row (1b) provides the effective-\\nness of doc2query–T5 expansion. In both cases, the\\nterm weights are from the BM25 scoring function,and hence unsupervised. Learned sparse retrieval\\ntechniques are shown in row group (2). Separat-\\ning the term weighting component from the ex-\\npansion component allows us to identify gaps in\\nmodel conﬁgurations that would be interesting to\\nexplore. For example, in row (2a), DeepCT pro-\\nposed a regression-based term weighting model,\\nbut performed no expansion. However, the term\\nweighting model can be applied to expanded doc-\\numents, as in row (2b); to our knowledge, this\\nconﬁguration has not been publicly reported.\\nSimilarly, DeepImpact combined doc2query–T5\\nas an expansion model and a term weighting model\\ntrained with pairwise loss. To better understand\\nthe contributions of each component, we could\\nrun the term weighting model without document\\nexpansion, as outlined in row (2c). This ablation\\nexperiment was not reported in Mallia et al. (2021),\\nbut would be interesting to conduct.\\nIn row (2e) we report the published results of\\nCOIL-tok (token dimension d= 32 ), which is the\\nsparse component in the full COIL model (which\\nis a dense–sparse hybrid). Through the lens of\\nour conceptual framework, a number of extensions\\nbecome immediately obvious. COIL can be com-bined with doc2query–T5. Using source code pro-\\nvided by the authors,3we trained such a model\\nfrom scratch, using the same hyperparameters as\\nthe authors. This variant leads to a nearly two-point\\ngain in effectiveness, as shown in row (2f).\\nIn another interesting extension, if we reduce the\\ntoken dimension of COIL to one, the model degen-\\nerates into producing scalar weights, which then\\nbecomes directly comparable to DeepCT, row (2a)\\nand the “no-expansion” variant of DeepImpact, row\\n(2c). These comparisons isolate the effects of differ-\\nent term weighting models. We dub this variant of\\nCOIL “uniCOIL”, on top of which we can also add\\ndoc2query–T5, which produces a fair comparison\\nto DeepImpact, row (2d). The original formulation\\nof COIL, even with a token dimension of one, is\\nnot directly amenable to retrieval using inverted\\nindexes because weights can be negative. To ad-\\ndress this issue, we added a ReLU operation on\\nthe output term weights of the base COIL model to\\nforce the model to generate non-negative weights.\\nOnce again, we retrained the model from scratch\\nusing the same hyperparameters provided by the\\nauthors. When encoding the corpus, we quantized\\nthese weights into 8 bits to obtain impact scores;\\nquery weights are similarly quantized. After these\\nmodiﬁcations, uniCOIL is directly compatible with\\ninverted indexes. Our experimental results are re-\\nported with the Anserini toolkit (Yang et al., 2017,\\n2018), which is built on Lucene.\\nIt is no surprise that uniCOIL without doc2query–\\nT5, row (2g), is less effective than COIL-tok ( d=\\n32), row (2e). However, uniCOIL with doc2query–\\nT5, row (2h), outperforms COIL-tok without need-\\ning any specialized retrieval infrastructure—the\\nweights are just impact scores, like in DeepImpact.\\nThese results suggest that contextualized “weight\\nvectors” in COIL aren’t necessary to achieve good\\neffectiveness—adding expansion appears sufﬁcient\\nto make up for the lost expressivity of weight vec-\\ntors, as shown in row (2h) vs. row (2e). To our\\nknowledge, our uniCOIL model, row (2h), repre-\\nsents the state of the art in sparse retrieval using\\nlearned impact weights, beating DeepImpact by\\naround two points.\\nThe second main block of Table 2 provides a\\nnumber of comparable dense retrieval results from\\nthe literature. The highest score that we are aware\\nof is RocketQA (Qu et al., 2021), whose effective-\\nness beats all known sparse conﬁgurations. Note\\n3https://github.com/luyug/COILthat ColBERT (Khattab and Zaharia, 2020) uses\\nthe more expressive MaxSim operator to compare\\nquery and document representations; all other tech-\\nniques use inner products.\\nThe ﬁnal block of Table 2 presents the results of\\ndense–sparse hybrids. Lin et al. (2021) reported\\nthe results of dense–sparse hybrids when TCT-\\nColBERTv2, row (3f), is combined with BM25,\\nrow (1a), and doc2query–T5, row (1b). To this,\\nwe added fusion with DeepImpact, uniCOIL, and\\nCOIL-tok (d= 32 ). For a fair comparison, we fol-\\nlowed the same technique for combining dense and\\nsparse results as Lin et al. (2021), which is from Ma\\net al. (2021). For each query q, we used the corre-\\nsponding dense and sparse techniques to retrieve\\ntop-1k documents. The ﬁnal fusion score of each\\ndocument is calculated by sdense +\\x0b\\x01ssparse . Since\\nthe range of the two different scores are quite differ-\\nent, we ﬁrst normalized the scores into range(0, 1).\\nThe\\x0bwas tuned in the range(0, 2) with a simple\\nline search on a subset of the MS MARCO passage\\ntraining set.\\nWith these hybrid combinations, we are able\\nto achieve, to our knowledge, the highest reported\\nscores on the MS MARCO passage ranking task for\\nsingle-stage techniques (i.e., no reranking). Note\\nthat, as before, uniCOIL is compatible with stan-\\ndard inverted indexes, unlike COIL-tok, which re-\\nquires custom infrastructure.\\n4 Next Steps\\nIn most recent work, dense retrieval techniques are\\ncompared to BM25 and experiments show that they\\nhandily win. However, this is not a fair compari-\\nson, since BM25 is unsupervised, whereas dense\\nretrieval techniques exploit supervised relevance\\nsignals from large datasets. A more appropriate\\ncomparison would be between learned dense vs.\\nsparse representations—and there, no clear win-\\nner emerges at present. However, it seems clear\\nthat they are complementary, as hybrid approaches\\nappear to be more effective than either alone.\\nAn important point to make here is that neu-\\nral networks, particularly transformers, have not\\nmade sparse representations obsolete. Both dense\\nand sparse learned representations clearly exploit\\ntransformers—the trick is that the latter class of\\ntechniques then “projects” the learned knowledge\\nback into the sparse vocabulary space. This al-\\nlows us to reuse decades of innovation in inverted\\nindexes (e.g., integer coding techniques to com-press inverted lists) and efﬁcient query evaluation\\nalgorithms (e.g., smart skipping to reduce query\\nlatency): for example, the Lucene index used in\\nour uniCOIL experiments is only 1.3 GB, com-\\npared to \\x1840 GB for COIL-tok, 26 GB for TCT-\\nColBERTv2, and 154 GB for ColBERT. We note,\\nhowever, that with dense retrieval techniques, ﬁxed-\\nwidth vectors can be approximated with binary\\nhash codes, yielding far more compact representa-\\ntions with sacriﬁcing much effectiveness (Yamada\\net al., 2021). Once again, no clear winner emerges\\nat present.\\nThe complete design space of modern informa-\\ntion retrieval techniques requires proper accounting\\nof the tradeoffs between output quality (effective-\\nness), time (query latency), and space (index size).\\nHere, we have only focused on the ﬁrst aspect.\\nLearned representations for information retrieval\\nare clearly the future, but the advantages and dis-\\nadvantages of dense vs. sparse approaches along\\nthese dimensions are not yet fully understood. It’ll\\nbe exciting to see what comes next!\\n5 Acknowledgments\\nThis research was supported in part by the Canada\\nFirst Research Excellence Fund and the Natural Sci-\\nences and Engineering Research Council (NSERC)\\nof Canada. Computational resources were provided\\nby Compute Ontario and Compute Canada.\\nReferences\\nV o Ngoc Anh, Owen de Kretser, and Alistair Moffat.\\n2001. Vector-space ranking with effective early ter-\\nmination. In Proceedings of the 24th Annual Inter-\\nnational ACM SIGIR Conference on Research and\\nDevelopment in Information Retrieval (SIGIR 2001) ,\\npages 35–42, New Orleans, Louisiana.\\nAvinash Atreya and Charles Elkan. 2010. Latent se-\\nmantic indexing (LSI) fails for TREC collections.\\nSIGKDD Explorations , 12(2):5–10.\\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\\nMir Rosenberg, Xia Song, Alina Stoica, Saurabh Ti-\\nwary, and Tong Wang. 2018. MS MARCO: A Hu-\\nman Generated MAchine Reading COmprehension\\nDataset. arXiv:1611.09268v3 .\\nZhuyun Dai and Jamie Callan. 2019. Context-aware\\nsentence/passage term importance estimation for\\nﬁrst stage retrieval. arXiv:1910.10687 .\\nScott Deerwester, Susan T. Dumais, George W. Furnas,\\nThomas K. Landauer, and Richard Harshman. 1990.Indexing by latent semantic analysis. Journal of\\nthe Association for Information Science , 41(6):391–\\n407.\\nGeorge W. Furnas, Thomas K. Landauer, Louis M.\\nGomez, and Susan T. Dumais. 1987. The vo-\\ncabulary problem in human-system communication.\\nCommunications of the ACM , 30(11):964–971.\\nLuyu Gao, Zhuyun Dai, and Jamie Callan. 2021a.\\nCOIL: Revisit exact lexical match in information\\nretrieval with contextualized inverted list. In Pro-\\nceedings of the 2021 Conference of the North Amer-\\nican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies , pages\\n3030–3042.\\nLuyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Ben-\\njamin Van Durme, and Jamie Callan. 2021b. Com-\\nplementing lexical retrieval with semantic residual\\nembedding. In Proceedings of the 43rd European\\nConference on Information Retrieval (ECIR 2021),\\nPart I , pages 146–160.\\nSebastian Hofstätter, Sophia Althammer, Michael\\nSchröder, Mete Sertkan, and Allan Hanbury.\\n2020. Improving efﬁcient neural ranking mod-\\nels with cross-architecture knowledge distillation.\\narXiv:2010.02666 .\\nSebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong\\nYang, Jimmy Lin, and Allan Hanbury. 2021. Ef-\\nﬁciently teaching an effective dense retriever with\\nbalanced topic aware sampling. In Proceedings of\\nthe 44th Annual International ACM SIGIR Confer-\\nence on Research and Development in Information\\nRetrieval (SIGIR 2021) .\\nVladimir Karpukhin, Barlas O ˘guz, Sewon Min, Patrick\\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Nat-\\nural Language Processing (EMNLP) , pages 6769–\\n6781.\\nOmar Khattab and Matei Zaharia. 2020. ColBERT: Ef-\\nﬁcient and effective passage search via contextual-\\nized late interaction over BERT. In Proceedings of\\nthe 43rd International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval\\n(SIGIR 2020) , pages 39–48.\\nJimmy Lin, Rodrigo Nogueira, and Andrew Yates.\\n2020. Pretrained transformers for text ranking:\\nBERT and beyond. arXiv:2010.06467 .\\nSheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.\\n2021. In-batch negatives for knowledge distillation\\nwith tightly-coupled teachers for dense retrieval. In\\nProceedings of the 6th Workshop on Representation\\nLearning for NLP .\\nXueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy\\nLin. 2021. A replication study of dense passage re-\\ntriever. arXiv:2104.05740 .Antonio Mallia, Omar Khattab, Torsten Suel, and\\nNicola Tonellotto. 2021. Learning passage impacts\\nfor inverted indexes. In Proceedings of the 44th An-\\nnual International ACM SIGIR Conference on Re-\\nsearch and Development in Information Retrieval\\n(SIGIR 2021) .\\nRodrigo Nogueira and Jimmy Lin. 2019. From\\ndoc2query to docTTTTTquery.\\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,\\nand Haifeng Wang. 2021. RocketQA: An opti-\\nmized training approach to dense passage retrieval\\nfor open-domain question answering. In Proceed-\\nings of the 2021 Conference of the North Ameri-\\ncan Chapter of the Association for Computational\\nLinguistics: Human Language Technologies , pages\\n5835–5847.\\nXing Wei and W. Bruce Croft. 2006. LDA-based doc-\\nument models for ad-hoc retrieval. In Proceedings\\nof the 29th Annual International ACM SIGIR Con-\\nference on Research and Development in Informa-\\ntion Retrieval (SIGIR 2006) , pages 178–185, Seattle,\\nWashington.\\nW. John Wilbur. 2001. Global term weights for docu-\\nment retrieval learned from TREC data. Journal of\\nInformation Science , 27(5):303–310.\\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\\nArnold Overwijk. 2021. Approximate nearest neigh-\\nbor negative contrastive learning for dense text re-\\ntrieval. In Proceedings of the 9th International Con-\\nference on Learning Representations (ICLR 2021) .\\nIkuya Yamada, Akari Asai, and Hannaneh Ha-\\njishirzi. 2021. Efﬁcient passage retrieval with\\nhashing for open-domain question answering.\\narXiv:2106.00882 .\\nPeilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini:\\nenabling the use of Lucene for information retrieval\\nresearch. In Proceedings of the 40th Annual Inter-\\nnational ACM SIGIR Conference on Research and\\nDevelopment in Information Retrieval (SIGIR 2017) ,\\npages 1253–1256, Tokyo, Japan.\\nPeilin Yang, Hui Fang, and Jimmy Lin. 2018. Anserini:\\nreproducible ranking baselines using Lucene. Jour-\\nnal of Data and Information Quality , 10(4):Article\\n16.COIL: Revisit Exact Lexical Match in Information Retrieval\\nwith Contextualized Inverted List\\nLuyu Gao, Zhuyun Dai, Jamie Callan\\nLanguage Technologies Institute\\nCarnegie Mellon University\\n{luyug, zhuyund, callan}@cs.cmu.edu\\nAbstract\\nClassical information retrieval systems such as\\nBM25 rely on exact lexical match and carry\\nout search efﬁciently with inverted list index.\\nRecent neural IR models shifts towards soft\\nsemantic matching all query document terms,\\nbut they lose the computation efﬁciency of\\nexact match systems. This paper presents\\nCOIL, a contextualized exact match retrieval\\narchitecture that brings semantic lexical match-\\ning. COIL scoring is based on overlapping\\nquery document tokens’ contextualized repre-\\nsentations. The new architecture stores con-\\ntextualized token representations in inverted\\nlists, bringing together the efﬁciency of exact\\nmatch and the representation power of deep\\nlanguage models. Our experimental results\\nshow COIL outperforms classical lexical re-\\ntrievers and state-of-the-art deep LM retrievers\\nwith similar or smaller latency.1\\n1 Introduction\\nWidely used, bag-of-words (BOW) information re-\\ntrieval (IR) systems such as BM25 rely on exact\\nlexical match2between query and document terms.\\nRecent study in neural IR takes a different approach\\nand compute soft matching between all query and\\ndocument terms to model complex matching.\\nThe shift to soft matching in neural IR models\\nattempts to address vocabulary mismatch problems,\\nthat query and the relevant documents use differ-\\nent terms, e.g. cat v.s. kitty, for the same con-\\ncept (Huang et al., 2013; Guo et al., 2016; Xiong\\net al., 2017). Later introduction of contextualized\\nrepresentations (Peters et al., 2018) from deep lan-\\nguage models (LM) further address semantic mis-\\nmatch , that the same term can refer to different\\nconcepts, e.g., bank of river vs. bank in ﬁnance.\\nFine-tuned deep LM rerankers produce token rep-\\nresentations based on context and achieve state-of-\\n1Our code is available at https://github.com/\\nluyug/COIL .\\n2Exact match up to morphological changes.the-art in text ranking with huge performance leap\\n(Nogueira and Cho, 2019; Dai and Callan, 2019b).\\nThough the idea of soft matching all tokens is\\ncarried through the development of neural IR mod-\\nels, seeing the success brought by deep LMs, we\\ntake a step back and ask: how much gain can we get\\nif we introduce contextualized representations back\\nto lexical exact match systems? In other words, can\\nwe build a system that still performs exact query-\\ndocument token matching but compute matching\\nsignals with contextualized token representations\\ninstead of heuristics? This may seem a constraint\\non the model, but exact lexical match produce more\\nexplainable and controlled patterns than soft match-\\ning. It also allows search to focus on only the\\nsubset of documents that have overlapping terms\\nwith query, which can be done efﬁciently with in-\\nverted list index. Meanwhile, using dense contex-\\ntualized token representations enables the model\\nto handle semantic mismatch, which has been a\\nlong-standing problem in classic lexical systems.\\nTo answer the question, we propose a new lexi-\\ncal matching scheme that uses vector similarities\\nbetween query-document overlapping term contex-\\ntualized representations to replace heuristic scor-\\ning used in classical systems. We present COn-\\ntextualized Inverted List (COIL), a new exact lex-\\nical match retrieval architecture armed with deep\\nLM representations. COIL processes documents\\nwith deep LM ofﬂine and produces representations\\nfor each document token. The representations are\\ngrouped by their surface tokens into inverted lists.\\nAt search time, we build representation vectors\\nfor query tokens and perform contextualized ex-\\nact match: use each query token to look up its\\nown inverted list and compute vector similarity\\nwith document vectors stored in the inverted list\\nas matching scores. COIL enables efﬁcient search\\nwith rich-in-semantic matching between query and\\ndocument.\\nOur contributions include 1) introduce a novelarXiv:2104.07186v1  [cs.IR]  15 Apr 2021retrieval architecture, contextualized inverted\\nlists (COIL) that brings semantic matching into\\nlexical IR systems, 2) show matching signals in-\\nduced from exact lexical match can capture com-\\nplicated matching patterns, 3) demonstrate COIL\\nsigniﬁcantly outperform classical and deep LM\\naugmented lexical retrievers as well as state-of-the-\\nart dense retrievers on two retrieval tasks.\\n2 Related Work\\nLexical Retriever Classical IR systems rely on\\nexact lexical match retrievers such as Boolean\\nRetrieval, BM25 (Robertson and Walker, 1994)\\nand statistical language models (Lafferty and Zhai,\\n2001). This type of retrieval model can process\\nqueries very quickly by organizing the documents\\ninto inverted index, where each distinct term has\\nan inverted list that stores information about docu-\\nments it appears in. Nowadays, they are still widely\\nused in production systems. However, these re-\\ntrieval models fall short of matching related terms\\n(vocabulary mismatch) or modeling context of the\\nterms (semantic mismatch). Much early effort\\nwas put into improving exact lexical match retriev-\\ners, such as matching n-grams (Metzler and Croft,\\n2005) or expanding queries with terms from related\\ndocuments (Lavrenko and Croft, 2001). However,\\nthese methods still use BOW framework and have\\nlimited capability of modeling human languages.\\nNeural Ranker In order to deal with vocab-\\nulary mismatch, neural retrievers that rely on\\nsoft matching between numerical text represen-\\ntations are introduced. Early attempts compute\\nsimilarity between pre-trained word embedding\\nsuch as word2vec (Mikolov et al., 2013) and\\nGLoVe (Pennington et al., 2014) to produce match-\\ning score (Ganguly et al., 2015; Diaz et al., 2016).\\nOne more recent approach encodes query and doc-\\nument each into a vector and computes vector sim-\\nilarity (Huang et al., 2013). Later researches real-\\nized the limited capacity of a single vector to en-\\ncode ﬁne-grained information and introduced full\\ninteraction models to perform soft matching be-\\ntween all term vectors (Guo et al., 2016; Xiong\\net al., 2017). In these approaches, scoring is\\nbased on learned neural networks and the hugely\\nincreased computation cost limited their use to\\nreranking a top candidate list generated by a lexical\\nretriever.Deep LM Based Ranker and Retriever Deep\\nLM made a huge impact on neural IR. Fine-\\ntuned Transformer (Vaswani et al., 2017) LM\\nBERT (Devlin et al., 2019) achieved state-of-the-\\nart reranking performance for passages and docu-\\nments (Nogueira and Cho, 2019; Dai and Callan,\\n2019b). As illustrated in Figure 1a, the common\\napproach is to feed the concatenated query docu-\\nment text through BERT and use BERT’s [CLS]\\noutput token to produce a relevance score. The\\ndeep LM rerankers addressed both vocabulary and\\nsemantic mismatch by computing full cross atten-\\ntion between contextualized token representations.\\nLighter deep LM rankers are developed (MacA-\\nvaney et al., 2020; Gao et al., 2020), but their cross\\nattention operations are still too expensive for full-\\ncollection retrieval.\\nLater research therefore resorted to augment-\\ning lexical retrieval with deep LMs by expanding\\nthe document surface form to narrow the vocab-\\nulary gap, e.g., DocT5Query (Nogueira and Lin,\\n2019), or altering term weights to emphasize impor-\\ntant terms, e.g., DeepCT (Dai and Callan, 2019a).\\nSmartly combining deep LM retriever and reranker\\ncan offer additive gain for end performance (Gao\\net al., 2021a). These retrievers however still suffer\\nfrom vocabulary and semantic mismatch as tradi-\\ntional lexical retrievers.\\nAnother line of research continues the work on\\nsingle vector representation and build dense retriev-\\ners, as illustrated in Figure 1b. They store docu-\\nment vectors in a dense index and retrieve them\\nthrough Nearest Neighbours search. Using deep\\nLMs, dense retrievers have achieved promising re-\\nsults on several retrieval tasks (Karpukhin et al.,\\n2020). Later researches show that dense retrieval\\nsystems can be further improved by better train-\\ning (Xiong et al., 2020; Gao et al., 2021b).\\nSingle vector systems have also been extended\\nto multi-vector representation systems. Poly-\\nencoder (Humeau et al., 2020) encodes queries\\ninto a set of vectors. Similarly, Me-BERT (Luan\\net al., 2020) represents documents with a set of vec-\\ntors. A concurrent work ColBERT (Figure 1c) use\\nmultiple vectors to encode both queries and docu-\\nments (Khattab and Zaharia, 2020). In particular, it\\nrepresents a documents with all its terms’ vectors\\nand a query with an expanded set of term vectors.\\nIt then computes all-to-all (Cartesian) soft match\\nbetween the tokens. ColBERT performs interaction\\nas dot product followed pooling operations, whichCLS bank account SEP bank river bankCLS bank account SEP bank river bankCLS bank account SEP bank river bankCLS bank account SEP bank river bankscore(a) Cross-Attention Model (e.g., BERT reranker)\\nCLS bank account CLS bank river bankCLS bank accountCLS bank account\\nCLS bank river bankCLS bank river bankscore (b) Dense Retrievers (e.g., DPR)\\nCLS bank account CLS bank river bankCLS bank accountCLS bank account\\nCLS bank river bankCLS bank river bank\\nEXP EXPscore\\nmax max max\\nEXP\\nEXPEXP\\nEXPmax max\\n(c) ColBERT: All-to-All Match\\nCLS bank account CLS bank river bankCLS bank accountCLS bank account\\nCLS bank river bankCLS bank river bankdot maxsum (d) COIL: Contextualized Exact Match\\nFigure 1: An illustration of reranking/retrieval mechanisms with deep LM, including our proposed model, COIL.\\nBank\\nRiver\\nAccountBank\\nAccount\\nTraditional Inverted Lists Querydocid: 3\\ntf: 2docid: 9\\ntf: 1docid: 1\\ntf: 1docid: 2\\ntf: 1docid: 4\\ntf: 1docid: 5\\ntf: 2docid: 1\\ntf: 1docid: 3\\ntf: 1docid: 6\\ntf: 1....\\n....BM25\\xa0\\nscoringBM25\\xa0\\nscoring\\nBM25\\xa0\\nscoring\\nFigure 2: An illustration of traditional inverted lists.\\nThe inverted list maps a term to the list of documents\\nwhere the term occurs. Retriever looks up query terms’\\ninverted lists and scores those documents with stored\\nstatistics such as term frequency (tf).\\nallows it to also leverage a dense index to do full\\ncorpus retrieval. However, since ColBERT encodes\\na document with all tokens, it adds another order\\nof magnitude of index complexity to all aforemen-\\ntioned methods: document tokens in the collection\\nneed to be stored in a single huge index and con-\\nsidered at query time. Consequently, ColBERT is\\nengineering and hardware demanding.\\n3 Methodologies\\nIn this section, we ﬁrst provide some preliminaries\\non exact lexical match systems. Then we discuss\\nCOIL’s contextualized exact match design and how\\nits search index is organized. We also give a com-\\nparison between COIL and other popular retrievers.\\nBank\\nRiver\\nAccountBank\\nAccount\\nContextualized Inverted Lists Querydocid\\xa0 [1 3 6 7]\\ndocid\\xa0 [1 2 4 5 5 9]\\ndocid\\xa0 [3 3 9]vectors\\nvectors\\nvectorsCLSdocid\\xa0 [1 2 3 4 .............C]\\nvectors ...CLSmatrix\\nproduct\\nmatrix\\nproduct\\nmatrix\\nproductFigure 3: COIL’s index and retrieval architecture.\\nCOIL-tok relies on the exact token matching (lower).\\nCOIL-full includes in addition CLS matching (upper).\\n3.1 Preliminaries\\nClassic lexical retrieval system relies on overlap-\\nping query document terms under morphological\\ngeneralization like stemming, in other words, exact\\nlexical match , to score query document pair. A\\nscoring function is deﬁned as a sum of matched\\nterm scores. The scores are usually based on statis-\\ntics like term frequency ( tf). Generally, we can\\nwrite,\\ns=X\\nt2q\\\\d\\x1bt(hq(q;t);hd(d;t)) (1)\\nwhere for each overlapping term tbetween query q\\nand document d, functionshqandhdextract terminformation and a term scoring function \\x1btcom-\\nbines them. A popular example is BM25, which\\ncomputes,\\nsBM25 =X\\nt2q\\\\didf(t)hBM25\\nq(q;t)hBM25\\nd(d;t)\\nhBM25\\nq(q;t) =tft;q(1 +k2)\\ntft;q+k2\\nhBM25\\nd(d;t) =tft;d(1 +k1)\\ntft;d+k1(1\\x00b+bjdj\\navgdl)(2)\\nwheretft;drefers to term frequency of term tin\\ndocumentd,tft;qrefers to the term frequency in\\nquery,idf(t)is inverse document frequency, and b,\\nk1,k2are hyper-parameters.\\nOne key advantage of exact lexical match sys-\\ntems lies in efﬁciency. With summation over exact\\nmatches, scoring of each query term only goes to\\ndocuments that contain matching terms. This can\\nbe done efﬁciently using inverted list indexing (Fig-\\nure 2). The inverted list maps back from a term\\nto a list of documents where the term occurs. To\\ncompute Equation 1, the retriever only needs to\\ntraverse the subset of documents in query terms’\\ninverted lists instead of going over the entire docu-\\nment collection.\\nWhile recent neural IR research mainly focuses\\non breaking the exact match bottleneck with soft\\nmatching of text, we hypothesize that exact match\\nitself can be improved by replacing semantic in-\\ndependent frequency-based scoring with semantic\\nrich scoring. In the rest of this section, we show\\nhow to modify the exact lexical match framework\\nwith contextualized term representations to build\\neffective and efﬁcient retrieval systems.\\n3.2 Contextualized Exact Lexical Match\\nInstead of term frequency, we desire to encode\\nthe semantics of terms to facilitate more effective\\nmatching. Inspired by recent advancements in deep\\nLM, we encode both query and document tokens\\ninto contextualized vector representations and carry\\nout matching between exact lexical matched tokens.\\nFigure 1d illustrates the scoring model of COIL.\\nIn this work, we use a Transformer language\\nmodel3as the contextualization function. We en-\\ncode a query qwith the language model (LM) and\\nrepresent its i-th token by projecting the corre-\\nsponding output:\\nvq\\ni=WtokLM(q;i) +btok (3)\\n3We used the base, uncased variant of BERT.where Wnt\\x02nlm\\ntokis a matrix that maps the LM’s\\nnlmdimension output into a vector of lower di-\\nmensionnt. We down project the vectors as we\\nhypothesize that it sufﬁces to use lower dimension\\ntoken vectors. We conﬁrm this in section 5. Simi-\\nlarly, we encode a document d’sj-th tokendjwith:\\nvd\\nj=WtokLM(d;j) +btok (4)\\nWe then deﬁne the contextualized exact lexical\\nmatch scoring function between query document\\nbased on vector similarities between exact matched\\nquery document token pairs:\\nstok(q;d) =X\\nqi2q\\\\dmax\\ndj=qi(vq\\ni|vd\\nj) (5)\\nNote that, importantly, the summation goes through\\nonly overlapping terms, qi2q\\\\d. For each query\\ntokenqi, we ﬁnds all same tokensdjin the docu-\\nment, computes their similarity with qiusing the\\ncontextualized token vectors. The maximum sim-\\nilarities are picked for query token qi. Max op-\\nerator is adopted to capture the most important\\nsignal (Kim, 2014). This ﬁts in the general lexical\\nmatch formulation, with hqgiving representation\\nforqi,htgiving representations for all dj=qi, and\\n\\x1btcompute dot similarities between query vector\\nwith document vectors and max pool the scores.\\nAs with classic lexical systems, stokdeﬁned in\\nEquation 5 does not take into account similarities\\nbetween lexical-different terms, thus faces vocabu-\\nlary mismatch. Many popular LMs (Devlin et al.,\\n2019; Yang et al., 2019; Liu et al., 2019) use a\\nspecial CLS token to aggregate sequence represen-\\ntation. We project the CLS vectos with Wnc\\x02nlm\\ncls\\nto represent the entire query or document,\\nvq\\ncls=WclsLM(q;CLS) +bcls\\nvd\\ncls=WclsLM(d;CLS) +bcls(6)\\nThe similarity between vq\\nclsandvd\\nclsprovides high-\\nlevel semantic matching and mitigates the issue of\\nvocabulary mismatch. The full form of COIL is:\\nsfull(q;d) =stok(q;d) +vq\\ncls|vd\\ncls (7)\\nIn the rest of the paper, we refer to systems with\\nCLS matching COIL-full and without COIL-tok .\\nCOIL’s scoring model (Figure 1d) is fully differ-\\nentiable. Following earlier work (Karpukhin et al.,\\n2020), we train COIL with negative log likelihood\\ndeﬁned over query q, a positive document d+and aset of negative documents fd\\x00\\n1;d\\x00\\n2;::d\\x00\\nl::gas loss.\\nL=\\x00logexp(s(q;d+))\\nexp(s(q;d+)) +P\\nlexp(s(q;d\\x00\\nl))\\n(8)\\nFollowing Karpukhin et al. (2020), we use in batch\\nnegatives and hard negatives generated by BM25.\\nDetails are discussed in implementation, section 4.\\n3.3 Index and Retrieval with COIL\\nCOIL pre-computes the document representations\\nand builds up a search index, which is illustrated in\\nFigure 3. Documents in the collection are encoded\\nofﬂine into token and CLS vectors. Formally, for\\na unique token tin the vocabulary V, we collect\\nits contextualized vectors from all of its mentions\\nfrom documents in collection C, building token t’s\\ncontextualized inverted list:\\nIt=fvd\\njjdj=t;d2Cg; (9)\\nwhere vd\\njis the BERT-based token encoding de-\\nﬁned in Equation 4. We deﬁne search index to\\nstore inverted lists for all tokens in vocabulary,\\nI=fItjt2Vg. For COIL-full, we also build an\\nindex for the CLS token Icls=fvd\\nclsjd2Cg.\\nAs shown in Figure 3, in this work we im-\\nplement COIL’s by stacking vectors in each in-\\nverted listItinto a matrix Mnt\\x02jIkj, so that sim-\\nilarity computation that traverses an inverted list\\nand computes vector dot product can be done ef-\\nﬁciently as one matrix-vector product with opti-\\nmized BLAS (Blackford et al., 2002) routines on\\nCPU or GPU. All vd\\nclsvectors can also be organized\\nin a similar fashion into matrix Mclsand queried\\nwith matrix product. The matrix implementation\\nhere is an exhaustive approach that involves all vec-\\ntors in an inverted list. As a collection of dense\\nvectors, it is also possible to organize each inverted\\nlist as an approximate search index (Johnson et al.,\\n2017; Guo et al., 2019) to further speed up search.\\nWhen a query qcomes in, we encode every of\\nits token into vectors vq\\ni. The vectors are sent to\\nthe subset of COIL inverted lists that corresponds\\nquery tokens J=fItjt2qg. where the matrix\\nproduct described above is carried out. This is\\nefﬁcient asjJj<<jIj, having only a small subset\\nof all inverted lists to be involved in search. For\\nCOIL-full, we also use encoded CLS vectors vq\\ncls\\nto query the CLS index to get the CLS matching\\nscores. The scoring over different inverted lists canserve in parallel. The scores are then combined by\\nEquation 5 to rank the documents.\\nReaders can ﬁnd detailed illustration ﬁgures in\\nthe Appendix A, for index building and querying,\\nFigure 4 and Figure 5, respectively.\\n3.4 Connection to Other Retrievers\\nDeep LM based Lexical Index Models like\\nDeepCT (Dai and Callan, 2019a, 2020) and\\nDocT5Query (Nogueira and Lin, 2019) alter tft;d\\nin documents with deep LM BERT or T5. This is\\nsimilar to a COIL-tok with token dimension nt= 1.\\nA single degree of freedom however measures more\\nof a term importance than semantic agreement .\\nDense Retriever Dense retrievers (Figure 1b)\\nare equivalent to COIL-full’s CLS matching. COIL\\nmakes up for the lost token-level interactions in\\ndense retriever with exact matching signals.\\nColBERT ColBERT (Figure 1c) computes rel-\\nevance by soft matching allquery and document\\nterm’s contextualized vectors.\\ns(q;d) =X\\nqi2[cls;q;exp]max\\ndj2[cls;d](vq\\ni|vd\\nj) (10)\\nwhere interactions happen among query q, docu-\\nmentd,clsand set of query expansion tokens exp.\\nThe all-to-all match contrasts COIL that only uses\\nexact match. It requires a dense retrieval over all\\ndocument tokens’ representations as opposed to\\nCOIL which only considers query’s overlapping to-\\nkens, and are therefore much more computationally\\nexpensive than COIL.\\n4 Experiment Methodologies\\nDatasets We experiment with two large scale ad\\nhoc retrieval benchmarks from the TREC 2019\\nDeep Learning (DL) shared task: MSMARCO\\npassage (8M English passages of average length\\naround 60 tokens) and MSMARCO document (3M\\nEnglish documents of average length around 900\\ntokens)4. For each, we train models with the\\nMSMARCO Train queries, and record results on\\nMSMARCO Dev queries and TREC DL 2019\\ntest queries. We report mainly full-corpus re-\\ntrieval results but also include the rerank task on\\nMSMARCO Dev queries where we use neural\\nscores to reorder BM25 retrieval results provided\\nby MSMARO organizers. Ofﬁcial metrics include\\n4Both datasets can be downloaded from https://\\nmicrosoft.github.io/msmarco/MRR@1K and NDCG@10 on test and MRR@10\\non MSMARCO Dev. We also report recall for the\\ndev queries following prior work (Dai and Callan,\\n2019a; Nogueira and Lin, 2019).\\nCompared Systems Baselines include 1) tradi-\\ntional exact match system BM25, 2) deep LM aug-\\nmented BM25 systems DeepCT (Dai and Callan,\\n2019a) and DocT5Query (Nogueira and Lin, 2019),\\n3) dense retrievers, and 4) soft all-to-all retriever\\nColBERT. For DeepCT and DocT5Query, we use\\nthe rankings provided by the authors. For dense\\nretrievers, we report two dense retrievers trained\\nwith BM25 negatives or with mixed BM25 and\\nrandom negatives, published in Xiong et al. (2020).\\nHowever since these systems use a robust version\\nof BERT, RoBERTa (Liu et al., 2019) as the LM\\nand train document retriever also on MSMARCO\\npassage set, we in addition reproduce a third dense\\nretriever, that uses the exact same training setup as\\nCOIL. All dense retrievers use 768 dimension em-\\nbedding. For ColBERT, we report its published re-\\nsults (available only on passage collection). BERT\\nreranker is added in the rerank task.\\nWe include 2 COIL systems: 1) COIL-tok, the\\nexact token match only system, and 2) COLL-full,\\nthe model with both token match and CLS match.\\nImplementation We build our models with Py-\\ntorch (Paszke et al., 2019) based on huggingface\\ntransformers (Wolf et al., 2019). COIL’s LM is\\nbased on BERT’s base variant. COIL systems use\\ntoken dimension nt= 32 and COIL-full use CLS\\ndimensionnc= 768 as default, leading to 110M\\nparameters. We add a Layer Normalization to CLS\\nvector when useful. All models are trained for 5\\nepochs with AdamW optimizer, a learning rate of\\n3e-6, 0.1 warm-up ratio, and linear learning rate\\ndecay, which takes around 12 hours. Hard neg-\\natives are sampled from top 1000 BM25 results.\\nEach query uses 1 positive and 7 hard negatives;\\neach batch uses 8 queries on MSMARCO passage\\nand 4 on MSMARCO document. Documents are\\ntruncated to the ﬁrst 512 tokens to ﬁt in BERT.\\nWe conduct validation on randomly selected 512\\nqueries from corresponding train set. Latency num-\\nbers are measured on dual Xeon E5-2630 v3 for\\nCPU and RTX 2080 ti for GPU. We implement\\nCOIL’s inverted lists as matrices as described in\\nsubsection 3.3, using NumPy (Harris et al., 2020)\\non CPU and Pytorch on GPU. We perform a) a set\\nof matrix products to compute token similaritiesover contextualized inverted lists, b) scatter to map\\ntoken scores back to documents, and c) sort to rank\\nthe documents. Illustration can be found in the\\nappendix, Figure 5.\\n5 Results\\nThis section studies the effectiveness of COIL\\nand how vector dimension in COIL affects the\\neffectiveness-efﬁciency tradeoff. We also provide\\nqualitative analysis on contextualized exact match.\\n5.1 Main Results\\nTable 1 reports various systems’ performance on\\nthe MARCO passage collection. COIL-tok ex-\\nact lexical match only system signiﬁcantly out-\\nperforms all previous lexical retrieval systems.\\nWith contextualized term similarities, COIL-tok\\nachieves a MRR of 0.34 compared to BM25’s MRR\\n0.18. DeepCT and DocT5Query, which also use\\ndeep LMs like BERT and T5, are able to break the\\nlimit of heuristic term frequencies but are still lim-\\nited by semantic mismatch issues. We see COIL-\\ntok outperforms both systems by a large margin.\\nCOIL-tok also ranks top of the candidate list bet-\\nter than dense retrieves. It prevails in MRR and\\nNDCG while performs on par in recall with the\\nbest dense system, indicating that COIL’s token\\nlevel interaction can improve precision. With the\\nCLS matching added, COIL-full gains the ability\\nto handle mismatched vocabulary and enjoys an-\\nother performance leap, outperforming all dense\\nretrievers.\\nCOIL-full achieves a very narrow performance\\ngap to ColBERT. Recall that ColBERT computes\\nall-to-all soft matches between all token pairs. For\\nretrieval, it needs to consider for each query token\\nallmentions of alltokens in the collection (MS-\\nMARCO passage collection has around 500M to-\\nken mentions). COIL-full is able to capture match-\\ning patterns as effectively with exact match signals\\nfrom only query tokens’ mentions and a single CLS\\nmatching to bridge the vocabulary gap.\\nWe observe a similar pattern in the rerank task.\\nCOIL-tok is already able to outperform dense re-\\ntriever and COIL-full further adds up to perfor-\\nmance with CLS matching, being on-par with Col-\\nBERT. Meanwhile, previous BERT rerankers have\\nlittle performance advantage over COIL5. In prac-\\ntice, we found BERT rerankers to be much more\\n5Close performance between COIL and BERT rerankers\\nis partially due to the bottleneck of BM25 candidates.Table 1: MSMARCO passage collection results. Results not applicable are denoted ‘–’ and no available ‘n.a.’.\\nMS MARCO Passage Ranking\\nDev Rerank Dev Retrieval DL2019 Retrieval\\nModel MRR@10 MRR@10 Recall@1K NDCG@10 MRR@1K\\nLexical Retriever\\nBM25 – 0.184 0.853 0.506 0.825\\nDeepCT – 0.243 0.909 0.572 0.883\\nDocT5Query – 0.278 0.945 0.642 0.888\\nBM25+BERT reranker 0.347 – – – –\\nDense Retriever\\nDense (BM25 neg) n.a. 0.299 0.928 0.600 n.a.\\nDense (rand + BM25 neg) n.a. 0.311 0.952 0.576 n.a.\\nDense (our train) 0.312 0.304 0.932 0.635 0.898\\nColBERT 0.349 0.360 0.968 n.a. n.a.\\nCOIL-tok 0.336 0.341 0.949 0.660 0.915\\nCOIL-full 0.348 0.355 0.963 0.704 0.924\\nTable 2: MSMARCO document collection results. Results not applicable are denoted ‘–’ and no available ‘n.a.’.\\nMS MARCO Document Ranking\\nDev Rerank Dev Retrieval DL2019 Retrieval\\nModel MRR@10 MRR@10 Recall@1K NDCG@10 MRR@1K\\nLexical Retriever\\nBM25 – 0.230 0.886 0.519 0.805\\nDeepCT – 0.320 0.942 0.544 0.891\\nDocT5Query – 0.288 0.926 0.597 0.837\\nBM25+BERT reranker 0.383 – – – –\\nDense Retriever\\nDense (BM25 neg) n.a. 0.299 0.928 0.600 n.a.\\nDense (rand + BM25 neg) n.a. 0.311 0.952 0.576 n.a.\\nDense (our train) 0.358 0.340 0.883 0.546 0.785\\nCOIL-tok 0.381 0.385 0.952 0.626 0.921\\nCOIL-full 0.388 0.397 0.962 0.636 0.913\\nexpensive, requiring over 2700 ms for reranking\\ncompared to around 10ms in the case of COIL.\\nTable 2 reports the results on MSMARCO docu-\\nment collection. In general, we observe a similar\\npattern as with the passage case. COIL systems\\nsigniﬁcantly outperform both lexical and dense sys-\\ntems in MRR and NDCG and retain a small advan-\\ntage measured in recall. The results suggest that\\nCOIL can be applicable to longer documents with\\na consistent advantage in effectiveness.\\nThe results indicate exact lexical match mecha-\\nnism can be greatly improved with the introduction\\nof contextualized representation in COIL. COIL’s\\ntoken-level match also yields better ﬁne-grained\\nsignals than dense retriever’s global match signal.\\nCOIL-full further combines the lexical signals with\\ndense CLS match, forming a system that can deal\\nwith both vocabulary and semantic mismatch, be-\\ning as effective as all-to-all system.5.2 Analysis of Dimensionality\\nThe second experiment tests how varying COIL’s\\ntoken dimension ntand CLS dimension ncaffect\\nmodel effectiveness and efﬁciency. We record re-\\ntrieval performance and latency on MARCO pas-\\nsage collection in Table 3.\\nIn COIL-full systems, reducing CLS dimension\\nfrom 768 to 128 leads to a small drop in perfor-\\nmance on the Dev set, indicating that a full 768\\ndimension may not be necessary for COIL. Keep-\\ning CLS dimension at 128, systems with token\\ndimension 32 and 8 have very small performance\\ndifference, suggesting that token-speciﬁc semantic\\nconsumes much fewer dimensions. Similar pattern\\ninntis also observed in COIL-tok ( nc= 0).\\nOn the DL2019 queries, we observe that reduc-\\ning dimension actually achieves better MRR. We\\nbelieve this is due to a regulatory effect, as theTable 3: Performance and latency of COIL systems with different representation dimensions. Results not applica-\\nble are denoted ‘–’ and no available ‘n.a.’. Here ncdenotes COIL CLS dimension and nttoken vector dimension.\\n*: ColBERT use approximate search and quantization. We exclude I/O time from measurements.\\nDev Retrieval DL2019 Retrieval Latency/ms\\nModel MRR@10 Recall@1K NDCG@10 MRR CPU GPU\\nBM25 0.184 0.853 0.506 0.825 36 n.a.\\nDense 0.304 0.932 0.635 0.898 293 32\\nColBERT 0.360 0.968 n.a. n.a. 458* –\\nCOIL\\nncnt\\n768 32 0.355 0.963 0.704 0.924 380 41\\n128 32 0.350 0.953 0.692 0.956 125 23\\n128 8 0.347 0.956 0.694 0.977 113 21\\n0 32 0.341 0.949 0.660 0.915 67 18\\n0 8 0.336 0.940 0.678 0.953 55 16\\nTable 4: Sample query document pairs with similarity scores produced by COIL. Tokens in examination are colored\\nblue. Numbers in brackets are query-document vector similarities computed with vectors generated by COIL.\\nQuery Token COIL Contextualized Exact Match Score Relevance\\nwhat is a cabinet in govtCabinet [16.28] (government) A cabinet [16.75] is a body of high-\\nranking state ofﬁcials, typically consisting of the top leaders of the ....+\\nCabinet [7.23] is 20x60 and top is 28x72. .... I had a 2cm granite counter-\\ntop installed with a 10 inch overhang on one side and a 14 inch....-\\nwhat is priority passPriority Pass [11.61] is an independent airport lounge access program. A\\nmembership provides you with access to their network of over 700 ....+\\nSnoqualmie Pass [7.98] is a mountain pass [6.83] that carries Interstate\\n90 through the Cascade Range in the U.S. State of Washington....-\\nwhat isnjstartNJSTART is [1.25] a self-service online platform that allows vendors to\\nmanage forms, certiﬁcations, submit proposals, access training ....+\\nContract awardees will receive their Blanket P.O. once it is [-0.10] con-\\nverted, and details regarding that process will also be sent...-\\ntest queries were labeled differently from the MS-\\nMARCO train/dev queries (Craswell et al., 2020).\\nWe also record CPU and GPU search latency\\nin Table 3. Lowering COIL-full’s CLS dimen-\\nsion from 768 to 128 gives a big speedup, making\\nCOIL faster than DPR system. Further dropping\\ntoken dimensions provide some extra speedup. The\\nCOIL-tok systems run faster than COIL-full, with a\\nlatency of the same order of magnitude as the tradi-\\ntional BM25 system. Importantly, lower dimension\\nCOIL systems still retain a performance advantage\\nover dense systems while being much faster. We\\ninclude ColBERT’s latency reported in the original\\npaper, which was optimized by approximate search\\nand quantization. All COIL systems have lower\\nlatency than ColBERT even though our current im-\\nplementation does not use those optimization tech-\\nniques. We however note that approximate search\\nand quantization are applicable to COIL, and leave\\nthe study of speeding up COIL to future work.5.3 Case Study\\nCOIL differs from all previous embedding-based\\nmodels in that it does not use a single uniﬁed em-\\nbedding space. Instead, for a speciﬁc token, COIL\\nlearns an embedding space to encode and measure\\nthe semantic similarity of the token in different\\ncontexts. In this section, we show examples where\\nCOIL differentiates different senses of a word un-\\nder different contexts. In Table 4, we show how\\nthe token similarity scores differ across contexts in\\nrelevant and irrelevant query document pairs.\\nThe ﬁrst query looks for “cabinet” in the context\\nof “govt” (abbreviation for “government”). The\\ntwo documents both include query token \"cabinet\"\\nbut of a different concept. The ﬁrst one refers to\\nthe government cabinet and the second to a case\\nor cupboard. COIL manages to match “cabinet” in\\nthe query to “cabinet” in the ﬁrst document with\\na much higher score. In the second query, \"pass\"\\nin both documents refer to the concept of permis-sion. However, through contextualization, COIL\\ncaptures the variation of the same concept and as-\\nsigns a higher score to “pass” in the ﬁrst document.\\nStop words like “it”, “a”, and “the” are com-\\nmonly removed in classic exact match IR systems\\nas they are not informative on their own. In the\\nthird query, on the other hand, we observe that\\nCOIL is able to differentiate “is” in an explanatory\\nsentence and “is” in a passive form, assigning the\\nﬁrst higher score to match query context.\\nAll examples here show that COIL can go be-\\nyond matching token surface form and introduce\\nrich context information to estimate matching. Dif-\\nferences in similarity scores across mentions under\\ndifferent contexts demonstrate how COIL systems\\ngain strength over lexical systems.\\n6 Conclusion and Future Work\\nExact lexical match systems have been widely used\\nfor decades in classical IR systems and prove to be\\neffective and efﬁcient. In this paper, we point out\\na critical problem, semantic mismatch, that gener-\\nally limits all IR systems based on surface token\\nfor matching. To ﬁx semantic mismatch, we in-\\ntroduce contextualized exact match to differentiate\\nthe same token in different contexts, providing ef-\\nfective semantic-aware token match signals. We\\nfurther propose contextualized inverted list (COIL)\\nsearch index which swaps token statistics in in-\\nverted lists with contextualized vector representa-\\ntions to perform effective search.\\nOn two large-scale ad hoc retrieval benchmarks,\\nwe ﬁnd COIL substantially improves lexical re-\\ntrieval and outperforms state-of-the-art dense re-\\ntrieval systems. These results indicate large head-\\nroom of the simple-but-efﬁcient exact lexical match\\nscheme. When the introduction of contextualiza-\\ntion handles the issue of semantic mismatch, exact\\nmatch system gains the capability of modeling com-\\nplicated matching patterns that were not captured\\nby classical systems.\\nV ocabulary mismatch in COIL can also be\\nlargely mitigated with a high-level CLS vector\\nmatching. The full system performs on par with\\nmore expensive and complex all-to-all match re-\\ntrievers. The success of the full system also shows\\nthat dense retrieval and COIL’s exact token match-\\ning give complementary effects, with COIL making\\nup dense system’s lost token level matching signals\\nand dense solving the vocabulary mismatch proba-\\nbly for COIL.With our COIL systems showing viable search\\nlatency, we believe this paper makes a solid step\\ntowards building next-generation index that stores\\nsemantics. At the intersection of lexical and neural\\nsystems, efﬁcient algorithms proposed for both can\\npush COIL towards real-world systems.References\\nS. Blackford, J. Demmel, J. Dongarra, I. Duff, S. Ham-\\nmarling, Greg Henry, M. Héroux, L. Kaufman, An-\\ndrew Lumsdaine, A. Petitet, R. Pozo, K. Remington,\\nand C. Whaley. 2002. An updated set of basic linear\\nalgebra subprograms (blas). ACM Transactions on\\nMathematical Software , 28.\\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\\nCampos, and Ellen M V oorhees. 2020. Overview\\nof the trec 2019 deep learning track. arXiv preprint\\narXiv:2003.07820 .\\nZhuyun Dai and J. Callan. 2019a. Context-aware sen-\\ntence/passage term importance estimation for ﬁrst\\nstage retrieval. ArXiv , abs/1910.10687.\\nZhuyun Dai and J. Callan. 2020. Context-aware docu-\\nment term weighting for ad-hoc search. Proceedings\\nof The Web Conference 2020 .\\nZhuyun Dai and Jamie Callan. 2019b. Deeper text un-\\nderstanding for IR with contextual neural language\\nmodeling. In Proceedings of the 42nd International\\nACM SIGIR Conference on Research and Develop-\\nment in Information Retrieval, SIGIR 2019, Paris,\\nFrance, July 21-25, 2019 , pages 985–988. ACM.\\nJ. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\\nToutanova. 2019. Bert: Pre-training of deep bidirec-\\ntional transformers for language understanding. In\\nNAACL-HLT .\\nFernando Diaz, Bhaskar Mitra, and Nick Craswell.\\n2016. Query expansion with locally-trained word\\nembeddings. In Proceedings of the 54th Annual\\nMeeting of the Association for Computational Lin-\\nguistics .\\nDebasis Ganguly, Dwaipayan Roy, Mandar Mitra,\\nand Gareth J. F. Jones. 2015. Word embedding\\nbased generalized language model for information\\nretrieval. In Proceedings of the 38th International\\nACM SIGIR Conference on Research and Develop-\\nment in Information Retrieval .\\nLuyu Gao, Zhuyun Dai, and Jamie Callan. 2020. Mod-\\nularized transfomer-based ranking framework. In\\nProceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing, EMNLP\\n2020, Online, November 16-20, 2020 . Association\\nfor Computational Linguistics.\\nLuyu Gao, Zhuyun Dai, and Jamie Callan. 2021a. Re-\\nthink training of BERT rerankers in multi-stage re-\\ntrieval pipeline. In Advances in Information Re-\\ntrieval - 43rd European Conference on IR Research,\\nECIR 2021, Virtual Event, March 28 - April 1, 2021,\\nProceedings, Part II .\\nLuyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Ben-\\njamin Van Durme, and Jamie Callan. 2021b. Com-\\nplement lexical retrieval model with semantic resid-\\nual embeddings. In Advances in Information Re-\\ntrieval - 43rd European Conference on IR Research,ECIR 2021, Virtual Event, March 28 - April 1, 2021,\\nProceedings, Part I .\\nJ. Guo, Y . Fan, Qingyao Ai, and W. Croft. 2016. A\\ndeep relevance matching model for ad-hoc retrieval.\\nProceedings of the 25th ACM International on Con-\\nference on Information and Knowledge Manage-\\nment .\\nR. Guo, Philip Y . Sun, E. Lindgren, Quan Geng, David\\nSimcha, Felix Chern, and S. Kumar. 2019. Accel-\\nerating large-scale inference with anisotropic vector\\nquantization. arXiv: Learning .\\nCharles R. Harris, K. Jarrod Millman, Stéfan J\\nvan der Walt, Ralf Gommers, Pauli Virtanen, David\\nCournapeau, Eric Wieser, Julian Taylor, Sebas-\\ntian Berg, Nathaniel J. Smith, Robert Kern, Matti\\nPicus, Stephan Hoyer, Marten H. van Kerkwijk,\\nMatthew Brett, Allan Haldane, Jaime Fernández del\\nRío, Mark Wiebe, Pearu Peterson, Pierre Gérard-\\nMarchant, Kevin Sheppard, Tyler Reddy, Warren\\nWeckesser, Hameer Abbasi, Christoph Gohlke, and\\nTravis E. Oliphant. 2020. Array programming with\\nNumPy. Nature .\\nPo-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,\\nAlex Acero, and Larry Heck. 2013. Learning deep\\nstructured semantic models for web search using\\nclickthrough data. In Proceedings of the 22nd ACM\\ninternational conference on Information & Knowl-\\nedge Management .\\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\\nand J. Weston. 2020. Poly-encoders: Architec-\\ntures and pre-training strategies for fast and accurate\\nmulti-sentence scoring. In ICLR .\\nJ. Johnson, M. Douze, and H. Jégou. 2017. Billion-\\nscale similarity search with gpus. ArXiv ,\\nabs/1702.08734.\\nV . Karpukhin, Barlas O ˘guz, Sewon Min, Patrick\\nLewis, Ledell Yu Wu, Sergey Edunov, Danqi\\nChen, and W. Yih. 2020. Dense passage re-\\ntrieval for open-domain question answering. ArXiv ,\\nabs/2004.04906.\\nO. Khattab and M. Zaharia. 2020. Colbert: Efﬁcient\\nand effective passage search via contextualized late\\ninteraction over bert. Proceedings of the 43rd Inter-\\nnational ACM SIGIR Conference on Research and\\nDevelopment in Information Retrieval .\\nYoon Kim. 2014. Convolutional neural networks for\\nsentence classiﬁcation. In EMNLP .\\nJohn Lafferty and Chengxiang Zhai. 2001. Document\\nlanguage models, query models, and risk minimiza-\\ntion for information retrieval. In Proceedings of the\\n24th Annual International ACM SIGIR Conference\\non Research and Development in Information Re-\\ntrieval .Victor Lavrenko and W. Bruce Croft. 2001. Relevance-\\nbased language models. In Proceedings of the 24th\\nAnnual International ACM SIGIR Conference on Re-\\nsearch and Development in Information Retrieval .\\nY . Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\\nJoshi, Danqi Chen, Omer Levy, M. Lewis, Luke\\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta:\\nA robustly optimized bert pretraining approach.\\nArXiv , abs/1907.11692.\\nYi Luan, Jacob Eisenstein, Kristina Toutanova, and\\nM. Collins. 2020. Sparse, dense, and atten-\\ntional representations for text retrieval. ArXiv ,\\nabs/2005.00181.\\nSean MacAvaney, F. Nardini, R. Perego, N. Tonellotto,\\nNazli Goharian, and O. Frieder. 2020. Efﬁcient doc-\\nument re-ranking for transformers by precomputing\\nterm representations. Proceedings of the 43rd Inter-\\nnational ACM SIGIR Conference on Research and\\nDevelopment in Information Retrieval .\\nDonald Metzler and W. Bruce Croft. 2005. A markov\\nrandom ﬁeld model for term dependencies. In SIGIR\\n2005: Proceedings of the 28th Annual International\\nACM SIGIR Conference on Research and Develop-\\nment in Information Retrieval .\\nTomas Mikolov, Ilya Sutskever, Kai Chen, G. S. Cor-\\nrado, and J. Dean. 2013. Distributed representations\\nof words and phrases and their compositionality. In\\nNIPS .\\nRodrigo Nogueira and Kyunghyun Cho. 2019. Passage\\nre-ranking with bert. ArXiv , abs/1901.04085.\\nRodrigo Nogueira and Jimmy Lin. 2019. From\\ndoc2query to doctttttquery.\\nAdam Paszke, Sam Gross, Francisco Massa, Adam\\nLerer, James Bradbury, Gregory Chanan, Trevor\\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\\nAntiga, Alban Desmaison, Andreas Kopf, Edward\\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\\nJunjie Bai, and Soumith Chintala. 2019. Py-\\ntorch: An imperative style, high-performance deep\\nlearning library. In H. Wallach, H. Larochelle,\\nA. Beygelzimer, F. d\\'Alché-Buc, E. Fox, and R. Gar-\\nnett, editors, Advances in Neural Information Pro-\\ncessing Systems 32 . Curran Associates, Inc.\\nJeffrey Pennington, R. Socher, and Christopher D.\\nManning. 2014. Glove: Global vectors for word rep-\\nresentation. In EMNLP .\\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\\nGardner, Christopher Clark, Kenton Lee, and Luke\\nZettlemoyer. 2018. Deep contextualized word repre-\\nsentations. ArXiv , abs/1802.05365.\\nStephen E Robertson and Steve Walker. 1994. Some\\nsimple effective approximations to the 2-poissonmodel for probabilistic weighted retrieval. In Pro-\\nceedings of the 17th Annual International ACM-\\nSIGIR Conference on Research and Development in\\nInformation Retrieval .\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N. Gomez, L. Kaiser,\\nand Illia Polosukhin. 2017. Attention is all you need.\\nInNIPS .\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\\nChaumond, Clement Delangue, Anthony Moi, Pier-\\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\\nQuentin Lhoest, and Alexander M. Rush. 2019.\\nHuggingface’s transformers: State-of-the-art natural\\nlanguage processing. ArXiv , abs/1910.03771.\\nChenyan Xiong, Zhuyun Dai, J. Callan, Zhiyuan Liu,\\nand R. Power. 2017. End-to-end neural ad-hoc rank-\\ning with kernel pooling. Proceedings of the 40th\\nInternational ACM SIGIR Conference on Research\\nand Development in Information Retrieval .\\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\\nJ. Liu, P. Bennett, Junaid Ahmed, and Arnold Over-\\nwijk. 2020. Approximate nearest neighbor negative\\ncontrastive learning for dense text retrieval. ArXiv ,\\nabs/2007.00808.\\nZ. Yang, Zihang Dai, Yiming Yang, J. Carbonell,\\nR. Salakhutdinov, and Quoc V . Le. 2019. Xlnet:\\nGeneralized autoregressive pretraining for language\\nunderstanding. In NeurIPS .A Appendix\\nA.1 Index Building Illustration\\nThe following ﬁgure demonstrates how the document \"apple pie baked ...\" is indexed by COIL. The\\ndocument is ﬁrst processed by a ﬁne-tuned deep LM to produce for each token a contextualized vector.\\nThe vectors of each term \"apple\" and \"juice\" are collected to the corresponding inverted list index along\\nwith the document id for lookup.\\napple\\nLM\\nuapple\\nv\\nappleDocument #10 - apple pie baked ...\\n10\\nv\\npie10u\\nvpie baked\\nvpie wbaked\\nv10\\nw\\nbaked\\nFigure 4: COIL Index Building of document \"apple pie baked...\"A.2 Search Illustration\\nThe following ﬁgure demonstrates how the query \"apple juice\" is processed by COIL. Contextualized\\nvectors of each term \"apple\" and \"juice\" go to the corresponding inverted list index consisting of a lookup\\nid array and a matrix stacked from document term vectors. For each index, a matrix vector product is run\\nto produce an array of scores. Afterwards a max-scatter of scores followed by a sortproduces the ﬁnal\\nranking. Note for each index, we show only operations for a subset of vectors (3 vectors) in the index\\nmatrix.\\nv u776\\nzyx776\\nScoreIdxzyx ScoreIndexapple\\nv w975\\nrqp975\\nScoreIdxrqp ScoreIndexjuice\\nSortMatrix V ector Product\\nMax Scatter\\nSortingQuery: apple juice\\nFigure 5: COIL Search of query \"apple juice\".ColBERT: Efficient and Effective Passage Search via\\nContextualized Late Interaction over BERT\\nOmar Khattab\\nStanford University\\nokhattab@stanford.eduMatei Zaharia\\nStanford University\\nmatei@cs.stanford.edu\\nABSTRACT\\nRecent progress in Natural Language Understanding (NLU) is driv-\\ning fast-paced advances in Information Retrieval (IR), largely owed\\nto fine-tuning deep language models (LMs) for document ranking.\\nWhile remarkably effective, the ranking models based on these LMs\\nincrease computational cost by orders of magnitude over prior ap-\\nproaches, particularly as they must feed each query–document pair\\nthrough a massive neural network to compute a single relevance\\nscore. To tackle this, we present ColBERT, a novel ranking model\\nthat adapts deep LMs (in particular, BERT) for efficient retrieval.\\nColBERT introduces a late interaction architecture that indepen-\\ndently encodes the query and the document using BERT and then\\nemploys a cheap yet powerful interaction step that models their fine-\\ngrained similarity. By delaying and yet retaining this fine-granular\\ninteraction, ColBERT can leverage the expressiveness of deep LMs\\nwhile simultaneously gaining the ability to pre-compute document\\nrepresentations offline, considerably speeding up query processing.\\nCrucially, ColBERT’s pruning-friendly interaction mechanism en-\\nables leveraging vector-similarity indexes for end-to-end retrieval\\ndirectly from millions of documents. We extensively evaluate Col-\\nBERT using two recent passage search datasets. Results show that\\nColBERT’s effectiveness is competitive with existing BERT-based\\nmodels (and outperforms every non-BERT baseline), while exe-\\ncuting two orders-of-magnitude faster and requiring up to four\\norders-of-magnitude fewer FLOPs per query.\\nKEYWORDS\\nNeural IR; Efficiency; Deep Language Models; BERT\\nACM Reference Format:\\nOmar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Pas-\\nsage Search via Contextualized Late Interaction over BERT. In Proceedings of\\nthe 43rd International ACM SIGIR Conference on Research and Development in\\nInformation Retrieval (SIGIR ’20), July 25–30, 2020, Virtual Event, China. ACM,\\nNew York, NY, USA, 10 pages. https://doi.org/10.1145/3397271.3401075\\n1 INTRODUCTION\\nOver the past few years, the Information Retrieval (IR) community\\nhas witnessed the introduction of a host of neural ranking models,\\nincluding DRMM [ 7], KNRM [ 4,36], and Duet [ 20,22]. In contrast\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n©2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-1-4503-8016-4/20/07. . . $15.00\\nhttps://doi.org/10.1145/3397271.3401075\\n0.15 0.20 0.25 0.30 0.35 0.40\\nMRR@10101102103104105Query Latency (ms)\\nBM25doc2queryKNRMDuet\\nDeepCTfT+ConvKNRM\\ndocTTTTTqueryBERT-baseBERT-large\\nColBERT (re-rank)ColBERT (full retrieval)Bag-of-Words (BoW) Model\\nBoW Model with NLU Augmentation\\nNeural Matching Model\\nDeep Language Model\\nColBERT (ours)Figure 1: Effectiveness (MRR@10) versus Mean Query La-\\ntency (log-scale) for a number of representative ranking\\nmodels on MS MARCO Ranking [24]. The figure also shows\\nColBERT. Neural re-rankers run on top of the official BM25\\ntop-1000 results and use a Tesla V100 GPU. Methodology and\\ndetailed results are in §4.\\nto prior learning-to-rank methods that rely on hand-crafted fea-\\ntures, these models employ embedding-based representations of\\nqueries and documents and directly model local interactions (i.e.,\\nfine-granular relationships) between their contents. Among them,\\na recent approach has emerged that fine-tunes deep pre-trained\\nlanguage models (LMs) like ELMo [ 29] and BERT [ 5] for estimating\\nrelevance. By computing deeply-contextualized semantic repre-\\nsentations of query–document pairs, these LMs help bridge the\\npervasive vocabulary mismatch [ 21,42] between documents and\\nqueries [ 30]. Indeed, in the span of just a few months, a number\\nof ranking models based on BERT have achieved state-of-the-art\\nresults on various retrieval benchmarks [ 3,18,25,39] and have\\nbeen proprietarily adapted for deployment by Google1and Bing2.\\nHowever, the remarkable gains delivered by these LMs come\\nat a steep increase in computational cost. Hofstätter et al. [9] and\\nMacAvaney et al. [18] observe that BERT-based models in the lit-\\nerature are 100-1000 ×more computationally expensive than prior\\nmodels—some of which are arguably notinexpensive to begin with\\n[13]. This quality–cost tradeoff is summarized by Figure 1, which\\ncompares two BERT-based rankers [ 25,27] against a representative\\nset of ranking models. The figure uses MS MARCO Ranking [ 24],\\na recent collection of 9M passages and 1M queries from Bing’s\\nlogs. It reports retrieval effectiveness (MRR@10) on the official\\nvalidation set as well as average query latency (log-scale) using a\\nhigh-end server that dedicates one Tesla V100 GPU per query for\\nneural re-rankers. Following the re-ranking setup of MS MARCO,\\nColBERT (re-rank), the Neural Matching Models, and the Deep LMs\\nre-rank MS MARCO’s official top-1000 documents per query. Other\\n1https://blog.google/products/search/search-language-understanding-bert/\\n2https://azure.microsoft.com/en-us/blog/bing-delivers-its-largest-improvement-\\nin-search-experience-using-azure-gpus/\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n39\\nQuery Document\\nMaxSim∑\\nMaxSim MaxSims\\nQueryCNN  /  Match KernelsCNN  /  Match Kernels / MLPMLPs\\nDocument\\n(c) All-to-all Interaction\\n(e.g., BERT)(b) Query-Document Interaction\\n(e.g., DRMM, KNRM, Conv-KNRM)(d) Late Interaction\\n(i.e., the proposed ColBERT)(a) Representation-based Similarity\\n(e.g., DSSM, SNRM)Query Document\\ns\\nQuery Document\\nsFigure 2: Schematic diagrams illustrating query–document matching paradigms in neural IR. The figure contrasts existing\\napproaches (sub-figures (a), (b), and (c)) with the proposed late interaction paradigm (sub-figure (d)).\\nmethods, including ColBERT (full retrieval), directly retrieve the\\ntop-1000 results from the entire collection.\\nAs the figure shows, BERT considerably improves search preci-\\nsion, raising MRR@10 by almost 7% against the best previous meth-\\nods; simultaneously, it increases latency by up to tens of thousands\\nof milliseconds even with a high-end GPU. This poses a challenging\\ntradeoff since raising query response times by as little as 100ms is\\nknown to impact user experience and even measurably diminish\\nrevenue [ 17]. To tackle this problem, recent work has started explor-\\ning using Natural Language Understanding (NLU) techniques to\\naugment traditional retrieval models like BM25 [ 32]. For example,\\nNogueira et al. [26,28] expand documents with NLU-generated\\nqueries before indexing with BM25 scores and Dai & Callan [ 2] re-\\nplace BM25’s term frequency with NLU-estimated term importance.\\nDespite successfully reducing latency, these approaches generally\\nreduce precision substantially relative to BERT.\\nTo reconcile efficiency and contextualization in IR, we propose\\nColBERT , a ranking model based on contextualized late interac-\\ntion over BERT . As the name suggests, ColBERT proposes a novel\\nlate interaction paradigm for estimating relevance between a query\\n𝑞and a document 𝑑. Under late interaction, 𝑞and𝑑are separately\\nencoded into two sets of contextual embeddings, and relevance is\\nevaluated using cheap and pruning-friendly computations between\\nboth sets—that is, fast computations that enable ranking without\\nexhaustively evaluating every possible candidate.\\nFigure 2 contrasts our proposed late interaction approach with\\nexisting neural matching paradigms. On the left, Figure 2 (a) illus-\\ntrates representation-focused rankers, which independently compute\\nan embedding for 𝑞and another for 𝑑and estimate relevance as\\na single similarity score between two vectors [ 12,41]. Moving to\\nthe right, Figure 2 (b) visualizes typical interaction-focused rankers.\\nInstead of summarizing 𝑞and𝑑into individual embeddings, these\\nrankers model word- and phrase-level relationships across 𝑞and𝑑\\nand match them using a deep neural network (e.g., with CNNs/MLPs\\n[22] or kernels [ 36]). In the simplest case, they feed the neural net-\\nwork an interaction matrix that reflects the similiarity between\\nevery pair of words across 𝑞and𝑑. Further right, Figure 2 (c) illus-\\ntrates a more powerful interaction-based paradigm, which models\\nthe interactions between words within as well as across𝑞and𝑑at\\nthe same time, as in BERT’s transformer architecture [25].These increasingly expressive architectures are in tension. While\\ninteraction-based models (i.e., Figure 2 (b) and (c)) tend to be su-\\nperior for IR tasks [ 8,21], a representation-focused model—by iso-\\nlating the computations among 𝑞and𝑑—makes it possible to pre-\\ncompute document representations offline [ 41], greatly reducing\\nthe computational load per query. In this work, we observe that\\nthe fine-grained matching in interaction-based models and the pre-\\ncomputation in representation-based models can be combined by\\nretaining yet judiciously delaying the query–document interaction.\\nFigure 2 (d) illustrates an architecture that precisely does so. As\\nillustrated, every query embedding interacts with all document\\nembeddings via a MaxSim operator, which computes maximum\\nsimilarity (e.g., cosine), and the scalar outputs of these operators\\nare summed across query terms. This paradigm allows ColBERT to\\nexploit deep LM-based representations while shifting the cost of\\nencoding documents offline and amortizing the cost of encoding\\nthe query once across all ranked documents. Crucially, it enables\\nColBERT to leverage vector-similarity search indexes (e.g., [ 1,15])\\nto retrieve the top- 𝑘results directly from a large document collec-\\ntion. This ability substantially improves recall over existing models,\\nwhich only re-rank the output of term-based retrieval.\\nAs Figure 1 illustrates, ColBERT can serve queries in tens or, for\\nend-to-end retrieval from millions of documents, few hundreds of\\nmilliseconds. For instance, when used for re-ranking as in “ColBERT\\n(re-rank)”, it delivers over 170 ×speedup (and requires 14,000 ×fewer\\nFLOPs) relative to existing BERT-based models [ 25,27], while be-\\ning more effective than every non-BERT baseline (§4.2 & 4.3). Col-\\nBERT’s indexing—the only time it needs to feed documents through\\nBERT—is also practical: it can index the MS MARCO collection of\\n9M passages in about 3 hours using a single server with four GPUs\\n(§4.5), retaining its effectiveness with a space footprint of as little\\nas few tens of GiBs. Our ablation study (§4.4) shows that late in-\\nteraction, its implementation via MaxSim operations, and crucial\\ndesign choices within our BERT-based encoders are all essential to\\nColBERT’s effectiveness.\\nOur main contributions are as follows.\\n(1)We propose late interaction (§3.1) as a paradigm for efficient\\nand effective neural ranking.\\n(2)We present ColBERT (§3.2 & 3.3), a highly-effective model\\nthat employs novel BERT-based query and document en-\\ncoders within the late interaction paradigm.\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n40(3)We show how to leverage ColBERT both for re-ranking on\\ntop of a term-based retrieval model (§3.5) and for searching\\na full collection using vector similarity indexes (§3.6).\\n(4)We evaluate ColBERT on MS MARCO and TREC CAR, two\\nrecent passage search collections.\\nWe release our reference implementation as open source.3\\n2 RELATED WORK\\nNeural Matching Models. Over the past few years, IR researchers\\nhave introduced numerous neural architectures for ranking. In\\nthis work, we compare against KNRM [ 4,36], Duet [ 20,22], Con-\\nvKNRM [ 4], and fastText+ConvKNRM [ 10]. KNRM proposes a dif-\\nferentiable kernel-pooling technique for extracting matching sig-\\nnals from an interaction matrix, while Duet combines signals from\\nexact-match-based as well as embedding-based similarities for rank-\\ning. Introduced in 2018, ConvKNRM learns to match 𝑛-grams in the\\nquery and the document. Lastly, fastText+ConvKNRM (abbreviated\\nfT+ConvKNRM) tackles the absence of rare words from typical\\nword embeddings lists by adopting sub-word token embeddings.\\nIn 2018, Zamani et al. [41] introduced SNRM, a representation-\\nfocused IR model that encodes each query and each document as\\na single, sparse high-dimensional vector of “latent terms”. By pro-\\nducing a sparse-vector representation for each document, SNRM\\nis able to use a traditional IR inverted index for representing docu-\\nments, allowing fast end-to-end retrieval. Despite highly promising\\nresults and insights, SNRM’s effectiveness is substantially outper-\\nformed by the state of the art on the datasets with which it was\\nevaluated (e.g., see [ 18,38]). While SNRM employs sparsity to al-\\nlow using inverted indexes, we relax this assumption and compare\\na (dense) BERT-based representation-focused model against our\\nlate-interaction ColBERT in our ablation experiments in §4.4. For a\\ndetailed overview of existing neural ranking models, we refer the\\nreaders to two recent surveys of the literature [8, 21].\\nLanguage Model Pretraining for IR. Recent work in NLU\\nemphasizes the importance pre-training language representation\\nmodels in an unsupervised fashion before subsequently fine-tuning\\nthem on downstream tasks. A notable example is BERT [ 5], a bi-\\ndirectional transformer-based language model whose fine-tuning\\nadvanced the state of the art on various NLU benchmarks. Nogueira et\\nal.[25], MacAvaney et al. [18], and Dai & Callan [ 3] investigate\\nincorporating such LMs (mainly BERT, but also ELMo [ 29]) on dif-\\nferent ranking datasets. As illustrated in Figure 2 (c), the common\\napproach (and the one adopted by Nogueira et al. on MS MARCO\\nand TREC CAR) is to feed the query–document pair through BERT\\nand use an MLP on top of BERT’s [CLS] output token to produce a\\nrelevance score. Subsequent work by Nogueira et al. [27] introduced\\nduoBERT, which fine-tunes BERT to compare the relevance of a\\npair of documents given a query. Relative to their single-document\\nBERT, this gives duoBERT about 1% MRR@10 advantage on MS\\nMARCO while increasing the cost by at least 1.4×.\\nBERT Optimizations. As discussed in §1, these rankers can be\\nhighly expensive in practice. Orthogonal to our approach, there are\\nongoing efforts in the NLU literature for distilling [ 14,33], compress-\\ning [ 40], and pruning [ 19] BERT. Other optimizations may trade\\nquality and speed specifically for IR (e.g., re-ranking with a smaller\\n3https://github.com/stanford-futuredata/ColBERT\\nQuery Document\\nQuery Encoder, fQ Document Encoder, fDMaxSim MaxSim MaxSimscore\\nOffline IndexingFigure 3: The general architecture of ColBERT given a query\\n𝑞and a document 𝑑.\\ndepth𝑘(§4.2) or truncating longer documents). While these efforts\\ncan be instrumental in narrowing the efficiency gap, they generally\\nachieve much smaller speedups than our re-designed architecture\\nfor IR, due to their generic nature, and the more aggressive ones\\noften come at the cost of noticeably lower quality.\\nEfficient NLU-based Models. Recently, a direction emerged\\nthat employs expensive NLU computation offline. This includes\\ndoc2query [ 28] and DeepCT [ 2]. The doc2query model expands\\neach document with a pre-defined number of synthetic queries, gen-\\nerated by a seq2seq transformer model trained to generate queries\\ngiven a document. It then relies on a BM25 index for retrieval from\\nthe (expanded) documents. DeepCT uses BERT to produce the term\\nfrequency component of BM25 in a context-aware manner, essen-\\ntially representing a feasible realization of the term-independence\\nassumption with neural networks [ 23]. Lastly, docTTTTTquery [ 26]\\nis identical to doc2query except that it fine-tunes a pre-trained\\nmodel (namely, T5 [31]) for generating the predicted queries.\\nConcurrently with the drafting of this paper, Hofstätter et al. [11]\\npublished their Transformer-Kernel (TK) model. At a high level, TK\\nimproves the KNRM architecture described earlier: while KNRM\\nemploys kernel pooling on top of word-embedding-based inter-\\naction, TK uses a Transformer [ 34] component for contextually\\nencoding queries and documents before kernel pooling. TK estab-\\nlishes a new state-of-the-art for non-BERT models on MS MARCO\\n(Dev); however, the best non-ensemble MRR@10 it achieves is 31%\\nwhile ColBERT reaches up to 36%. Moreover, due to indexing docu-\\nment representations offline and employing a MaxSim-based late\\ninteraction mechanism, ColBERT is much more scalable, enabling\\nend-to-end retrieval which is not supported by TK.\\n3 COLBERT\\nColBERT prescribes a simple framework for balancing the quality\\nand cost of neural IR, particularly deep language models like BERT.\\nAs introduced earlier, delaying the query–document interaction can\\nfacilitate cheap neural re-ranking (i.e., through pre-computation)\\nand even support practical end-to-end neural retrieval (i.e., through\\npruning via vector-similarity search). ColBERT addresses how to\\ndo so while still preserving the effectiveness of state-of-the-art\\nmodels, which condition the bulk of their computations on the joint\\nquery–document pair.\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n41Even though ColBERT’s late-interaction framework can be ap-\\nplied to a wide variety of architectures (e.g., CNNs, RNNs, transform-\\ners, etc.), we choose to focus this work on bi-directional transformer-\\nbased encoders (i.e., BERT) owing to their state-of-the-art effective-\\nness yet very high computational cost.\\n3.1 Architecture\\nFigure 3 depicts the general architecture of ColBERT, which com-\\nprises: (a) a query encoder 𝑓𝑄, (b) a document encoder 𝑓𝐷, and (c)\\nthe late interaction mechanism. Given a query 𝑞and document 𝑑,\\n𝑓𝑄encodes𝑞into a bag of fixed-size embeddings 𝐸𝑞while𝑓𝐷en-\\ncodes𝑑into another bag 𝐸𝑑. Crucially, each embeddings in 𝐸𝑞and\\n𝐸𝑑iscontextualized based on the other terms in 𝑞or𝑑, respectively.\\nWe describe our BERT-based encoders in §3.2.\\nUsing𝐸𝑞and𝐸𝑑, ColBERT computes the relevance score be-\\ntween𝑞and𝑑via late interaction, which we define as a summation\\nof maximum similarity (MaxSim) operators. In particular, we find\\nthe maximum cosine similarity of each 𝑣∈𝐸𝑞with vectors in 𝐸𝑑,\\nand combine the outputs via summation. Besides cosine, we also\\nevaluate squared L2 distance as a measure of vector similarity. In-\\ntuitively, this interaction mechanism softly searches for each query\\nterm𝑡𝑞—in a manner that reflects its context in the query—against\\nthe document’s embeddings, quantifying the strength of the “match”\\nvia the largest similarity score between 𝑡𝑞and a document term 𝑡𝑑.\\nGiven these term scores, it then estimates the document relevance\\nby summing the matching evidence across all query terms.\\nWhile more sophisticated matching is possible with other choices\\nsuch as deep convolution and attention layers (i.e., as in typical\\ninteraction-focused models), a summation of maximum similarity\\ncomputations has two distinctive characteristics. First, it stands\\nout as a particularly cheap interaction mechanism, as we examine\\nits FLOPs in §4.2. Second, and more importantly, it is amenable\\nto highly-efficient pruning for top- 𝑘retrieval, as we evaluate in\\n§4.3. This enables using vector-similarity algorithms for skipping\\ndocuments without materializing the full interaction matrix or even\\nconsidering each document in isolation. Other cheap choices (e.g.,\\na summation of average similarity scores, instead of maximum) are\\npossible; however, many are less amenable to pruning. In §4.4, we\\nconduct an extensive ablation study that empirically verifies the ad-\\nvantage of our MaxSim-based late interaction against alternatives.\\n3.2 Query & Document Encoders\\nPrior to late interaction, ColBERT encodes each query or document\\ninto a bag of embeddings, employing BERT-based encoders. We\\nshare a single BERT model among our query and document en-\\ncoders but distinguish input sequences that correspond to queries\\nand documents by prepending a special token [Q]to queries and\\nanother token [D]to documents.\\nQuery Encoder. Given a textual query 𝑞, we tokenize it into its\\nBERT-based WordPiece [ 35] tokens𝑞1𝑞2...𝑞𝑙. We prepend the token\\n[Q]to the query. We place this token right after BERT’s sequence-\\nstart token [CLS] . If the query has fewer than a pre-defined number\\nof tokens𝑁𝑞, we pad it with BERT’s special [mask] tokens up\\nto length𝑁𝑞(otherwise, we truncate it to the first 𝑁𝑞tokens).\\nThis padded sequence of input tokens is then passed into BERT’sdeep transformer architecture, which computes a contextualized\\nrepresentation of each token.\\nWe denote the padding with masked tokens as query augmen-\\ntation , a step that allows BERT to produce query-based embeddings\\nat the positions corresponding to these masks. Query augmentation\\nis intended to serve as a soft, differentiable mechanism for learning\\nto expand queries with new terms or to re-weigh existing terms\\nbased on their importance for matching the query. As we show in\\n§4.4, this operation is essential for ColBERT’s effectiveness.\\nGiven BERT’s representation of each token, our encoder passes\\nthe contextualized output representations through a linear layer\\nwith no activations. This layer serves to control the dimension of\\nColBERT’s embeddings, producing 𝑚-dimensional embeddings for\\nthe layer’s output size 𝑚. As we discuss later, we typically fix 𝑚to\\nbe much smaller than BERT’s fixed hidden dimension.\\nWhile ColBERT’s embedding dimension has limited impact on\\nthe efficiency of query encoding, this step is crucial for controlling\\nthe space footprint of documents, as we show in §4.5. In addition,\\nit can have a significant impact on query execution time, particu-\\nlarly the time taken for transferring the document representations\\nonto the GPU from system memory (where they reside before pro-\\ncessing a query). In fact, as we show in §4.2, gathering, stacking,\\nand transferring the embeddings from CPU to GPU can be the\\nmost expensive step in re-ranking with ColBERT. Finally, the out-\\nput embeddings are normalized so each has L2 norm equal to one.\\nThe result is that the dot-product of any two embeddings becomes\\nequivalent to their cosine similarity, falling in the [−1,1]range.\\nDocument Encoder. Our document encoder has a very similar\\narchitecture. We first segment a document 𝑑into its constituent\\ntokens𝑑1𝑑2...𝑑𝑚, to which we prepend BERT’s start token [CLS] fol-\\nlowed by our special token [D]that indicates a document sequence.\\nUnlike queries, we do not append [mask] tokens to documents. Af-\\nter passing this input sequence through BERT and the subsequent\\nlinear layer, the document encoder filters out the embeddings corre-\\nsponding to punctuation symbols, determined via a pre-defined list.\\nThis filtering is meant to reduce the number of embeddings per doc-\\nument, as we hypothesize that (even contextualized) embeddings\\nof punctuation are unnecessary for effectiveness.\\nIn summary, given 𝑞=𝑞0𝑞1...𝑞𝑙and𝑑=𝑑0𝑑1...𝑑𝑛, we compute\\nthe bags of embeddings 𝐸𝑞and𝐸𝑑in the following manner, where\\n#refers to the [mask] tokens:\\n𝐸𝑞:=Normalize(CNN(BERT(“[𝑄]𝑞0𝑞1...𝑞𝑙##...#”))) (1)\\n𝐸𝑑:=Filter(Normalize(CNN(BERT(“[𝐷]𝑑0𝑑1...𝑑𝑛”)))) (2)\\n3.3 Late Interaction\\nGiven the representation of a query 𝑞and a document 𝑑, the rele-\\nvance score of 𝑑to𝑞, denoted as𝑆𝑞,𝑑, is estimated via late interaction\\nbetween their bags of contextualized embeddings. As mentioned\\nbefore, this is conducted as a sum of maximum similarity computa-\\ntions, namely cosine similarity (implemented as dot-products due\\nto the embedding normalization) or squared L2 distance.\\n𝑆𝑞,𝑑:=Õ\\n𝑖∈[|𝐸𝑞|]max\\n𝑗∈[|𝐸𝑑|]𝐸𝑞𝑖·𝐸𝑇\\n𝑑𝑗(3)\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n42ColBERT is differentiable end-to-end. We fine-tune the BERT\\nencoders and train from scratch the additional parameters (i.e., the\\nlinear layer and the [Q] and [D] markers’ embeddings) using the\\nAdam [ 16] optimizer. Notice that our interaction mechanism has\\nno trainable parameters. Given a triple ⟨𝑞,𝑑+,𝑑−⟩with query 𝑞,\\npositive document 𝑑+and negative document 𝑑−, ColBERT is used\\nto produce a score for each document individually and is optimized\\nvia pairwise softmax cross-entropy loss over the computed scores\\nof𝑑+and𝑑−.\\n3.4 Offline Indexing: Computing & Storing\\nDocument Embeddings\\nBy design, ColBERT isolates almost all of the computations be-\\ntween queries and documents to enable pre-computing document\\nrepresentations offline. At a high level, our indexing procedure is\\nstraight-forward: we proceed over the documents in the collection\\nin batches, running our document encoder 𝑓𝐷on each batch and\\nstoring the output embeddings per document. Although indexing a\\nset of documents is an offline process, we incorporate a few simple\\noptimizations for enhancing its throughput. As we show in §4.5,\\nthese can considerably reduce the offline cost of indexing.\\nTo begin with, we exploit multiple GPUs, if available, for faster\\nencoding of batches of documents in parallel. When batching, we\\npad all documents to the maximum length of a document within\\nthe batch.4To make capping the sequence length on a per-batch\\nbasis effective, our indexer proceeds through documents in large\\ngroups of𝐵(e.g.,𝐵=100,000) documents. It sorts these documents\\nby length and then feeds batches of 𝑏(e.g.,𝑏=128) documents of\\ncomparable length through our encoder. Such length-based bucket-\\ning is sometimes refered to as a BucketIterator in some libraries\\n(e.g., allenNLP). Lastly, while most computations occur on the GPU,\\nwe found that a non-trivial portion of the indexing time is spent on\\npre-processing the text sequences, primarily BERT’s WordPiece to-\\nkenization. Exploiting that these operations are independent across\\ndocuments in a batch, we parallelize the pre-processing across the\\navailable CPU cores.\\nOnce the document representations are produced, they are saved\\nto disk using 32-bit or 16-bit values to represent each dimension.\\nAs we describe in §3.5 and 3.6, these representations are either\\nsimply loaded from disk for ranking or are subsequently indexed\\nfor vector-similarity search, respectively.\\n3.5 Top-𝑘Re-ranking with ColBERT\\nRecall that ColBERT can be used for re-ranking the output of an-\\nother retrieval model, typically a term-based model, or directly\\nfor end-to-end retrieval from a document collection. In this sec-\\ntion, we discuss how we use ColBERT for ranking a small set of\\n𝑘(e.g.,𝑘=1000) documents given a query 𝑞. Since𝑘is small, we\\nrely on batch computations to exhaustively score each document\\n(unlike our approach in §3.6). To begin with, our query serving sub-\\nsystem loads the indexed documents representations into memory,\\nrepresenting each document as a matrix of embeddings.\\nGiven a query 𝑞, we compute its bag of contextualized embed-\\ndings𝐸𝑞(Equation 1) and, concurrently, gather the document repre-\\nsentations into a 3-dimensional tensor 𝐷consisting of 𝑘document\\n4The public BERT implementations we saw simply pad to a pre-defined length.matrices. We pad the 𝑘documents to their maximum length to\\nfacilitate batched operations, and move the tensor 𝐷to the GPU’s\\nmemory. On the GPU, we compute a batch dot-product of 𝐸𝑞and\\n𝐷, possibly over multiple mini-batches. The output materializes a\\n3-dimensional tensor that is a collection of cross-match matrices\\nbetween𝑞and each document. To compute the score of each docu-\\nment, we reduce its matrix across document terms via a max-pool\\n(i.e., representing an exhaustive implementation of our MaxSim\\ncomputation) and reduce across query terms via a summation. Fi-\\nnally, we sort the 𝑘documents by their total scores.\\nRelative to existing neural rankers (especially, but not exclusively,\\nBERT-based ones), this computation is very cheap that, in fact, the\\ncost of a simple implementation is dominated by the gathering\\nand transferring of the pre-computed embeddings. To illustrate,\\nranking𝑘documents via typical BERT rankers requires feeding\\nBERT𝑘different inputs each of length 𝑙=|𝑞|+|𝑑𝑖|for query𝑞and\\ndocuments𝑑𝑖, where attention has quadratic cost in the length of\\nthe sequence. In contrast, ColBERT feeds BERT only a single, much\\nshorter sequence of length 𝑙=|𝑞|. Consequently, ColBERT is not\\nonly cheaper, it also scales much better with 𝑘(§4.2).\\n3.6 End-to-end Top- 𝑘Retrieval with ColBERT\\nAs mentioned before, ColBERT’s late-interaction operator is specifi-\\ncally designed to enable end-to-end retrieval from a large collection,\\nlargely to improve recall relative to term-based retrieval approaches.\\nThis section is concerned with cases where the number of docu-\\nments to be ranked is too large for exhaustive evaluation of each\\npossible candidate document, particularly when we are only in-\\nterested in the highest scoring ones. Concretely, we focus here on\\nretrieving the top- 𝑘results directly from a large document collec-\\ntion with𝑁(e.g.,𝑁=10,000,000) documents, where 𝑘≪𝑁.\\nTo do so, we leverage the pruning-friendly nature of the MaxSim\\noperations at the backbone of late interaction. Instead of applying\\nMaxSim between one of the query embeddings and all of one docu-\\nment’s embeddings, we can use fast vector-similarity data structures\\nto efficiently conduct this search between the query embedding\\nandalldocument embeddings across the full collection. For this,\\nwe employ an off-the-shelf library for large-scale vector-similarity\\nsearch, namely faiss [15] from Facebook.5In particular, at the\\nend of offline indexing (§3.4), we maintain a mapping from each\\nembedding to its document of origin and then index all document\\nembeddings into faiss.\\nSubsequently, when serving queries, we use a two-stage pro-\\ncedure to retrieve the top- 𝑘documents from the entire collection.\\nBoth stages rely on ColBERT’s scoring: the first is an approximate\\nstage aimed at filtering while the second is a refinement stage. For\\nthe first stage, we concurrently issue 𝑁𝑞vector-similarity queries\\n(corresponding to each of the embeddings in 𝐸𝑞) onto our faiss in-\\ndex. This retrieves the top- 𝑘′(e.g.,𝑘′=𝑘/2) matches for that vector\\nover all document embeddings. We map each of those to its docu-\\nment of origin, producing 𝑁𝑞×𝑘′document IDs, only 𝐾≤𝑁𝑞×𝑘′\\nof which are unique. These 𝐾documents likely contain one or more\\nembeddings that are highly similar to the query embeddings. For\\nthe second stage, we refine this set by exhaustively re-ranking only\\nthose𝐾documents in the usual manner described in §3.5.\\n5https://github.com/facebookresearch/faiss\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n43In our faiss -based implementation, we use an IVFPQ index\\n(“inverted file with product quantization”). This index partitions\\nthe embedding space into 𝑃(e.g.,𝑃=1000) cells based on 𝑘-means\\nclustering and then assigns each document embedding to its nearest\\ncell based on the selected vector-similarity metric. For serving\\nqueries, when searching for the top- 𝑘′matches for a single query\\nembedding, only the nearest 𝑝(e.g.,𝑝=10) partitions are searched.\\nTo improve memory efficiency, every embedding is divided into 𝑠\\n(e.g.,𝑠=16) sub-vectors, each represented using one byte. Moreover,\\nthe index conducts the similarity computations in this compressed\\ndomain, leading to cheaper computations and thus faster search.\\n4 EXPERIMENTAL EVALUATION\\nWe now turn our attention to empirically testing ColBERT, address-\\ning the following research questions.\\nRQ1: In a typical re-ranking setup, how well can ColBERT bridge\\nthe existing gap (highlighted in §1) between highly-efficient and\\nhighly-effective neural models? (§4.2)\\nRQ2: Beyond re-ranking, can ColBERT effectively support end-\\nto-end retrieval directly from a large collection? (§4.3)\\nRQ3: What does each component of ColBERT (e.g., late interac-\\ntion, query augmentation) contribute to its quality? (§4.4)\\nRQ4: What are ColBERT’s indexing-related costs in terms of\\noffline computation and memory overhead? (§4.5)\\n4.1 Methodology\\n4.1.1 Datasets & Metrics. Similar to related work [ 2,27,28], we\\nconduct our experiments on the MS MARCO Ranking [ 24] (hence-\\nforth, MS MARCO) and TREC Complex Answer Retrieval (TREC-\\nCAR) [ 6] datasets. Both of these recent datasets provide large train-\\ning data of the scale that facilitates training and evaluating deep\\nneural networks. We describe both in detail below.\\nMS MARCO. MS MARCO is a dataset (and a corresponding\\ncompetition) introduced by Microsoft in 2016 for reading compre-\\nhension and adapted in 2018 for retrieval. It is a collection of 8.8M\\npassages from Web pages, which were gathered from Bing’s re-\\nsults to 1M real-world queries. Each query is associated with sparse\\nrelevance judgements of one (or very few) documents marked as\\nrelevant and no documents explicitly indicated as irrelevant. Per\\nthe official evaluation, we use MRR@10 to measure effectiveness.\\nWe use three query sets in our evaluation. The official develop-\\nment and evaluation sets contain roughly 7k queries. The relevance\\njudgements of the evaluation set are held-out by Microsoft and\\nMRR@10 results can only be obtained by submitting to the com-\\npetition’s organizers. We submitted our main re-ranking ColBERT\\nmodel for §4.2. In addition, the collection includes roughly 55k\\nqueries (with labels) that are provided as additional validation data.\\nWe re-purpose a random sample of 5k queries among those (i.e.,\\nones not in our development or training sets) as a “local” evaluation\\nset. Along with the official development set, we use this held-out\\nset for testing our models as well as baselines in §4.3. We do so to\\navoid submitting multiple variants of the same model at once, as\\nthe organizers discourage too many submissions by the same team.\\nTREC CAR. Introduced by Dietz [ 6]et al. in 2017, TREC CAR is\\na synthetic dataset based on Wikipedia that consists of about 29M\\npassages. Similar to related work [ 25], we dedicate the first fourof five pre-defined folds for training (and the fifth for validation),\\nwhich amounts to roughly 3M queries generated by concatenating\\nthe title of a Wikipedia page with the heading of one of its sections.\\nThat section’s passages are marked as relevant to the corresponding\\nquery. Our evaluation is conducted on the test set used in TREC\\n2017 CAR, which contains 2,254 queries.\\n4.1.2 Implementation. Our ColBERT models are implemented us-\\ning Python 3 and PyTorch 1. We use the popular transformers6\\nlibrary for pre-trained BERT. Similar to [ 25], we fine-tune all Col-\\nBERT models with learning rate 3×10−6with a batch size 32. We fix\\nthe number of embeddings per query at 𝑁𝑞=32. Unless otherwise\\nstated, we set our ColBERT embedding dimension 𝑚to 128; §4.5\\ndemonstrates ColBERT’s robustness to a wide range of dimensions.\\nFor MS MARCO, we initialize the BERT components of the Col-\\nBERT query and document encoders using Google’s official pre-\\ntrained BERT basemodel and train all models for 200k iterations.\\nFor TREC CAR, we follow related work [ 2,25] and use a different\\npre-trained model to the official ones. To explain, the official BERT\\nmodels were pre-trained on Wikipedia, which is the source of TREC\\nCAR’s training and test sets. To avoid leaking test data into train,\\nNogueira and Cho [ 25] pre-train a BERT model on the Wiki pages\\ncorresponding to training subset of TREC CAR. They release their\\nBERT large pre-trained model, which we fine-tune for ColBERT’s ex-\\nperiments on TREC CAR. As BERT large embeddings are larger, we\\nset𝑚to 200, and since fine-tuning this model is significantly slower\\nthan BERT base, we train on TREC CAR for only 125k iterations.\\nIn our re-ranking results, unless otherwise stated, we use 4 bytes\\nper dimension in our embeddings and employ cosine as our vector-\\nsimilarity function. For end-to-end ranking, we use (squared) L2\\ndistance, as we found our faiss index was faster at L2-based re-\\ntrieval. For our faiss index, we set the number of partitions to\\n𝑃=2,000, and search the nearest 𝑝=10to each query embedding to\\nretrieve𝑘′=𝑘=1000 document vectors per query embedding. We\\ndivide each embedding into 𝑠=16sub-vectors, each encoded using\\none byte. To represent the index used for the second stage of our\\nend-to-end retrieval procedure, we use 16-bit values per dimension.\\n4.1.3 Hardware & Time Measurements. To evaluate the latency of\\nneural re-ranking models in §4.2, we use a single Tesla V100 GPU\\nthat has 32 GiBs of memory on a server with two Intel Xeon Gold\\n6132 CPUs, each with 14 physical cores (24 hyperthreads), and 469\\nGiBs of RAM. For the mostly CPU-based retrieval experiments in\\n§4.3 and the indexing experiments in §4.5, we use another server\\nwith the same CPU and system memory specifications but which\\nhas four Titan V GPUs attached, each with 12 GiBs of memory.\\nAcross all experiments, only one GPU is dedicated per query for\\nretrieval (i.e., for methods with neural computations) but we use\\nup to all four GPUs during indexing.\\n4.2 Quality–Cost Tradeoff: Top- 𝑘Re-ranking\\nIn this section, we examine ColBERT’s efficiency and effectiveness\\nat re-ranking the top- 𝑘results extracted by a bag-of-words retrieval\\nmodel, which is the most typical setting for testing and deploying\\nneural ranking models. We begin with the MS MARCO dataset. We\\n6https://github.com/huggingface/transformers\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n44Method MRR@10 (Dev) MRR@10 (Eval) Re-ranking Latency (ms) FLOPs/query\\nBM25 (official) 16.7 16.5 - -\\nKNRM 19.8 19.8 3 592M (0.085×)\\nDuet 24.3 24.5 22 159B (23×)\\nfastText+ConvKNRM 29.0 27.7 28 78B (11×)\\nBERT base[25] 34.7 - 10,700 97T (13,900×)\\nBERT base(our training) 36.0 - 10,700 97T (13,900×)\\nBERT large [25] 36.5 35.9 32,900 340T (48,600×)\\nColBERT (over BERT base) 34.9 34.9 61 7B (1×)\\nTable 1: “Re-ranking” results on MS MARCO. Each neural model re-ranks the official top-1000 results produced by BM25.\\nLatency is reported for re-ranking only. To obtain the end-to-end latency in Figure 1, we add the BM25 latency from Table 2.\\nMethod MRR@10 (Dev) MRR@10 (Local Eval) Latency (ms) Recall@50 Recall@200 Recall@1000\\nBM25 (official) 16.7 - - - - 81.4\\nBM25 (Anserini) 18.7 19.5 62 59.2 73.8 85.7\\ndoc2query 21.5 22.8 85 64.4 77.9 89.1\\nDeepCT 24.3 - 62(est.) 69 [2] 82 [2] 91 [2]\\ndocTTTTTquery 27.7 28.4 87 75.6 86.9 94.7\\nColBERT L2(re-rank) 34.8 36.4 - 75.3 80.5 81.4\\nColBERT L2(end-to-end) 36.0 36.7 458 82.9 92.3 96.8\\nTable 2: End-to-end retrieval results on MS MARCO. Each model retrieves the top-1000 documents per query directly from the\\nentire 8.8M document collection.\\ncompare against KNRM, Duet, and fastText+ConvKNRM, a repre-\\nsentative set of neural matching models that have been previously\\ntested on MS MARCO. In addition, we compare against the adapta-\\ntion of BERT for ranking by Nogueira and Cho [ 25], in particular,\\ntheir BERT baseand its deeper counterpart BERT large.7\\nWe report the competition’s official metric, namely MRR@10, on\\nthe validation set (Dev) and the evaluation set (Eval). We also report\\nthe re-ranking latency, which we measure using a single Tesla V100\\nGPU, and the FLOPs per query for each neural ranking model. To do\\nso, we adapt the baselines’ publicly-available reference implemen-\\ntations into our pytorch testbed. For ColBERT, our reported latency\\nsubsumes the entire computation from gathering the document\\nrepresentations, moving them to the GPU, tokenizing then encod-\\ning the query, and applying late interaction to compute document\\nscores. For the baselines, we measure the scoring computations on\\nthe GPU and exclude the CPU-based text preprocessing (similar\\nto [9]). In principle, the baselines can pre-compute most of this\\npreprocessing (e.g., document tokenization) offline. We estimate\\nthe FLOPs using the torchprofile8library.\\nWe now proceed to study the results, which are reported in Ta-\\nble 1. To begin with, we notice the fast progress from KNRM in\\n2017 to the BERT-based models in 2019, manifesting itself in over\\n16% increase in MRR@10. As described in §1, the simultaneous\\nincrease in computational cost is difficult to miss. Judging by their\\nrather monotonic pattern of increasingly larger cost and higher ef-\\nfectiveness, these results appear to paint a picture where expensive\\nmodels are necessary for high-quality ranking.\\nIn contrast with this trend, ColBERT (which employs late in-\\nteraction over BERT base) performs competitively with the original\\n7https://github.com/nyu-dl/dl4marco-bert/\\n8https://github.com/mit-han-lab/torchprofileadaptation of BERT baseand BERT large for ranking by Nogueira\\nand Cho [ 25,27]. Interestingly, ColBERT appears no worse than\\nBERT basein MRR@10—although the latter uses a different loss func-\\ntion to ColBERT’s (§3.3). To confirm the intuition that ColBERT’s\\nlate interaction does trade away some of BERT’s quality, the table\\nalso reports results of “BERT base(our training)”, which is based on\\nNogueira and Cho’s model of the same size but is optimized with\\npairwise softmax cross-entropy loss. We train it with learning rate\\n3×10−6and batch size 16 for 200k iterations. Unlike the original\\nBERT baseranker, results show that this model does in fact have an\\nedge over ColBERT’s effectiveness.\\nWhile highly competitive in retrieval quality, ColBERT is orders\\nof magnitude cheaper than BERT base, in particular, by over 170 ×in\\nlatency and 13,900×in FLOPs. This highlights the expressiveness\\nof our proposed late interaction mechanism when coupled with a\\npowerful pre-trained LM like BERT. While ColBERT’s re-ranking\\nlatency is slightly higher than the non-BERT models shown (i.e., by\\n10s of milliseconds), this difference is explained by the time it takes\\na simple Python implementation to gather, stack, and transfer the\\ndocument embeddings to the GPU. In particular, the query encoding\\nand interaction in ColBERT consume only 13 milliseconds of its\\ntotal execution time.\\nDiving deeper into the quality–cost tradeoff between BERT and\\nColBERT, Figure 4 demonstrates the relationships between FLOPs\\nand effectiveness (MRR@10) as a function of the re-ranking depth\\n𝑘when re-ranking the top- 𝑘results by BM25, comparing ColBERT\\nand BERT base(our training). We conduct this experiment on MS\\nMARCO (Dev). We note here that as the official top-1000 ranking\\ndoes not provide the BM25 order (and also lacks documents beyond\\nthe top-1000 per query), the models in this experiment re-rank the\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n450.27 0.29 0.31 0.33 0.35 0.37\\nMRR@10103104105106107108109Million FLOPs (log-scale)\\nk=10205010020050010002000\\nk=10 20 50 100\\n200500\\n10002000\\nBERTbase (our training)\\nColBERTFigure 4: FLOPs (in millions) and MRR@10 as functions\\nof the re-ranking depth 𝑘. Since the official BM25 ranking\\nis not ordered, the initial top- 𝑘retrieval is conducted with\\nAnserini’s BM25.\\nAnserini [ 37] toolkit’s BM25 output. Consequently, both MRR@10\\nvalues at𝑘=1000 are slightly higher from those reported in Table 1.\\nStudying the results in Figure 4, we notice that not only is Col-\\nBERT much cheaper than BERT for the same model size (i.e., 12-\\nlayer “base” transformer encoder), it also scales better with the\\nnumber of ranked documents. In part, this is because ColBERT\\nonly needs to process the query once, irrespective of the number of\\ndocuments evaluated. For instance, at 𝑘=10, BERT requires nearly\\n180×more FLOPs than ColBERT; at 𝑘=1000, BERT’s overhead\\njumps to 13,900×. It then reaches 23,000× at𝑘=2000.\\nWe observe that this orders-of-magnitude reduction in FLOPs\\nmakes it practical to run ColBERT entirely on the CPU. In fact, sub-\\nsequent informal experimentation suggests that ColBERT’s latency\\nand FLOPs can be considerably reduced further by a number of\\noptimizations, some entailing a controllable quality tradeoff. These\\ninclude using smaller vector dimensions (whose MRR@10 is tested\\nin §4.5), padding queries to shorter 𝑁𝑞, processing documents in\\na lengths-aware fashion, and distilling/quantizing the encoder(s)\\n(§2), the final two of which are also applicable to the baseline BERT\\nreference implementation by Nogueira and Cho [ 25]. Addition-\\nally, caching the document embeddings on the GPU(s)—if sufficient\\nGPU memory exists—can significantly reduce ColBERT’s latency.\\nLastly, batch-processing of multiple queries can enhance ColBERT’s\\nthroughput by improving the GPU utilization of query encoding.\\nWe leave exploring these opportunities for future work.\\nMethod MAP MRR@10\\nBM25 (Anserini) 15.3 -\\ndoc2query 18.1 -\\nDeepCT 24.6 33.2\\nBM25 + BERT base 31.0 -\\nBM25 + BERT large 33.5 -\\nBM25 + ColBERT 31.3 44.2\\nTable 3: Results on TREC CAR.\\nHaving studied our results on MS MARCO, we now consider\\nTREC CAR, whose official metric is MAP. Similar to Table 1, we\\nalso report MRR@10. The results are summarized in Table 3, which\\nincludes a number of important baselines (BM25, doc2query, and\\nDeepCT) in addition to re-ranking baselines that have previouslybeen tested on this dataset. As the table shows, the results mirror\\nthose seen with MS MARCO.\\n4.3 End-to-end Top- 𝑘Retrieval\\nBeyond cheap re-ranking, ColBERT is amenable to top- 𝑘retrieval di-\\nrectly from a full collection. Table 2 considers full retrieval, wherein\\neach model retrieves the top-1000 documents directly from MS\\nMARCO’s 8.8M documents per query. In addition to MRR@10 and\\nlatency in milliseconds, the table reports Recall@50, Recall@200,\\nand Recall@1000, important metrics for a full-retrieval model that\\nessentially filters down a large collection on a per-query basis.\\nWe compare against BM25, in particular MS MARCO’s official\\nBM25 ranking as well as a well-tuned baseline based on the Anserini\\ntoolkit.9While many other traditional models exist, we are not\\naware of any that substantially outperform Anserini’s BM25 im-\\nplementation (e.g., see RM3 in [ 28], LMDir in [ 2], or Microsoft’s\\nproprietary feature-based RankSVM on the leaderboard).\\nWe also compare against doc2query, DeepCT, and docTTTTT-\\nquery. All three rely on a traditional bag-of-words model (primarily\\nBM25) for retrieval. Crucially, however, they re-weigh the frequency\\nof terms per document and/or expand the set of terms in each doc-\\nument before building the BM25 index. In particular, doc2query\\nexpands each document with a pre-defined number of synthetic\\nqueries generated by a seq2seq transformer model (which docTTT-\\nTquery replaced with a pre-trained language model, T5 [ 31]). In\\ncontrast, DeepCT uses BERT to produce the term frequency com-\\nponent of BM25 in a context-aware manner.\\nFor the latency of Anserini’s BM25, doc2query, and docTTTT-\\nquery, we use the authors’ [ 26,28] Anserini-based implementation.\\nWhile this implementation supports multi-threading, it only utilizes\\nparallelism across different queries. We thus report single-threaded\\nlatency for these models, noting that simply parallelizing their com-\\nputation over shards of the index can substantially decrease their\\nalready-low latency. For DeepCT, we only estimate its latency us-\\ning that of BM25 (as denoted by (est.) in the table), since DeepCT\\nre-weighs BM25’s term frequency without modifying the index\\notherwise.10As discussed in §4.1, we use ColBERT L2for end-to-\\nend retrieval, which employs negative squared L2 distance as its\\nvector-similarity function. For its latency, we measure the time for\\nfaiss -based candidate filtering and the subsequent re-ranking. In\\nthis experiment, faiss uses all available CPU cores.\\nLooking at Table 2, we first see Anserini’s BM25 baseline at 18.7\\nMRR@10, noticing its very low latency as implemented in Anserini\\n(which extends the well-known Lucene system), owing to both\\nvery cheap operations and decades of bag-of-words top- 𝑘retrieval\\noptimizations. The three subsequent baselines, namely doc2query,\\nDeepCT, and docTTTTquery, each brings a decisive enhancement\\nto effectiveness. These improvements come at negligible overheads\\nin latency, since these baselines ultimately rely on BM25-based\\nretrieval. The most effective among these three, docTTTTquery,\\ndemonstrates a massive 9% gain over vanilla BM25 by fine-tuning\\nthe recent language model T5.\\n9http://anserini.io/\\n10In practice, a myriad of reasons could still cause DeepCT’s latency to differ\\nslightly from BM25’s. For instance, the top- 𝑘pruning strategy employed, if any, could\\ninteract differently with a changed distribution of scores.\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n46Shifting our attention to ColBERT’s end-to-end retrieval effec-\\ntiveness, we see its major gains in MRR@10 over all of these end-\\nto-end models. In fact, using ColBERT in the end-to-end setup is su-\\nperior in terms of MRR@10 to re-ranking with the same model due\\nto the improved recall. Moving beyond MRR@10, we also see large\\ngains in Recall@ 𝑘for𝑘equals to 50, 200, and 1000. For instance, its\\nRecall@50 actually exceeds the official BM25’s Recall@1000 and\\neven all but docTTTTTquery’s Recall@200, emphasizing the value\\nof end-to-end retrieval (instead of just re-ranking) with ColBERT.\\n4.4 Ablation Studies\\n0.220.240.260.280.300.320.340.36\\nMRR@10BERT [CLS]-based dot-product (5-layer)  [A]\\nColBERT via average similarity (5-layer)  [B]\\nColBERT without query augmentation (5-layer)  [C]\\nColBERT (5-layer)  [D]\\nColBERT (12-layer)  [E]\\nColBERT + e2e retrieval (12-layer)  [F]\\nFigure 5: Ablation results on MS MARCO (Dev). Between\\nbrackets is the number of BERT layers used in each model.\\nThe results from §4.2 indicate that ColBERT is highly effective\\ndespite the low cost and simplicity of its late interaction mechanism.\\nTo better understand the source of this effectiveness, we examine a\\nnumber of important details in ColBERT’s interaction and encoder\\narchitecture. For this ablation, we report MRR@10 on the validation\\nset of MS MARCO in Figure 5, which shows our main re-ranking\\nColBERT model [E], with MRR@10 of 34.9%.\\nDue to the cost of training all models, we train a copy of our\\nmain model that retains only the first 5 layers of BERT out of 12\\n(i.e., model [D]) and similarly train all our ablation models for\\n200k iterations with five BERT layers. To begin with, we ask if the\\nfine-granular interaction in late interaction is necessary. Model [A]\\ntackles this question: it uses BERT to produce a single embedding\\nvector for the query and another for the document, extracted from\\nBERT’s [CLS] contextualized embedding and expanded through a\\nlinear layer to dimension 4096 (which equals 𝑁𝑞×128=32×128).\\nRelevance is estimated as the inner product of the query’s and the\\ndocument’s embeddings, which we found to perform better than\\ncosine similarity for single-vector re-ranking. As the results show,\\nthis model is considerably less effective than ColBERT, reinforcing\\nthe importance of late interaction.\\nSubsequently, we ask if our MaxSim-based late interaction is bet-\\nter than other simple alternatives. We test a model [B] that replaces\\nColBERT’s maximum similarity with average similarity. The results\\nsuggest the importance of individual terms in the query paying\\nspecial attention to particular terms in the document. Similarly,\\nthe figure emphasizes the importance of our query augmentation\\nmechanism: without query augmentation [C], ColBERT has a no-\\nticeably lower MRR@10. Lastly, we see the impact of end-to-end\\nretrieval not only on recall but also on MRR@10. By retrieving\\ndirectly from the full collection, ColBERT is able to retrieve to the\\ntop-10 documents missed entirely from BM25’s top-1000.\\n4.5 Indexing Throughput & Footprint\\nLastly, we examine the indexing throughput and space footprint\\nof ColBERT. Figure 6 reports indexing throughput on MS MARCO\\n0 10000 20000 30000 40000 50000\\nThroughput (documents/minute)Basic ColBERT Indexing\\n+multi-GPU document processing\\n+per-batch maximum sequence length\\n+length-based bucketing\\n+multi-core pre-processingFigure 6: Effect of ColBERT’s indexing optimizations on the\\noffline indexing throughput.\\ndocuments with ColBERT and four other ablation settings, which\\nindividually enable optimizations described in §3.4 on top of basic\\nbatched indexing. Based on these throughputs, ColBERT can index\\nMS MARCO in about three hours. Note that any BERT-based model\\nmust incur the computational cost of processing each document\\nat least once. While ColBERT encodes each document with BERT\\nexactly once, existing BERT-based rankers would repeat similar\\ncomputations on possibly hundreds of documents for each query.\\nSetting Dimension( 𝑚) Bytes/Dim Space(GiBs) MRR@10\\nRe-rank Cosine 128 4 286 34.9\\nEnd-to-end L2 128 2 154 36.0\\nRe-rank L2 128 2 143 34.8\\nRe-rank Cosine 48 4 54 34.4\\nRe-rank Cosine 24 2 27 33.9\\nTable 4: Space Footprint vs MRR@10 (Dev) on MS MARCO.\\nTable 4 reports the space footprint of ColBERT under various\\nsettings as we reduce the embeddings dimension and/or the bytes\\nper dimension. Interestingly, the most space-efficient setting, that\\nis, re-ranking with cosine similarity with 24-dimensional vectors\\nstored as 2-byte floats, is only 1% worse in MRR@10 than the most\\nspace-consuming one, while the former requires only 27 GiBs to\\nrepresent the MS MARCO collection.\\n5 CONCLUSIONS\\nIn this paper, we introduced ColBERT, a novel ranking model that\\nemploys contextualized late interaction over deep LMs (in particular,\\nBERT) for efficient retrieval. By independently encoding queries\\nand documents into fine-grained representations that interact via\\ncheap and pruning-friendly computations, ColBERT can leverage\\nthe expressiveness of deep LMs while greatly speeding up query\\nprocessing. Crucially, doing so allows scaling ColBERT to end-\\nto-end neural retrieval directly from a large document collection,\\nwhich can greatly improve recall over existing models. Our results\\nshow that ColBERT is two orders-of-magnitude faster than existing\\nBERT-based models, all while only minimally impacting re-ranking\\nquality and while outperforming every non-BERT baseline.\\nAcknowledgments. OK was supported by the Eltoukhy Family\\nGraduate Fellowship at the Stanford School of Engineering. This\\nresearch was supported in part by affiliate members and other\\nsupporters of the Stanford DAWN project—Ant Financial, Facebook,\\nGoogle, Infosys, NEC, and VMware—as well as Cisco, SAP, and the\\nNSF under CAREER grant CNS-1651570. Any opinions, findings,\\nand conclusions or recommendations expressed in this material are\\nthose of the authors and do not necessarily reflect the views of the\\nNational Science Foundation.\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n47REFERENCES\\n[1]Firas Abuzaid, Geet Sethi, Peter Bailis, and Matei Zaharia. 2019. To Index or Not\\nto Index: Optimizing Exact Maximum Inner Product Search. In 2019 IEEE 35th\\nInternational Conference on Data Engineering (ICDE). IEEE, 1250–1261.\\n[2]Zhuyun Dai and Jamie Callan. 2019. Context-Aware Sentence/Passage Term\\nImportance Estimation For First Stage Retrieval. arXiv preprint arXiv:1910.10687\\n(2019).\\n[3]Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with\\nContextual Neural Language Modeling. arXiv preprint arXiv:1905.09217 (2019).\\n[4]Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional\\nneural networks for soft-matching n-grams in ad-hoc search. In Proceedings of the\\neleventh ACM international conference on web search and data mining. 126–134.\\n[5]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\\nPre-training of deep bidirectional transformers for language understanding. arXiv\\npreprint arXiv:1810.04805 (2018).\\n[6]Laura Dietz, Manisha Verma, Filip Radlinski, and Nick Craswell. 2017. TREC\\nComplex Answer Retrieval Overview.. In TREC.\\n[7]Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. 2016. A deep relevance\\nmatching model for ad-hoc retrieval. In Proceedings of the 25th ACM International\\non Conference on Information and Knowledge Management. ACM, 55–64.\\n[8]Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani, Chen\\nWu, W Bruce Croft, and Xueqi Cheng. 2019. A deep look into neural ranking\\nmodels for information retrieval. arXiv preprint arXiv:1903.06902 (2019).\\n[9]Sebastian Hofstätter and Allan Hanbury. 2019. Let’s measure run time! Extending\\nthe IR replicability infrastructure to include performance aspects. arXiv preprint\\narXiv:1907.04614 (2019).\\n[10] Sebastian Hofstätter, Navid Rekabsaz, Carsten Eickhoff, and Allan Hanbury. 2019.\\nOn the effect of low-frequency terms on neural-IR models. In Proceedings of\\nthe 42nd International ACM SIGIR Conference on Research and Development in\\nInformation Retrieval. 1137–1140.\\n[11] Sebastian Hofstätter, Markus Zlabinger, and Allan Hanbury. 2019. TU Wien@\\nTREC Deep Learning’19–Simple Contextualization for Re-ranking. arXiv preprint\\narXiv:1912.01385 (2019).\\n[12] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry\\nHeck. 2013. Learning deep structured semantic models for web search using\\nclickthrough data. In Proceedings of the 22nd ACM international conference on\\nInformation & Knowledge Management. 2333–2338.\\n[13] Shiyu Ji, Jinjin Shao, and Tao Yang. 2019. Efficient Interaction-based Neural\\nRanking with Locality Sensitive Hashing. In The World Wide Web Conference.\\nACM, 2858–2864.\\n[14] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,\\nand Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding.\\narXiv preprint arXiv:1909.10351 (2019).\\n[15] Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. Billion-scale similarity\\nsearch with GPUs. arXiv preprint arXiv:1702.08734 (2017).\\n[16] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\\nmization. arXiv preprint arXiv:1412.6980 (2014).\\n[17] Ron Kohavi, Alex Deng, Brian Frasca, Toby Walker, Ya Xu, and Nils Pohlmann.\\n2013. Online controlled experiments at large scale. In SIGKDD.\\n[18] Sean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. 2019. Cedr:\\nContextualized embeddings for document ranking. In Proceedings of the 42nd\\nInternational ACM SIGIR Conference on Research and Development in Information\\nRetrieval. ACM, 1101–1104.\\n[19] Paul Michel, Omer Levy, and Graham Neubig. 2019. Are Sixteen Heads Really\\nBetter than One?. In Advances in Neural Information Processing Systems . 14014–\\n14024.\\n[20] Bhaskar Mitra and Nick Craswell. 2019. An Updated Duet Model for Passage\\nRe-ranking. arXiv preprint arXiv:1903.07666 (2019).\\n[21] Bhaskar Mitra, Nick Craswell, et al .2018. An introduction to neural information\\nretrieval. Foundations and Trends® in Information Retrieval 13, 1 (2018), 1–126.\\n[22] Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to match using\\nlocal and distributed representations of text for web search. In Proceedings of the26th International Conference on World Wide Web. International World Wide Web\\nConferences Steering Committee, 1291–1299.\\n[23] Bhaskar Mitra, Corby Rosset, David Hawking, Nick Craswell, Fernando Diaz,\\nand Emine Yilmaz. 2019. Incorporating query term independence assumption\\nfor efficient retrieval and ranking using deep neural networks. arXiv preprint\\narXiv:1907.03693 (2019).\\n[24] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\\nMajumder, and Li Deng. 2016. MS MARCO: A Human-Generated MAchine\\nReading COmprehension Dataset. (2016).\\n[25] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.\\narXiv preprint arXiv:1901.04085 (2019).\\n[26] Rodrigo Nogueira, Jimmy Lin, and AI Epistemic. 2019. From doc2query to\\ndocTTTTTquery. (2019).\\n[27] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-Stage\\nDocument Ranking with BERT. arXiv preprint arXiv:1910.14424 (2019).\\n[28] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\\nExpansion by Query Prediction. arXiv preprint arXiv:1904.08375 (2019).\\n[29] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher\\nClark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word\\nrepresentations. arXiv preprint arXiv:1802.05365 (2018).\\n[30] Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. Understanding\\nthe Behaviors of BERT in Ranking. arXiv preprint arXiv:1904.07531 (2019).\\n[31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the lim-\\nits of transfer learning with a unified text-to-text transformer. arXiv preprint\\narXiv:1910.10683 (2019).\\n[32] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu,\\nMike Gatford, et al. 1995. Okapi at TREC-3. NIST Special Publication (1995).\\n[33] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin.\\n2019. Distilling task-specific knowledge from BERT into simple neural networks.\\narXiv preprint arXiv:1903.12136 (2019).\\n[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in neural information processing systems. 5998–6008.\\n[35] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi,\\nWolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al .\\n2016. Google’s neural machine translation system: Bridging the gap between\\nhuman and machine translation. arXiv preprint arXiv:1609.08144 (2016).\\n[36] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017.\\nEnd-to-end neural ad-hoc ranking with kernel pooling. In Proceedings of the 40th\\nInternational ACM SIGIR conference on research and development in information\\nretrieval. 55–64.\\n[37] Peilin Yang, Hui Fang, and Jimmy Lin. 2018. Anserini: Reproducible ranking\\nbaselines using Lucene. Journal of Data and Information Quality (JDIQ) 10, 4\\n(2018), 1–20.\\n[38] Wei Yang, Kuang Lu, Peilin Yang, and Jimmy Lin. 2019. Critically Examining\\nthe\" Neural Hype\" Weak Baselines and the Additivity of Effectiveness Gains\\nfrom Neural Ranking Models. In Proceedings of the 42nd International ACM SIGIR\\nConference on Research and Development in Information Retrieval. 1129–1132.\\n[39] Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang, and Jimmy Lin. 2019.\\nCross-domain modeling of sentence-level evidence for document retrieval. In\\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-\\ncessing and the 9th International Joint Conference on Natural Language Processing\\n(EMNLP-IJCNLP). 3481–3487.\\n[40] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8bert:\\nQuantized 8bit bert. arXiv preprint arXiv:1910.06188 (2019).\\n[41] Hamed Zamani, Mostafa Dehghani, W Bruce Croft, Erik Learned-Miller, and\\nJaap Kamps. 2018. From neural re-ranking to neural ranking: Learning a sparse\\nrepresentation for inverted indexing. In Proceedings of the 27th ACM International\\nConference on Information and Knowledge Management. ACM, 497–506.\\n[42] Le Zhao. 2012. Modeling and solving term mismatch for full-text retrieval. Ph.D.\\nDissertation. Carnegie Mellon University.\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n48'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#need to split the text that we read into smaller chunks so that during information retreival we dont hit the tokeb size limits\n",
        "text_splitter=CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=0,\n",
        "    length_function=len,\n",
        ")\n",
        "texts=text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "id": "VazABGc4LX1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text[:150]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "42cP2vErLXzb",
        "outputId": "9dadbf17-bb62-4e4b-91f7-bf68bafdbfa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Query2doc: Query Expansion with Large Language Models\\nLiang Wang and Nan Yang and Furu Wei\\nMicrosoft Research\\n{wangliang,nanya,fuwei}@microsoft.com\\nAb'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download embedding from OpenAI\n",
        "embeddings=OpenAIEmbeddings(openai_api_key=os.environ[\"OPENAI_API_KEY\"])"
      ],
      "metadata": {
        "id": "kI6tZkY7LXuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch=FAISS.from_texts(texts,embeddings)\n",
        "docsearch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBpoL517LXsd",
        "outputId": "3496ff03-e01e-4096-9104-a95c199c8fc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain.vectorstores.faiss.FAISS at 0x7a61022a13f0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain=load_qa_chain(OpenAI(),chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "tPUwlWYsLXqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##building chatbot  "
      ],
      "metadata": {
        "id": "qGygQ7HgMLf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#some important lib to build chatbot as chatgpt\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "from langchain.chains import ConversationalRetrievalChain"
      ],
      "metadata": {
        "id": "RDpjvH-kLXmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create conversation chain that uses vectordb as retriever\n",
        "qa=ConversationalRetrievalChain.from_llm(OpenAI(temperature=0.1),docsearch.as_retriever())"
      ],
      "metadata": {
        "id": "6MnUJTOwNIes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history=[]\n",
        "def on_submit(_):\n",
        "  query=input_box.value\n",
        "  input_box.vakue=\"\"\n",
        "\n",
        "  if query.lower()=='exit':\n",
        "    print(\"thanks for using the chatbot\")\n",
        "    return\n",
        "\n",
        "  result=qa({\"question\": query ,\"chat_history\":chat_history})\n",
        "  chat_history.append((query,result['answer']))\n",
        "\n",
        "  display(widgets.HTML(f'<b><font color=\"red\">User : </font></b>{query}'))\n",
        "  display(widgets.HTML(f'<b><font color=\"blue\">Chatbot :</font></b>{result[\"answer\"]}'))\n",
        "\n",
        "print('Welcome to the article')\n",
        "\n",
        "input_box= widgets.Text(placeholder='please type question: ')\n",
        "input_box.on_submit(on_submit)\n",
        "\n",
        "display(input_box)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "58ff365cfc8140e2b298c880a6971cc6",
            "08bd0e1dcdf54b22abd93c93cc482fc4",
            "93d8a3d6be4f4462b18b9e6f279ac436",
            "b5f530a5b3c14689b69bdabe445f1e14",
            "ee2c0724ff394844a69340f9ec04d0d1",
            "f165e5a55cb24099bd9f6d0f2425d4fe",
            "5e79213c89ad472090424ac717e2b5e5",
            "9fb6a6e50c824566b26ec922580a3cf5",
            "02d8190d2db843ea96121e3736c3abb1",
            "4f0cb01978f4419684d5dde8785cab45",
            "25d9a4339a9c4f9c9e9b9fe10a76e1c4",
            "c4f3ad06d9b44a3eb4911d7303b7a7e5",
            "427613941e284d7897e9bda31fff6c2b",
            "b4cc6cd4fa5b45689bb1d76a35aac63b",
            "cbe1231509cf434981e8ea215aba4f13",
            "054c74edba7a49fb94bb4f2ef24bef4d",
            "b6a230f9e36346168ed429026be71b2f",
            "e3e213ba2b99432ebac79239431ee177",
            "6656c481303a46cfb79a3e494bf66e16",
            "ca90d2a23ab6460b9fc055d5870c0c91",
            "b4f0a9f041944d75b89dd48873363c2e",
            "2f0194cb626f445fa33351d84acd8134",
            "bf3764bbd1c04b5ab8ca793ed1d4e19e",
            "709d6e7454b342d69c1a95448a8f21c1",
            "d4d2d5391ab74014975d750aacdca4a1",
            "6efd3462cffd4217b8794ed4165e0ffd",
            "51e657ca3ced405dafd005c5422a4b51"
          ]
        },
        "id": "bp-SKJfgLXiT",
        "outputId": "8bb89810-8653-44d0-fcca-cd5a7ae52d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the article\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', placeholder='please type question: ')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58ff365cfc8140e2b298c880a6971cc6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTML(value='<b><font color=\"red\">User : </font></b>what is this in artic')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5f530a5b3c14689b69bdabe445f1e14"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTML(value='<b><font color=\"blue\">Chatbot :</font></b> This is an article about ColBERT: Efficient and Effecti…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e79213c89ad472090424ac717e2b5e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTML(value='<b><font color=\"red\">User : </font></b>how many pages in thiss artic')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f0cb01978f4419684d5dde8785cab45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTML(value='<b><font color=\"blue\">Chatbot :</font></b> 10 pages.')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "427613941e284d7897e9bda31fff6c2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTML(value='<b><font color=\"red\">User : </font></b>who is main auther')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "054c74edba7a49fb94bb4f2ef24bef4d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTML(value='<b><font color=\"blue\">Chatbot :</font></b> The main author of this article is Rodrigo Nogueira.')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6656c481303a46cfb79a3e494bf66e16"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTML(value='<b><font color=\"red\">User : </font></b>is this article is worth')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f0194cb626f445fa33351d84acd8134"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTML(value='<b><font color=\"blue\">Chatbot :</font></b> No, this article is not worth reading. It is a research…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4d2d5391ab74014975d750aacdca4a1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Zb7BTpPLXek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fOovRfvrLXcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WRTXc1HfLXZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H9P-PaTrLXXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9yUltfI8LXU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ubCKOtCELWxQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}